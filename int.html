<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Knowledge of the U.S. Social Sciences</title>
  <meta name="description" content="Knowledge of the U.S. Social Sciences">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Knowledge of the U.S. Social Sciences" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Knowledge of the U.S. Social Sciences" />
  
  
  

<meta name="author" content="Brooks Ambrose">


<meta name="date" content="2019-07-16">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="abstracts.html">
<link rel="next" href="gen.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<link href="libs/wordcloud2-0.0.1/wordcloud.css" rel="stylesheet" />
<script src="libs/wordcloud2-0.0.1/wordcloud2-all.js"></script>
<script src="libs/wordcloud2-0.0.1/hover.js"></script>
<script src="libs/wordcloud2-binding-0.2.0/wordcloud2.js"></script>
<script src="libs/d3-4.5.0/d3.min.js"></script>
<script src="libs/sankey-1/sankey.js"></script>
<script src="libs/sankeyNetwork-binding-0.4/sankeyNetwork.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.41.3/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.41.3/plotly-latest.min.js"></script>
<script src="libs/plotly-binding-4.8.0/plotly.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<script src="libs/forceNetwork-binding-0.4/forceNetwork.js"></script>



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Brooks Ambrose</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Getting Started</a></li>
<li class="chapter" data-level="" data-path="knowledge-of-the-u-s-social-sciences.html"><a href="knowledge-of-the-u-s-social-sciences.html"><i class="fa fa-check"></i>Knowledge of the U.S. Social Sciences</a></li>
<li class="chapter" data-level="" data-path="abstracts.html"><a href="abstracts.html"><i class="fa fa-check"></i>Abstracts</a><ul>
<li class="chapter" data-level="" data-path="abstracts.html"><a href="abstracts.html#genre-and-the-literature"><i class="fa fa-check"></i>Genre and the Literature</a></li>
<li class="chapter" data-level="" data-path="abstracts.html"><a href="abstracts.html#social-science-genres-today"><i class="fa fa-check"></i>Social Science Genres Today</a></li>
<li class="chapter" data-level="" data-path="abstracts.html"><a href="abstracts.html#the-social-science-citation-landscape-1900-1940"><i class="fa fa-check"></i>The Social Science Citation Landscape, 1900-1940</a></li>
<li class="chapter" data-level="" data-path="abstracts.html"><a href="abstracts.html#vocabularies-of-anthropology-and-sociology-1888-1922"><i class="fa fa-check"></i>Vocabularies of Anthropology and Sociology, 1888-1922</a></li>
<li class="chapter" data-level="" data-path="abstracts.html"><a href="abstracts.html#the-development-of-intensive-referencing"><i class="fa fa-check"></i>The Development of Intensive Referencing</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="int.html"><a href="int.html"><i class="fa fa-check"></i><b>1</b> Genre and the Literature</a><ul>
<li class="chapter" data-level="1.1" data-path="int.html"><a href="int.html#what-to-read"><i class="fa fa-check"></i><b>1.1</b> What to read?</a><ul>
<li class="chapter" data-level="1.1.1" data-path="int.html"><a href="int.html#genres"><i class="fa fa-check"></i><b>1.1.1</b> Genres</a></li>
<li class="chapter" data-level="1.1.2" data-path="int.html"><a href="int.html#disciplines"><i class="fa fa-check"></i><b>1.1.2</b> Disciplines</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="int.html"><a href="int.html#method"><i class="fa fa-check"></i><b>1.2</b> Method</a><ul>
<li class="chapter" data-level="1.2.1" data-path="int.html"><a href="int.html#distant-sampling"><i class="fa fa-check"></i><b>1.2.1</b> Distant sampling</a></li>
<li class="chapter" data-level="1.2.2" data-path="int.html"><a href="int.html#no-cigar"><i class="fa fa-check"></i><b>1.2.2</b> No cigar</a></li>
<li class="chapter" data-level="1.2.3" data-path="int.html"><a href="int.html#topic-models"><i class="fa fa-check"></i><b>1.2.3</b> Topic Models</a></li>
<li class="chapter" data-level="1.2.4" data-path="int.html"><a href="int.html#qualitative-cross-validation"><i class="fa fa-check"></i><b>1.2.4</b> Qualitative Cross Validation</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="int.html"><a href="int.html#data"><i class="fa fa-check"></i><b>1.3</b> Data</a></li>
<li class="chapter" data-level="1.4" data-path="int.html"><a href="int.html#estimation"><i class="fa fa-check"></i><b>1.4</b> Estimation</a></li>
<li class="chapter" data-level="1.5" data-path="int.html"><a href="int.html#results"><i class="fa fa-check"></i><b>1.5</b> Results</a></li>
<li class="chapter" data-level="1.6" data-path="int.html"><a href="int.html#discussion"><i class="fa fa-check"></i><b>1.6</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="gen.html"><a href="gen.html"><i class="fa fa-check"></i><b>2</b> Social Science Genres Today</a><ul>
<li class="chapter" data-level="2.1" data-path="gen.html"><a href="gen.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a><ul>
<li class="chapter" data-level="2.1.1" data-path="gen.html"><a href="gen.html#taste"><i class="fa fa-check"></i><b>2.1.1</b> Taste</a></li>
<li class="chapter" data-level="2.1.2" data-path="gen.html"><a href="gen.html#structuralist"><i class="fa fa-check"></i><b>2.1.2</b> Structuralist</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="gen.html"><a href="gen.html#culture"><i class="fa fa-check"></i><b>2.2</b> Culture</a></li>
<li class="chapter" data-level="2.3" data-path="gen.html"><a href="gen.html#genre-profession-and-cultural-morphodynamics"><i class="fa fa-check"></i><b>2.3</b> Genre, Profession, and Cultural Morphodynamics</a><ul>
<li class="chapter" data-level="2.3.1" data-path="gen.html"><a href="gen.html#generic-vs-typical"><i class="fa fa-check"></i><b>2.3.1</b> Generic vs Typical</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="gen.html"><a href="gen.html#kd-dq1"><i class="fa fa-check"></i><b>2.4</b> JSTOR Journals</a></li>
<li class="chapter" data-level="2.5" data-path="gen.html"><a href="gen.html#network-mode-projection"><i class="fa fa-check"></i><b>2.5</b> Network Mode Projection</a></li>
<li class="chapter" data-level="2.6" data-path="gen.html"><a href="gen.html#network-community-detection"><i class="fa fa-check"></i><b>2.6</b> Network Community Detection</a></li>
<li class="chapter" data-level="2.7" data-path="gen.html"><a href="gen.html#network-visualization"><i class="fa fa-check"></i><b>2.7</b> Network Visualization</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="cit.html"><a href="cit.html"><i class="fa fa-check"></i><b>3</b> The Social Science Citation Landscape, 1900-1940</a><ul>
<li class="chapter" data-level="3.1" data-path="cit.html"><a href="cit.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="cit.html"><a href="cit.html#scholarly-communication-vs-knowledge-terrain"><i class="fa fa-check"></i><b>3.1.1</b> Scholarly Communication vs Knowledge Terrain</a></li>
<li class="chapter" data-level="3.1.2" data-path="cit.html"><a href="cit.html#wok"><i class="fa fa-check"></i><b>3.1.2</b> Mapping Knowledge Terrain</a></li>
<li class="chapter" data-level="3.1.3" data-path="cit.html"><a href="cit.html#disciplines-as-a-large-world-co-reference-network"><i class="fa fa-check"></i><b>3.1.3</b> Disciplines as a Large World Co-reference Network</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="cit.html"><a href="cit.html#methods"><i class="fa fa-check"></i><b>3.2</b> Methods</a></li>
<li class="chapter" data-level="3.3" data-path="cit.html"><a href="cit.html#data-1"><i class="fa fa-check"></i><b>3.3</b> Data</a></li>
<li class="chapter" data-level="3.4" data-path="cit.html"><a href="cit.html#results-1"><i class="fa fa-check"></i><b>3.4</b> Results</a><ul>
<li class="chapter" data-level="3.4.1" data-path="cit.html"><a href="cit.html#continents"><i class="fa fa-check"></i><b>3.4.1</b> Continents</a></li>
<li class="chapter" data-level="3.4.2" data-path="cit.html"><a href="cit.html#peaks"><i class="fa fa-check"></i><b>3.4.2</b> Peaks</a></li>
<li class="chapter" data-level="3.4.3" data-path="cit.html"><a href="cit.html#valleys"><i class="fa fa-check"></i><b>3.4.3</b> Valleys</a></li>
<li class="chapter" data-level="3.4.4" data-path="cit.html"><a href="cit.html#do-reference-lists-describe-author-knowledge"><i class="fa fa-check"></i><b>3.4.4</b> Do reference lists describe author knowledge?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="voc.html"><a href="voc.html"><i class="fa fa-check"></i><b>4</b> Vocabularies of Anthropology and Sociology, 1888-1922</a><ul>
<li class="chapter" data-level="4.1" data-path="voc.html"><a href="voc.html#kd"><i class="fa fa-check"></i><b>4.1</b> Knowledge Development</a></li>
<li class="chapter" data-level="4.2" data-path="voc.html"><a href="voc.html#kd-dq2"><i class="fa fa-check"></i><b>4.2</b> Social Science Journals</a></li>
<li class="chapter" data-level="4.3" data-path="voc.html"><a href="voc.html#topics-ideas"><i class="fa fa-check"></i><b>4.3</b> Topics ≟ Ideas</a></li>
<li class="chapter" data-level="4.4" data-path="voc.html"><a href="voc.html#kd-lit"><i class="fa fa-check"></i><b>4.4</b> Prior Work</a></li>
<li class="chapter" data-level="4.5" data-path="voc.html"><a href="voc.html#information"><i class="fa fa-check"></i><b>4.5</b> Information</a></li>
<li class="chapter" data-level="4.6" data-path="voc.html"><a href="voc.html#meaning"><i class="fa fa-check"></i><b>4.6</b> Meaning</a></li>
<li class="chapter" data-level="4.7" data-path="voc.html"><a href="voc.html#communication"><i class="fa fa-check"></i><b>4.7</b> Communication</a></li>
<li class="chapter" data-level="4.8" data-path="voc.html"><a href="voc.html#full-text"><i class="fa fa-check"></i><b>4.8</b> Full-Text</a></li>
<li class="chapter" data-level="4.9" data-path="voc.html"><a href="voc.html#kd-dd"><i class="fa fa-check"></i><b>4.9</b> Data</a></li>
<li class="chapter" data-level="4.10" data-path="voc.html"><a href="voc.html#kd-dp1"><i class="fa fa-check"></i><b>4.10</b> Sampling</a></li>
<li class="chapter" data-level="4.11" data-path="voc.html"><a href="voc.html#units-of-analysis"><i class="fa fa-check"></i><b>4.11</b> Units of Analysis</a></li>
<li class="chapter" data-level="4.12" data-path="voc.html"><a href="voc.html#kd-dp2"><i class="fa fa-check"></i><b>4.12</b> Topics</a><ul>
<li class="chapter" data-level="4.12.1" data-path="voc.html"><a href="voc.html#how-many-topics"><i class="fa fa-check"></i><b>4.12.1</b> How many topics?</a></li>
</ul></li>
<li class="chapter" data-level="4.13" data-path="voc.html"><a href="voc.html#model-selection"><i class="fa fa-check"></i><b>4.13</b> Model selection</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ten.html"><a href="ten.html"><i class="fa fa-check"></i><b>5</b> The Development of Intensive Referencing</a><ul>
<li class="chapter" data-level="5.1" data-path="ten.html"><a href="ten.html#method-1"><i class="fa fa-check"></i><b>5.1</b> method</a></li>
<li class="chapter" data-level="5.2" data-path="ten.html"><a href="ten.html#lit-review"><i class="fa fa-check"></i><b>5.2</b> lit review</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/brooksambrose" target="blank">GitHub</a></li>
<!--<li><a href="https://www.ischool.berkeley.edu/people/brooks-ambrose" target="blank">UC Berkeley</a></li>-->

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Knowledge of the U.S. Social Sciences</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="int" class="section level1">
<h1><span class="header-section-number">1</span> Genre and the Literature</h1>
<div id="abstract" class="section level4 unnumbered">
<h4>Abstract</h4>
<p>There are many different analytic approaches to the phenomenon of genre classification of cultural products. This study explores how the use of the term genre varies across a wide swath of humanities and social science publications. I use computational text analysis to classify articles in the JSTOR archive into groups of common vocabulary. I use these groups as strata for a sampling approach to content analysis of the “entire” corpus of literature using the term genre. Through a combination of machine distant reading and human close reading, I arrive at a theory of five genres of the term genre. In an argument for pandisciplinarity, I conclude with an analysis of the logical possibility of new metaconcepts of genre that satisfy the strictures of multiple genres.</p>
</div>
<div id="keywords" class="section level4 unnumbered">
<h4>Keywords</h4>
<p>genre, disciplines, computational text analysis, topic modeling, content analysis, digital humanities, distant reading</p>
</div>
<div id="what-to-read" class="section level2">
<h2><span class="header-section-number">1.1</span> What to read?</h2>
<p>The question of what to read is simple to be sure, but in fields of scholarly consumption and production it is nonetheless fundamental. Scholarship is a creative profession where a stock of cultural knowledge forms a greater part of the infrastructure of production than in other fields. This is not to say that other occupations, especially manual ones, lack creativity. It is to say that in such fields knowledge has a limited infrastructure. Whereas the know-how of the brick layer is black boxed in her tools and technology and in the human capital she develops by experience and tacit social learning, for the scholar as bricoleur there exists in addition the distinctively overdeveloped feature of cultural archiving as a universal memory. Except perhaps in outstanding feats of primary research, contributions to scholarship are legitimate to the extent that they have used the archive correctly.</p>
<p>This problem of using the archive, by which we mean all libraries and other organizations that help scholars find published work, is easily expressed by the question, “what to read?” Paradoxically, the overdevelopment of the archive promotes a functional imperative: to the extent that more and more of scholarship is memorable, mechanisms must develop to forget large swaths of intellectual history. A person who studied a random draw from the archive, even a monumental one, would no doubt qualify as an educated person. Professionally, however, they would have answered the question in a tragically wrong way. From the perspective of other scholars, there are right and wrong choices about what to read. Because it is so easy to access scholarly memory, the operative question really becomes “what not to read?”</p>
<p>Though Internet search and self-publishing services, especially video and image based ones, are creating archive-like infrastructure for all occupations, even manual ones, the functions are different. Contemporary Internet repositories provide knowledge as factors of production to anyone who queries them, but many do not purport to be archives in the sense that a historical record of cultural produce is preserved for posterity. They are much more concerned with access to contemporaneous than to historical material, and indeed the particular configuration of the contemporary that sells the most ads ahead of search results. True historical archives of the Internet, such as the Internet Archive or Common Crawl, are not used by the public. Indeed why would they be; they expose the dizzying complexity of the history of the Internet, which, even in only its contemporaneous facet is already overwhelming. The Internet searcher tends to be satisficing, and the search companies have refined their ranking of results to meet their users’ search budgets efficiently.</p>
<p>Thus Internet search services perform the function of complexity reduction in their own arbitrary way. They do this without the scholarly paradox of memory, which is that in the university system great pains are made to remember everything just so that the correct material may be forgotten. In the cynical view of professions, scholarship is the encryption of memory by secret sets. A lay seeker approaches the academic archive and at great cost of attention plumbs its depths for enlightenment. Tragically, the archive’s complexity dooms her to check out a curriculum so hopelessly tacky that it will only certify her lay status. To be professional is to know what are the tasteful combinations of resources. To be a successful professional is to never have wasted time tasting the bad fruit. Librarians much prefer to help undergraduates because they lack taste. They gifted scholars with access to an immortal memory, and looking the horse in the mouth scholars made rules to protect themselves from the responsibilities of using most of it. In this way a taste for scholarship is the axis sorting the field between education and profession.</p>
<p>So again, how do professional scholars know what (not) to read? What then are the structures that lead scholars new and old to answer the question correctly? How does one know what are the lucrative curricula that can be developed from the archive? There are several formal and informal structures that facilitate and compel scholars to make the same choices about what to read. An obvious one is the supposed normative isomorphism of graduate program syllabi, yet it is a common concern that the quality of these are variable. Universities tend to grant great autonomy to professors in writing syllabi, who in the course of their professional travails may not be given opportunities to read what they want. In being forced to carve out time with subordinates, faculty are caught between personal indulgence and a more or less strongly felt fiduciary responsibility to set students on the correct path. If we have less than perfect faith in the strength of educational ethics among faculty, then we should expect that among graduate syllabi are many lists of what not to read. Students who trust too much in the formal curriculum may be lead astray, and even without trust, they may still be left ignorant of where to invest their labor.</p>
<p>In each program there then must be a hidden curriculum of higher quality. The argument of this study addresses the question of where such a curriculum could possibly come from. The provisional answer is that in the informal spaces of graduate programs knowledge of scholarly genre is learned from extracurricular engagement with professional conferences. It is in conference programs that the tacit rules of academic genre are learnable. These genres form the first parsing of the archive for neophytes. Indeed at the most generic level graduate students, if they are confident enough to locate themselves quickly enough, develop a taste for what not to read. If they can do so early on in their careers, they will be armed with the stereotypes necessary to stop reading the wrong and start reading the right material. While this is not enough certainly to make a cleric of a lay worshiper, it is a necessary first step.</p>
<div id="genres" class="section level3">
<h3><span class="header-section-number">1.1.1</span> Genres</h3>
<p>I take genre as a candidate explicans for the ability of scholars to know what to read and what to avoid from the cultural archive. A theory of genre will benefit from a review of the literature, yet to do so would catch me in the conundrum of performing the phenomenon I wish to explain. The genre structure of sociology should guide me to a definition of genre, a statement that already presumes an ontological difference and morphological relation between disciplines and genres, namely that scholarly genres are not equivalent to scholarly disciplines and that the former are located within the later. I will begin with an unstudied attempt to tease out the relation of discipline and genre before turning to a more rigourous, even empirical, treatment of genre as a term in American scholarship.</p>
<p>Genre is a loanword from French. The origin of the French-Latin word “genre” and the English-German word “kind” both mean membership by inheritance of innate class characteristics, archaically by presumptive blood descent within a family, race, or nation. In common English it is restricted to mean a broad category of art, especially literature and music, and some but not all other cultural fields (e.g. baseball is not a genre of sports). As a term in scholarship, genre may be an observable phenomenon, a conceptual component of a theory, or a conflation of the two. In the social sciences genre is a specialty concept as in sociology, while in the humanities it is ubiquitous especially in cultural studies. Academics define and use the term differently between and within disciplines.</p>
<p>Figure <a href="int.html#fig:ttsgnr">1.1</a> shows the count of mentions of the term genre in the Google Books Ngram database for English terms <span class="citation">(Michel et al. <a href="#ref-Michel2011Quantitative" role="doc-biblioref">2011</a>)</span>. The trend exhibits the typical take-off in publishing in the second half of the twentieth century. I apply change point analysis, which detects significant differences in time series data <span class="citation">(James and Matteson <a href="#ref-James2019ecp" role="doc-biblioref">2019</a>; Matteson and James <a href="#ref-Matteson2013Nonparametric" role="doc-biblioref">2013</a>)</span>, to the second difference of the trend, a measure of acceleration, to get clues as to whether the trend is a single process or whether there are inflection points. The first segment of the curve from 1899 to 1984 indicates a period of positive acceleration or quickening of the growth trend. On average in the first period the rate of change from one year to the next increased by a modest 13.3 occurences a year. However, during the period from 1985 to 2008 the rate of change, though always steep, began to decline by an average of 238.3 occurences a year. Like a projectile that is simultanously climbing and falling, 1984 acts as launch point of precipitous yet unsustainable growth.</p>
<div class="figure" style="text-align: center"><span id="fig:ttsgnr"></span>
<img src="ambrose_dissertation_files/figure-html/ttsgnr-1.svg" alt="Absolute count of term &quot;genre&quot;, 1901-2008. Segments correspond to significantly different second derivatives." width="672" />
<p class="caption">
Figure 1.1: Absolute count of term “genre”, 1901-2008. Segments correspond to significantly different second derivatives.
</p>
</div>
<p>Figure <a href="int.html#fig:ttsgnr2">1.2</a> shows a similar trend but using relative frequencies instead of absolute counts. Here “genre” is plotted as its share of all terms in the corpus. This trend exhibits no inflection point at 1984 that is statistically significant, and visually the trend does appear the same on both sides. No other inflections points are detectable due likely to greater year over year variability in this series in the first half of the century, reducing confidence in any estimate of a change point. To interpret this difference in statistical significance between relative and absolute measures would indicate that interest in genre continued to grow even within a secular slow-down in the volume of texts that resembles the familiar S-shaped diffusion curve. Alternatively, on visual inspection of the relative curve it appears that indeed there is an inflection point, just one a decade later in 1996. After this point the relative frequency seems to drop rapidly, a change that would no doubt be picked up statistically after a few more years of data and one that may in fact be located a few years earlier than the peak suggests. Together these trends describe a career to the term genre that has been strong for a century and that may now be in decline.</p>
<div class="figure" style="text-align: center"><span id="fig:ttsgnr2"></span>
<img src="ambrose_dissertation_files/figure-html/ttsgnr2-1.svg" alt="Relative frequency of term &quot;genre&quot;, 1901-2008." width="672" />
<p class="caption">
Figure 1.2: Relative frequency of term “genre”, 1901-2008.
</p>
</div>
<p>Figure <a href="int.html#fig:genre-goog">1.3</a> gives an indication of what things the term genre has been used to describe. It illustrates the frequency of terms appearing in the Google Books Ngram corpus as the third term in the trigrams beginning with “genre of” or “genres of” <span class="citation">(Google <a href="#ref-2012Google" role="doc-biblioref">2012</a>)</span>. The size of the words is proportional to the total frequency of the trigram in the English corpus, which spans centuries from 1590 to 2008. The bias of the source–books–is clear in the outsized importance of “literature” and “writing” which are followed closely by “music”. The next ten largest nouns are poetry, discourse, fiction, art, autobiography, painting, film, romance, folklore, and history. Ranked within that series would be several adjectives as well: popular, science (fiction), historical, and literary. Each of these terms refers to a field of concrete cultural products, with the exception of “discourse” which is more abstract.</p>
<div class="figure" style="text-align: center"><span id="fig:genre-goog"></span>
<div id="htmlwidget-23cad7345f4f83d1b39b" style="width:672px;height:480px;" class="wordcloud2 html-widget"></div>
<script type="application/json" data-for="htmlwidget-23cad7345f4f83d1b39b">{"x":{"word":["literature","writing","music","poetry","discourse","popular","fiction","science","art","historical","autobiography","painting","film","romance","folklore","literary","history","tragedy","oral","comedy","narrative","drama","travel","political","folk","children","social","books","television","biography","prose","texts","religious","research","epic","cultural","fantasy","opera","modern","contemporary","speech","performance","traditional","women","medieval","horror","detective","landscape","classical","song","communication","dance","satire","portraiture","self","work","academic","composition","stories","melodrama","early","public","written","romantic","short","black","entertainment","ancient","programming","personal","choice","novel","crime","scientific","expression","documentary","lyric","legal","biblical","pastoral","theatre","verbal","rhetoric","language","historiography","dramatic","rock","representation","philosophical","photography","life","criticism","verse","still","war","pornography","urban","news","love","storytelling","spiritual","media","medical","poetic","journalism","mass","domestic","realism","comic","creative","games","feminist","artistic","commentary","oratory","instrumental","movies","hagiography","myth","apocalyptic","visual","theater","revelatory","scholarship","ritual","sacred","talk","high","cinema","fairy","video","local","vocal","family","action","female","pop","ethnographic","autobiographical","studies","human","tragicomedy","late","national","heroic","advertising","adventure","sentimental","erotic","computer","mystery","secular","wisdom","argument","power","scholarly","moral","nonfiction","nature","letters","imaginative","knowledge","plays","elegy","speaking","silence","devotional","material","memoir","didactic","anti","poems","chamber","humor","colonial","theological","spoken","software","nineteenth","non","praise","philosophy","vernacular","advice","tales","ethnography","reading","rap","light","farce","everyday","apocalypse","school","jazz","prophetic","sculpture","western","antiquity","prayer","dialogue","post","radio","theatrical","reality","information","thought","remarriage","electronic","intellectual","commercial","one","portrait","biographical","courtly","rabbinic","filmmaking","realistic","country","technical","world","analysis","professional","reportage","prophecy","utopian","serious","soap","business","allegory","reporting","new","gay","inquiry","print","law","military","anthropological","interpretation","programme","great","essay","formal","lament","images","liturgical","educational","interest","male","documents","architecture","eighteenth","testimonio","hip","sports","fantastic","jokes","city","animal","today","conversation","official","lesbian","slave","gospel","pictorial","magical","production","parody","confession","genres","church","folktale","interaction","industrial","sung","symphony","protest","singing","revenge","magazine","young","choral","situation","instruction","economic","utopia","speculative","collective","twentieth","descriptive","newspaper","content","explanation","pseudo","pre","universal","adult","experience","community","sexual","psychological","prison","confessional","cyberpunk","testimony","socialist","spectacle","particular","picture","royal","legend","symbolic","laughter","case","blues","international","midrash","hard","qualitative","many","old","theory","diary","criminal","first","sources","organizational","thinking","illustrated","parable","shamisen","presentation","pure","true","ballet","resistance","digital","individual","tragic","belles","fine","journalistic","mystical","preaching","postmodern","epistolary","small","governance","rural","funeral","long","chivalric","regional","martial","environmental","people","imagery","activity","stage","aesthetic","men","tourist","fable","dream","piano","student","character","complaint","indigenous","experimental","private","mainstream","bourgeois","theoretical","building","middle","sociological","imperial","caricature","billingsgate","grand","recent","spy","immigrant","epideictic","saints","show","critique","online","battle","ekphrasis","southern","evidence","style","apologia","apologetic","auto","heavy","nonsense","mixed","questions","translation","gothic","captivity","pulp","revolutionary","different","textual","writers","problems","sex","models","investigative","miracle","propaganda","general","imaginary","pantomime","white","ethnic","panegyric","feature","gangster","humour","low","sea","topographical","design","youth","teen","manga","consolation","course","solo","comparative","leadership","teaching","violence","treatises","mythological","sensation","motion","publishing","patriotic","various","group","alternative","cinematic","articles","tragi","ballad","ghost","similar","ethical","sermons","native","drawing","fashion","juvenile","neo","travelogue","graphic","fan","woman","monastic","violent","space","free","quasi","novelistic","special","polemical","radical","flower","gossip","real","conduct","policy","cartoon","suspense","mediaeval","girls","postwar","scenes","chant","later","development","photographic","state","ecclesiastical","sf","adolescent","campaign","home","sorts","allegorical","disaster","burlesque","hymn","food","symphonic","mock","operetta","oratorio","missionary","presidential","us","dystopian","humanist","future","broadcasting","creation","street","corporate","learning","abstract","ukiyo","brief","porn","sci","manuals","supernatural","becoming","miniature","screwball","periodical","epigram","reggae","teacher","avant","progressive","semi","earlier","postcolonial","communal","mid","frontier","decorative","considerable","debate","institutional","cheap","exploration","sayings","celebrity","seventeenth","mathematical","data","large","boys","proverbs","encomium","serial","death","time","teenage","improvised","legislation","orchestral","practical","called","foreign","occasional","exegesis","independent","childhood","pornographic","wedding","ideas","soft","garden","murder","combat","conflict","figure","exotic","conventional","peasant","intimate","textbooks","proletarian","live","costume","ads","objects","police","eulogy","deliberative","heterosexual","race","consumer","historiographic","espionage","sophisticated","pieces","memory","archaeological","nude","escapist","management","chronicle","organization","worship","systematic","dialect","tourism","gangsta","classroom","monumental","little","factual","geographical","concert","extended","visionary","noir","vision","statistical","either","child","much","wartime","pious","big","literacy","genealogy","expository","secondary","psalms","preference","thriller","empirical","apology","exemplary","civic","installation","feminine","body","reference","nostalgia","response","pilgrimage","global","imitation","discussion","racist","photo","reform","health","truth","scriptural","linguistic","marine","things","polyphonic","limited","slasher","dubious","edifying","amateur","current","associated","physical","used","cross","ordinary","operatic"],"freq":[17117,13462,13757,7918,6594,6438,5591,4410,4192,4081,3626,3912,5133,2843,2786,2754,2755,2566,2425,2316,2777,2300,2285,2343,2150,1885,1872,2258,1763,1818,1701,3193,1546,1509,1494,1589,1443,1410,1502,1390,1360,1469,1486,1292,1282,1275,1298,1199,1349,1934,1142,1089,1281,1022,1016,1753,985,1011,1779,954,934,1366,931,919,908,894,890,885,1169,836,830,1333,808,802,916,782,905,776,763,759,754,739,930,707,684,675,660,659,658,656,645,1037,629,620,619,618,605,597,595,593,591,584,579,565,597,552,532,500,584,484,849,482,638,552,469,467,645,464,565,462,460,458,454,454,451,448,437,534,434,433,433,431,431,430,416,415,413,412,409,552,407,406,403,400,399,443,391,390,386,381,380,377,376,449,371,366,364,364,631,663,361,360,668,356,355,354,353,474,583,347,344,491,338,528,335,487,332,330,329,328,325,324,324,323,469,315,313,312,310,310,310,309,407,305,303,301,353,299,298,293,285,285,283,279,341,275,274,271,269,268,266,337,263,487,255,254,347,252,251,250,247,247,246,244,244,244,240,239,236,289,232,228,228,457,227,226,226,226,358,225,308,223,364,311,219,266,216,210,277,374,206,200,200,242,198,244,193,313,189,286,187,181,179,178,177,282,340,174,174,228,172,246,292,172,171,171,170,170,168,290,163,161,160,248,160,159,159,200,158,224,157,157,156,155,155,152,152,152,150,149,149,149,148,147,234,146,146,146,232,144,220,143,142,263,142,202,141,140,139,137,137,214,136,135,134,193,134,133,187,132,132,131,130,130,129,128,128,126,126,125,125,124,124,124,123,123,123,121,121,120,120,119,119,119,119,118,118,160,116,116,115,115,170,115,114,114,112,112,112,110,110,109,108,108,107,107,106,106,106,105,105,105,105,104,104,103,102,102,101,100,99,99,99,99,98,98,97,97,96,96,157,95,95,95,95,95,94,94,93,166,93,155,92,92,92,92,92,92,91,91,89,89,89,88,88,88,88,87,87,87,86,86,85,85,85,85,85,85,85,84,144,84,168,83,83,83,82,82,131,81,140,80,79,79,157,79,79,79,78,78,78,78,77,77,76,76,76,76,76,75,75,130,73,73,72,72,72,72,129,71,71,71,71,71,71,71,71,138,71,70,70,70,70,70,70,70,69,69,131,69,69,68,68,68,67,66,66,66,66,66,65,65,65,65,65,64,64,64,64,64,64,64,64,63,63,63,63,63,62,62,62,62,62,62,62,61,61,104,61,61,60,60,60,60,60,60,60,60,59,59,59,59,59,59,59,59,59,59,58,58,58,58,58,58,58,58,57,57,57,56,56,56,56,56,56,56,55,55,55,100,55,55,55,55,54,54,54,54,53,53,53,53,53,52,52,52,52,52,51,51,98,51,51,51,51,50,50,50,50,50,50,50,49,49,49,49,49,49,48,48,48,48,48,48,47,47,47,47,47,46,46,46,46,46,46,46,46,46,46,45,45,45,45,45,44,44,44,44,43,43,43,43,43,43,42,42,42,42,42,42,42,41,41,41,40,40,40],"fontFamily":"Times","fontWeight":"normal","color":["#00204DFF","#2E436CFF","#28406CFF","#787877FF","#898679FF","#8B8779FF","#979178FF","#A79D75FF","#AA9F75FF","#ACA174FF","#B2A672FF","#AEA374FF","#9D9677FF","#BDAF6FFF","#BEAF6FFF","#BFAF6EFF","#BFAF6EFF","#C1B26DFF","#C3B46DFF","#C4B56CFF","#BEAF6FFF","#C5B56CFF","#C5B56CFF","#C4B56CFF","#C7B76BFF","#CBBA69FF","#CCBA69FF","#C5B56CFF","#CDBC68FF","#CCBB69FF","#CEBC68FF","#B8AB70FF","#D0BE67FF","#D0BE67FF","#D0BE67FF","#CFBD67FF","#D1BF66FF","#D2C066FF","#D0BE67FF","#D2C066FF","#D2C066FF","#D1BF66FF","#D1BF66FF","#D3C165FF","#D3C165FF","#D3C164FF","#D3C165FF","#D5C264FF","#D2C066FF","#CABA69FF","#D6C364FF","#D7C463FF","#D3C165FF","#D8C563FF","#D8C563FF","#CDBC68FF","#D8C563FF","#D8C563FF","#CDBC68FF","#D9C562FF","#D9C562FF","#D2C066FF","#D9C562FF","#D9C562FF","#D9C562FF","#DAC561FF","#DAC561FF","#DAC661FF","#D6C364FF","#DAC661FF","#DAC661FF","#D3C165FF","#DBC761FF","#DBC761FF","#D9C562FF","#DBC761FF","#D9C562FF","#DBC761FF","#DBC761FF","#DBC761FF","#DBC761FF","#DCC861FF","#D9C562FF","#DCC860FF","#DCC860FF","#DCC85FFF","#DDC95FFF","#DDC95FFF","#DDC95FFF","#DDC95FFF","#DDC95FFF","#D8C563FF","#DDC95FFF","#DDC95FFF","#DDC95FFF","#DDC95FFF","#DDC95FFF","#DDCA5FFF","#DDCA5FFF","#DDCA5FFF","#DDCA5FFF","#DECA5FFF","#DECA5FFF","#DECA5FFF","#DDCA5FFF","#DECA5FFF","#DFCA5FFF","#E0CB5EFF","#DECA5FFF","#E0CB5EFF","#DAC661FF","#E0CB5EFF","#DDC95FFF","#DECA5FFF","#E0CB5EFF","#E0CB5EFF","#DDC95FFF","#E0CB5EFF","#DECA5FFF","#E0CB5EFF","#E0CB5EFF","#E0CB5EFF","#E1CB5EFF","#E1CB5EFF","#E1CB5EFF","#E1CC5DFF","#E1CC5DFF","#DFCA5FFF","#E1CC5DFF","#E1CC5DFF","#E1CC5DFF","#E1CC5DFF","#E1CC5DFF","#E1CC5DFF","#E1CC5DFF","#E1CC5DFF","#E1CC5DFF","#E1CC5DFF","#E1CC5DFF","#DECA5FFF","#E1CC5DFF","#E1CC5DFF","#E1CC5DFF","#E1CC5DFF","#E1CC5DFF","#E1CC5DFF","#E1CC5DFF","#E1CC5DFF","#E1CC5DFF","#E1CC5DFF","#E1CC5DFF","#E1CC5CFF","#E1CC5CFF","#E1CB5EFF","#E2CD5CFF","#E2CD5CFF","#E2CD5CFF","#E2CD5CFF","#DDC95FFF","#DDC95FFF","#E2CD5CFF","#E2CD5CFF","#DDC95FFF","#E2CD5CFF","#E2CD5CFF","#E2CD5CFF","#E2CD5CFF","#E0CB5EFF","#DECA5FFF","#E2CD5CFF","#E2CD5CFF","#E0CB5EFF","#E2CD5CFF","#DFCA5FFF","#E2CD5CFF","#E0CB5EFF","#E2CD5CFF","#E2CD5CFF","#E2CD5CFF","#E2CD5CFF","#E2CD5CFF","#E2CD5CFF","#E2CD5CFF","#E2CD5CFF","#E0CB5EFF","#E2CD5CFF","#E2CD5CFF","#E2CD5CFF","#E2CD5CFF","#E2CD5CFF","#E2CD5CFF","#E2CD5CFF","#E1CC5DFF","#E2CD5CFF","#E2CD5CFF","#E2CD5CFF","#E2CD5CFF","#E3CE5CFF","#E3CE5CFF","#E3CE5CFF","#E3CE5CFF","#E3CE5CFF","#E3CE5CFF","#E3CE5CFF","#E2CD5CFF","#E3CE5CFF","#E3CE5CFF","#E3CE5CFF","#E3CE5CFF","#E3CE5CFF","#E3CE5CFF","#E2CD5CFF","#E3CE5CFF","#E0CB5EFF","#E3CE5CFF","#E3CE5CFF","#E2CD5CFF","#E3CE5CFF","#E3CE5CFF","#E3CE5CFF","#E3CE5CFF","#E3CE5CFF","#E3CE5CFF","#E3CE5CFF","#E3CE5CFF","#E3CE5CFF","#E3CE5CFF","#E3CE5CFF","#E3CE5CFF","#E3CE5CFF","#E3CE5CFF","#E3CE5CFF","#E3CE5CFF","#E0CB5EFF","#E3CE5CFF","#E3CE5CFF","#E3CE5CFF","#E3CE5CFF","#E2CD5CFF","#E3CE5CFF","#E2CD5CFF","#E4CF5CFF","#E2CD5CFF","#E2CD5CFF","#E4CF5CFF","#E3CE5CFF","#E4CF5BFF","#E4CF5BFF","#E3CE5CFF","#E1CC5CFF","#E4CF5BFF","#E4CF5BFF","#E4CF5BFF","#E3CE5CFF","#E4CF5BFF","#E3CE5CFF","#E4CF5BFF","#E2CD5CFF","#E4CF5BFF","#E3CE5CFF","#E4CF5BFF","#E4CF5BFF","#E4CF5BFF","#E4CF5BFF","#E4CF5BFF","#E3CE5CFF","#E2CD5CFF","#E4CF5BFF","#E4CF5BFF","#E3CE5CFF","#E4CF5BFF","#E3CE5CFF","#E3CE5CFF","#E4CF5BFF","#E4CF5BFF","#E4CF5BFF","#E4CF5BFF","#E4CF5BFF","#E4CF5BFF","#E3CE5CFF","#E4CF5BFF","#E4CF5BFF","#E4CF5BFF","#E3CE5CFF","#E4CF5BFF","#E4CF5BFF","#E4CF5BFF","#E4CF5BFF","#E4CF5BFF","#E4CF5CFF","#E4CF5BFF","#E4CF5BFF","#E4CF5BFF","#E4CF5BFF","#E4CF5BFF","#E5CF5BFF","#E5CF5BFF","#E5CF5BFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E3CE5CFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E3CE5CFF","#E5D05AFF","#E4CF5CFF","#E5D05AFF","#E5D05AFF","#E3CE5CFF","#E5D05AFF","#E4CF5BFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E4CF5BFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E4CF5BFF","#E5D05AFF","#E5D05AFF","#E4CF5BFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E4CF5BFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E4CF5BFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E4CF5BFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E4CF5BFF","#E5D05AFF","#E4CF5BFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E4CF5BFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D05AFF","#E5D059FF","#E5D05AFF","#E5D059FF","#E5D059FF","#E5D059FF","#E4CF5BFF","#E5D059FF","#E5D059FF","#E5D059FF","#E5D059FF","#E5D059FF","#E5D059FF","#E5D059FF","#E5D059FF","#E5D059FF","#E5D159FF","#E5D159FF","#E5D159FF","#E5D159FF","#E5D159FF","#E5D159FF","#E5D159FF","#E5D05AFF","#E5D159FF","#E5D159FF","#E5D159FF","#E5D159FF","#E5D159FF","#E5D159FF","#E5D05AFF","#E5D159FF","#E5D159FF","#E5D159FF","#E5D159FF","#E5D159FF","#E5D159FF","#E5D159FF","#E5D159FF","#E5D05AFF","#E5D159FF","#E5D159FF","#E5D159FF","#E5D159FF","#E5D159FF","#E5D159FF","#E5D159FF","#E5D159FF","#E5D159FF","#E5D159FF","#E5D05AFF","#E5D159FF","#E5D159FF","#E5D159FF","#E5D159FF","#E5D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E5D05AFF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E5D05AFF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E5D05AFF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF","#E6D159FF"],"minSize":0,"weightFactor":0.00893848221066776,"backgroundColor":"white","gridSize":1,"minRotation":-0.785398163397448,"maxRotation":0.785398163397448,"shuffle":false,"rotateRatio":0,"shape":"circle","ellipticity":0.65,"figBase64":null,"hover":null},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 1.3: Wordcloud of third term in 3gram beginning with “genre of”.
</p>
</div>
<p>Most items toward the top of the list are less popular (to write about) art forms like television, dance, and theater. Toward the middle of the list begin to appear adaptations of the term from the cultural to the social context. These include practical fields like medicine and journalism, political areas like law, government, and crime, and social arenas like identity and protest.</p>
<p>To impose a sociological gloss on the term, these varied uses of genre would make reviewing the literature on the topic difficult, were it not for the discipline structure of the academy. The uses of the term genre are themselves systematically organized by discipline, and a disciple who is adequately trained will know the correct ways to use the term in her local context. To use genre as a disciplinary convention means to first identify your location in the disciplinary field, and then to accept the limitations on scope by excluding those treatments of the term that are extradisciplinary, that is, irrelevant. Disciplinary structure reduces the true cultural complexity of the meaning of genre to a restricted form, which in turn allows humble knowledge workers to engage upon a set of shared assumptions. Such simplicity begets new complexity as disciples spin out the consequences of their local use of genre.</p>
<p>In the sociology of culture, genre is a form of classification enacted by people in various social contexts. In the context of industrial capitalism, genre is an economic principle helping to organize supply and demand within markets for cultural products. There actors see genres among the borders between economic, social, and cultural uses, as a market category helping them acquire or produce content, as a card in proximate games of prestige, or as something to taste, to consume and enjoy directly. Less often genre is knowledge, a component of culture separate from taste, that is a factor in the formation of ideas and skills, whether these lead in turn to economic production or not.</p>
<p>Genre’s meaning is context specific and variable across sociological subfields, though it is amplified in empiricist fashion by economy and society approaches that reduce genre to the act of classification itself. I say empiricist because of the empirical ease with which the classification or labeling actions are observed. Especially with Internet distribution systems, it becomes trivial for corporations to observe when a consumer labels her preferences while browsing a content catalog; it is much harder to observe the ideas that consuming a particular piece of content sparks in the mind.</p>
<p>In cultural studies genre is used much differently and much more in accordance with its etymology. To the humanist, genre is an ontological phenomenon, which is to say, genres are differentiated from each other by combinations of discrete features of signs and signifieds. Humanists are trained to establish these ontologies through methodological readings of texts and through cultivation of theories of genre types. These methods are at the same time empirical and interpretive, because in consuming objective texts the researcher actually observes the ideas that form in her own consciousness, and they may hope that others have a resonant experience. Genres have more substance to them for humanists than they do for sociologists. For sociologists of culture, genres are how people use genre labels, while for humanists, genres are knowledge.</p>
<p>As we have said, the meaning of genre for a sociologist of culture is economistic, at the same time a market category and a taste configuration for consumers. The term differs for a sociologist of knowledge, and perhaps for a cultural sociologist. It is ontology as it is for the humanist, however it is not the ontology as represented to the researcher in the consumption of texts. It is the hopelessly unobservable distribution of ontologies appearing in a population of consumers attending to similar texts. The sociologist of knowledge accuses the sociologist of culture of reducing internalist concepts, thought and experience, to externalist ones, taste and preference. The sociologist of culture rejoins, show me proof, and on and on the interlocutors spin around the axis defining the boundary between their subfields.</p>
<p>But this description of subdisciplinary differences really is just an example of a structural theory of genre that is within scope for the sociologist of knowledge and beyond it for the sociologist of culture, due to their epistemological differences. If genres are ontological, then they deeply structure a person’s experience of reality. Ontologies form basic perceptual categories, and people with different ontologies of an object experience different things even if oriented to the same objective phenomenon. A ontological theory of genre would, for example, attempt to explain differences in taste as differences in phenomenological perception, whereas a taste theory of genre treats consumption behavior as revealing preferences whose downstream consequences are then explored.</p>
<p>Yet for all the differing treatments of genre mentioned above, do these views of really contradict, or are they in fact complementary? Does genre as distribution serve the economic sociologist’s goals, or does a lack of ontological substance lead her astray? Does genre as knowledge remain hopelessly unverifiable, or are theoretical constructs necessary to achieve a correct interpretation of facts? Is everyone at the club listening to the same song hearing the same thing?</p>
</div>
<div id="disciplines" class="section level3">
<h3><span class="header-section-number">1.1.2</span> Disciplines</h3>
<p>To return to disciplines, answering such questions would put the researcher in an adisciplinary predicament, for it would mean eschewing the scope restrictions cherished by disciples. The challenges of a meta-disciplinary analysis are manifold and uncertain. One is as likely to grow her audience as to lose it altogether. She risks the dilettantism of a jack-of-all-trades. What’s worse, she exposes herself to a dizzying scale of content to consider. If disciplines are indeed functional, then the meta-analysis that effaces disciplines risks being dysfunctional. The upside, however, is appealing. If the universe of meanings given to the term genre does contain complementary uses, a meta-analysis will allow one to consider the consequences of the now arbitrary segregation of a superior metaconcept across disciplinary boundaries. Both sides of the divide could be strengthened by a cultural exchange of their respective terms. The redundancy of parallel discovery can be avoided.</p>
<p>Pathologies of disciplinarity can be diagnosed and treated. Disciplines are social substructures embedded in a larger society and culture. Disciplinarity is a kind of controlled ignorance exchanged for access to secret knowledge. The wayward uses of a term like genre are always lurking at the edges of the firelight defined by a particular disciplinary camp. Discipline as rigor instructs disciples to resist flirtations with the available complexity of the term. The essential tension is very rarely between what is known and unknown; rather it is more commonly between what is known “here” versus what we are conditioned to be willfully ignorant of “there”.</p>
<p>In a motivation of some of the arguments to follow, I take a metadisciplinary approach, which is to cast as wide a net as possible on the term genre. In so doing I hope to test the tacit cultural assumption that discipline-based decisions of relevance are valid, that is, that when we exclude arguments from other disciplines we remove distractions and focus on what is important. The alternative possibility is that we are wasting intellectual resources, because to exclude important work about our topic, even if it is codified in foreign terms, is to risk ignorance and redundancy. The topic is genre and how disciplinary boundaries form such that people using the same word nonetheless cannot communicate effectively. They draw on different paradigms, which is to say the term is not really the same term. What I hope to do is uncover the knowledge contexts surrounding the terms, and map these contexts in a way that enumerates the various communities of discourse and theories constituting the term.</p>
</div>
</div>
<div id="method" class="section level2">
<h2><span class="header-section-number">1.2</span> Method</h2>
<p>As I have said, the first consequence of eschewing disciplinary limitations is to bloat the size of the “literature” on genre, since no uses of the term would be excluded. An empirical approach to the standard academic convention of a literature review will help reign in the scale and complexity of the task. My aim, however, remains practical rather than scientific. The methods need to be good enough to yield results that offer something new above a traditional literature review relying on library search and disciplinary wisdom about what is important. This is not because a scientific approach is undesirable, it is that it is not yet demanded of “the literature”. Sociologists are not expected to take a sociological orientation toward the history of their fields. Rather the literature review serves the social purpose of taking a position in a field of cultural production. It is a listing of a roster of political support and rivalry, and an advertisement to attract a desired audience.</p>
<p>To take an empirical approach to the literature review would be subversive were it not the first function of disciplinary genres to render atypical draws from the archive irrelevant. Disciplinary subfields, genres, are credentialed by secret sets of references, and most comers are held at the door. This in and of itself can be subersive of even more arbitrary club rules, namely those of educational pedigree, such that anyone willing to invest in a presentation of the genre definition will be granted access to the venues, if not the invisible colleges, of the subfield. To be admitted to the arena is no gaurantee of achievement within it, but it is a start. Nevertheless, the scale of the archive will always supply entropy enough to create a deterrent of flotsam and jetsam around subfields composed of projects and persons who either never cracked the code or who willfully eschewed it.</p>
<div id="distant-sampling" class="section level3">
<h3><span class="header-section-number">1.2.1</span> Distant sampling</h3>
<p>The research strategy here attempts to parry the entropic tendency of the archive by substituting human for machine limits. The methodological premise of a meta-analysis of genre is that the Gordian knot of the global cultural complexity of the archive can be cut by stratified sampling. I use a large digital archive of texts, JSTOR, to represent the whole of the academic archive. Though clearly a toy representing only a fraction of all networked scholarly produce, JSTOR is large enough to easily surpass individual cognition and compel the equivalent types of complexity reduction facing any researcher approaching the real archive via their local university library portal.</p>
<p>I use a simple term search of the keyword “genre” to define half of a sampling frame.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> I could then take a simple random sample of texts, analyze how each uses the term genre, develop a classification scheme, and enumerate the different uses of the term. Unfortunately, a small sample in a statistical sense may be larger than a poor researcher can handle. 1,000 texts is not large statistically, but it is huge from a content analysis perspective. What’s worse, 1,000 texts may still exclude, by random chance, small subcultures of the term. Stratification within a more or less global sampling frame resolves this issue by delineating those subcultures so none would be left out.</p>
<p>Alas, the JSTOR digital archive lacks subject labels at the article level, though it does include them for book chapters and for journals. While not foolish, inheriting a journal label to the articles included within it may be a coarse approximation if within-journal content variation exceeds between-journal variation. We can use text analytic classification methods to cluster articles directly and discover latent groups of articles, and in so doing we can have an independent standard to compare to the discipline labels given to journals. It is an open question whether such methods align with what we have discussed above as disciplinary and subdisciplinary groupings, for us whether regularities in vocabulary correspond to regularities in the meaning of the term genre. If they do not, then the study will only be a stop en route to a true census of the uses of the term genre, and the contribution will be to have interrogated the quality of the methods used, though this would be a small consolation indeed!</p>
<p>The choice in computational text analysis (CTA) about how to represent texts as data hinges on whether word order is preserved. The older and more tested approach is to not preserve word order. The name given to this “bag of words” format reminds one of its inelegance. A bag of words is a frequency table for each document counting up the number of times particular words are used, a representation that effectively reduces a text to its vocabulary. It is the analyst’s crude operational decision to treat vocabularies as indicators of meaning, but social scientists conventionally insist on cross validation via qualitative analysis. While the ambitions of computational text analysis may start with a replacement of, for instance, the standard literature review, the conventional distrust, at least in sociology, of mathematical models of text makes CTA more of a sampling method than an analytic method. The study will culminate in a reading of texts, albeit one that is different than traditional qualitative analysis because the CTA researcher welcomes the introduction of interpretive bias from an understanding of the mathematical model before, during, or after the texts are read. In the game of “choose your influence”, CTA is one choice while disciplinary wisdom is another.</p>
<p>There are two types of classification methods in text analysis, direct document clustering and topic modeling. Direct document clustering treats the bag of words as a vector space and calculates distance or similarity metrics between documents, which are then clustered. In a topic model, the relationship between documents is mediated by an unobserved but latently modeled representation of their content; documents are similar because they are formed from the same topics.</p>
<p>Whichever approach one takes, and both may be used, recall that the goal is to organize the texts into strata for the purpose of stratified sampling. We said that we wish to typify and enumerate the different uses of the term genre. By qualitative analysis, we could read every text in a simple random sample and come up with a theory of the use of genre in that text. The demerits of this approach are several <span class="citation">(c.f. Nelson <a href="#ref-Nelson2017Computational" role="doc-biblioref">2017</a>:5)</span>. It would take longer than we want even for too small a sample. We are not humanists and have not been trained in text analysis (this will hound us no matter what). Fatigue will set in, and accuracy and consistency will suffer. We may limit our set of theories to spare us the agony of complexity. It will be hard to reproduce our results. There may be path dependency with a different reading order producing different theories. On the upside, we would be more educated for it.</p>
<p>Instead, we will stratify the sample, and it is in the configuration of the strata that much of the work will be done. The strata impose upon our interpretation of the texts the assumption of sameness.</p>
</div>
<div id="no-cigar" class="section level3">
<h3><span class="header-section-number">1.2.2</span> No cigar</h3>
<p>The popular yet maligned distant reading approach taken by digital humanists <span class="citation">(e.g. Moretti <a href="#ref-Moretti2005Graphs" role="doc-biblioref">2005</a>)</span> is being taken up with gusto by social scientists who are less skeptical of quantitative methods <span class="citation">(e.g. DiMaggio, Nag, and Blei <a href="#ref-DiMaggio2013Exploiting" role="doc-biblioref">2013</a>)</span>. Following Nelson <span class="citation">(<a href="#ref-Nelson2017Computational" role="doc-biblioref">2017</a>)</span> I employ a quantitative analysis of texts not to replace human reading with machine reading but to support reproducibility in traditional qualitative content analysis. While CTA makes it possible to dispense with reading altogether, knowledge, understanding, and the cultural logics of arguments–especially their ontologies–are still only obtainable by reading primary texts, closely or not. The most radical interpretive CTA method would involve deep neural net supervised machine learning, which may be able to predict how a particular human reader would classify a text without their needing to read it, though this has never been demonstrated. What I gain from CTA is guidance in answering the question of what to read, and perhaps in what order to do so.</p>
<p>As we know, the question of what to read is answered institutionally for scholars already by way of canon, curriculum, word of mouth, and digital reference term search services. These are their own forms of distant reading, because they each make obsolete the archaic image, true of figures like Weber, of a scholar buried in library stacks reading everything they come across (and so it has been said of Weber, forgetting nothing).<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> These contemporary shortcuts are historically arbitrary, but what is important is first that they serve the function of reducing the overwhelming cognitive complexity of published scholarship, and second structure that reduction in the same way for all scholars. An arbitrary reduction needs to be consistent to act as an infrastructure for subdisciplinary scholarship, otherwise scholars would find themselves located in different literatures.</p>
<p>If distant reading is a criticism of close reading then it has a big hill to climb especially among humanists who are trained to deal methodically with texts very carefully. In the social sciences a type of customary distant reading is that of ritual citations, those that have developed a meaning that may be oblique to their content or at odds with the intentions of the the original authors. A ritual citation is simply one that is cited but not read, but also one that is so often used that its socially acceptable usages are known from other secondary accounts.</p>
<p>What are the social patterns of the traditional literature review are topics for the sociology of knowledge and science and for the information sciences. This is not the task of the current study. What we take from the traditional approach is the consequences of excluding large segments of intellectual history. What CTA makes possible for the first time is a nonarbitrary, inclusive analysis of <em>all</em> content in a digitized corpus. It will not necessarily be a good analysis, but what it will lack in quality it will make up for in coverage. A CTA approach to the literature review will at least make clear what lacuna would be left by the traditional approach. They also reduce the potential idiosyncrasy of a particular author’s literature review because, unlike a personal reading, a CTA model can be communicated precisely.</p>
<p>Of course the cognitive limitation of how much any scholar can actually read and understand remains. There will be an exclusion mechanism no matter what, therefore a chief assumption of a CTA literature review is that corpus segmentation is both possible and that some reduced form of reading, some sampling procedure, can be said to be representative of the unread portion in each segment. These representative texts will be subjected to a close reading, but their interpretation will be generalized to unread documents. Hence I call this a “no cigar” approach to reading, as in “close but”. If on the contrary to the assumption no two snowflakes are alike, then the enterprise of knowing more than we have before is fraught, and CTA becomes yet another arbitrary reducer.</p>
<p>What’s worse, or perhaps better, is that there is reason to believe that idiosyncrasy itself is an historically variable feature of disciplines. If institutional isomorphism has proceeded to some high level in contemporary disciplines, then the assumption that reading the bellwether texts is as good as reading the entire herd may hold. If this is true, however, it raises as many questions about the process of institutionalization in cultural production than it answers about the potential to learn truer versions of intellectual history.</p>
</div>
<div id="topic-models" class="section level3">
<h3><span class="header-section-number">1.2.3</span> Topic Models</h3>
<p>We have referred generically to computational text analysis, and now we can discuss the topic model as our technique of choice. There are many ways of estimating a topic model (e.g. the famous Latent Dirichlet Allocation or LDA estimator) but the model itself is simple. It is a latent variable model that decomposes a document-by-term matrix–in which every document is represented as a frequency distribution over every term appearing in the corpus–into two unobserved matrices:</p>
<ul>
<li>a topic-by-term matrix, and</li>
<li>a document-by-topic matrix.</li>
</ul>
<p>Topics are directly represented by they topic-by-term matrix. A topic is a probability distribution over a vocabulary. To draw on a topic means to choose vocabulary as a random draw from this distribution, where words with higher probabilities will be chosen more often. In the case of genres we might imagine a topic about film and a topic about music. Some words may be important (highly probable), to both topics, such as the word “genre”, while others would be distinct, such as the words “movie” (probable for film but improbable for music) and “band” (vice versa).</p>
<p>Given topics as term distributions, a document can be represented not as a distribution over terms, but as a distribution over topics. The topic mediates the relationship between documents and terms. In order to generate diction for a document, all that need be understood is the ratio of topics out of which it is composed. This is sometimes explained as a generative mechanism; to ask what word will be chosen next in composing a document, one first samples from the document’s own topic distribution to decide which topic the word will be drawn from, and given that topic, one then samples from the topic’s word distribution to decide which word will be included in the document. A document’s topic probabilities also create the expectation of how many words are attributed to eac topic. A document with topic probabilities .7 from music and .3 from film would be 70 percent about music and 30 percent about film, making for a parsimonious albeit reductive description of document content.</p>
<p>It is important not to overinterpret a topic model. Topic models are sometimes called “generative” as if they explained how documents are written. Such a generative metaphor reveals the absurdity of a topic model as a representation of writing. Not to mention the fact that punctuation tends not to be represented (though it could be), the terms chosen would be in a random order incapable of making meaningful sentences. Hence it is best to avoid the generative metaphor as an explanation of texts. If topic models touch on the generation of real, meaningful documents, it is only a very limited sense. What the topic model really represents is how vocabularies are organized to condition an author’s diction. A vocabulary can be thought of as an infrastructure of meaning more trivial than grammar or syntax. A topic is a simple list of words that is known or knowable across all authors in a field. Topics do not tell stories; authors tell stories in part by making diction choices that are conditioned by topics.</p>
<p>From a sense or meaning making perspective topics are trivial; this is because so little is known about what an author says by knowing the topic or even the term distribution of a document. What topics are useful for, however, is the type of segmentation or cartography of a corpus. Topics are really a global feature, perhaps a cultural feature, of a corpus of texts that is itself meaningfully selected. If indeed a field of texts is oriented to common if not always overlapping vocabularies, then topics can represent this well.</p>
<p>A topic model could be posited based on the domain knowledge of an expert, and this would be a form of estimation. The practical value of statistical topic modeling is that the unobserved topics can be induced, with a raft of statistical assumptions, directly from the observed document-by-term matrix to arrive at a model with the features just described. An estimated topic model will contain several other parameters filling in assumptions necessary to make it possible to identify the unobservable topic probabilities in each of the two matrices of the model. For instance, the parameter commonly called alpha makes an assumption about how many topics tend to comprise each document.</p>
<div id="choosing-k" class="section level4">
<h4><span class="header-section-number">1.2.3.1</span> Choosing K</h4>
<p>Finally, topic models require the analyst to choose the number of topics <span class="math inline">\(K\)</span>. The approach we take to guiding this decision is not to expect one correct specification of K but rather to see it as a changing resolution. A K=2 model usefully bifurcates the sample and is not simply wrong because it is too restrictive. As K increases we expect the samples to continue to divide as new parameter spaces become available to partition the sample. While this is not strictly a hierarchical design, since each K model is fit independently, we should expect to see aspects of hierarchical topics as well as some degree of stability in the relationships among topics.</p>
<p>Between model cross-validation means that document and term groupings should be relatively stable as K increases. The document overlaps between, say, a three topic model and a four topic model should not be random. By graphing the document overlaps between psuedo hierarchically organized models, it should be clear which topics are the most stable and which are constituted by chance. An ensemble approach would then recommend itself; if the content of a topic is stable across different specifications of K, within limits, then we should have even more confidence in that topic. Another interesting feature of this approach is that it shows when and how topics are able to appear given the parameter space constraints. We expect the most dominant topics, those that appear at low K and remain stable as K increases, to derive from the most semantically distinct documents. For sets of documents that are constituted by multiple true topics, we expect to see splitting of larger topics as the resolution increases to meet the real diversity. Hierarchy will reveal itself as topics with stronger topic signals subsume weaker ones until K reaches a point where there is enough space to separate them. On the other hand, in the classic tradeoff between variance and bias, where K overshoots the true number of topics, we expect to see random splitting and possibly “dust bin” effects where spare topics allow larger topics to prune their weaker term associations.</p>
<p>Presumably K can be set so high as to approach the saturation point of a topic for each document. In this event topics that would otherwise appear in common may alter to represent the uncommon parts of a document, and the topic would merely reproduce the term distribution of a particular document.</p>
</div>
<div id="bias" class="section level4">
<h4><span class="header-section-number">1.2.3.2</span> Bias</h4>
<p>Before documenting the data preparation below, it is important to keep in mind several sampling and modeling considerations that tend to be overlooked. First, idiosyncracy is assumed to be unmodelable. A flaw of traditional topic models is that, at one level, all documents are generic. Originality exists only in novel admixtures of vocabularies held in common. Vocabularies that are limited to trivially small sets of works, be they idiosyncracies of content or style, become sources of bias to topic model estimators. Because idiosyncratic vocabulary is be definition rare, it lacks both the mass of frequency and distribution aross documents to be reliably picked up as a topic. Indeed, if each document were expected to contain some idiosycracy, then the number of topics needed to catch all of the idiosycracy would be equal to the number of texts in the corpus. Each document would then be a combination its own idiosyncratic topic (of which it would account for 100 of topic content) and a distribution over other topics held in common. The real number of topics would then be <span class="math inline">\(K+N\)</span> where <span class="math inline">\(N\)</span> is the number of texts and practically always much greater than <span class="math inline">\(K\)</span>. Researchers would balk at including such a large set of extraneous topics, while estimators would both be strained by the greater paramters space and would collide with hyperparamters designed to militate against estimating topics distributed only over a single document.</p>
<p>The impracticality from a modeling perspective of representing idiosyncracy coincides with the undertheorized tendency among reseachers for extreme pruning of idiosyncracy during data preparation. A more parsimonious modeling solution would be to allow a single extra topic designed to catch all idiosyncracy. Yet this would tend to violate the assumptions behind construction of the other topics for two reasons, first because one topic would have significant distribution across all documents and second because terms within the topic would never be estimated together as they would really be a mixture of <span class="math inline">\(N\)</span> uncorrelated subtopics.</p>
<p>Idiosyncracy tends to be pruned in a desire to limit the length of the vocabulary to bring it within the bounds of computational power and the chances of a successful paramater optimization. Depending on the task, however, the researcher may not be so concerned with performance, and may leave plenty of idiosyncracy in the sample. What then is the effect on the topic estimation of such idiosycracy, since the idiosyncracy must end up somewhere?</p>
<p>First, there will be a tendency to muddy the content of common topics with the particular idiosyncracies of the documents that happen to draw on them. This in part explains the long, non-zero tails of topic by term distributions, which are usually filtered out during post-estimation and interpretation of the models. We would however expect them to corrupt the error structure of the topic they contaminate, leading to suboptimal estmates of the true terms in the topic.</p>
<p>Second, the document proportion of the contaminated topic will be inflated in the contaminating document. After all, the idiosyncracy of the document was represented, erroneously, in the contaminated topic. Because of the lengh of the term vector it is not difficult to imagine the truly pathological case wherein the probability sum of the false portion of the topic is greater than that of the true portion. In this event, a document could be categorized within a topic due more to the false content than to the true content, especially if the idiosyncracy was placed in topics randomly. Contrary to the effect of random error in an explanatory variable in ordinary least squares linear regression, which is to bias the regression coefficient downward, in a topic model the effect will be to bias the topic probabilty of a document upward.</p>
<p><span class="citation">DiMaggio et al. (<a href="#ref-DiMaggio2013Exploiting" role="doc-biblioref">2013</a>)</span> represent a typical albeit conservative approach to topic modeling as distant reading. Their data preparation of a newspaper corpus about U.S. arts policy in the 1980s and 1990s resulted in 54,982 unique terms and 7,598 documents <span class="citation">(<a href="#ref-DiMaggio2013Exploiting" role="doc-biblioref">2013</a>:582)</span>. This incredible dimensionality in the term vector, which eliminated only stopwords and a few hand-picked terms and did not stemming, represents a very conservative approach to term filtering admitting to no performance based truncation. They chose a model with 12 topics. Thus in a strict interpretation of their 12-topic model, we are to believe that the extreme idiosyncrasy of news, with all of its historical specificity, is contained in a noise or junk topic rather than creating bias on the estimation of the signal topics. With such a huge term mass to classify and so few topics in which to do it, it is incredible to think that the algorithm would alight on a junk topic rather than using that spot for a signal topic. It is plausible that the noise (and so offensive a term to those reporters trying to say something new!) is distributed across signal topics rather than being safely tossed in the dust bin. To wit, their choice of a low alpha parameter of 0.1, which assumes that each document is generated from relatively few topics, makes it even less likely that the estimator would spend precious parameter space on a noise rather than on a signal topic.</p>
<p><span class="citation">DiMaggio et al. (<a href="#ref-DiMaggio2013Exploiting" role="doc-biblioref">2013</a>)</span>- attempt to placate statistical cricitism by substituting quantitative, statistical forms of validation for qualitative cross validation of topics. This may be more treacherous than the authors admit. Their analytical approach is:</p>
<ol style="list-style-type: decimal">
<li>Fit the topic model.</li>
<li>Sort the topic by term vectors in decreasing order.
<ol style="list-style-type: lower-alpha">
<li>Split the fat head from the skinny tail.</li>
<li>Interpret the terms in the fat head.</li>
</ol></li>
<li>Sort the topic by document vectors in decreasing order.
<ol style="list-style-type: lower-alpha">
<li>Split the fat head from the skinny tail.</li>
<li>Classify those documents in the fat head according to 2.b.</li>
</ol></li>
<li>Interpret the documents according to 3.b.</li>
</ol>
<p>The sorting procedures are a typical low-hanging fruit use of the model. Even though the model is a much simpler ball of string than the original full text corpus, it is still a very complicated statistical equation with, in this case, 12 * 54,982 + 12 * 7,598 = 750,960 estimated parameters. Sorting the term and document vectors allows the analyst to proceed from an intepretation of the strongest signals toward the weakest, stopping when the author feels satisfied that the research question is satisfactorily addressed. The assumption here is that the strongest statistical signals are unbiased, that when parameters are converted to ranks, and the ranks are converted to truncated lists of words and documents, that those lists are correct.</p>
<p>The specter that I raised above applies to the document ranking more than to the term ranking. A formal feature of topic models is that each topic is composed of all terms in the corpus. Of course this is an artifact rather than an intention of the model, as the goal is to separate relevant from irrelevant terms in the constitution of topics. Similarly, all documents are distributions over all topics, but this is not (necessarily) the intention; again we expect an elbow in the sorted topic document vector in front of which are relevant and after which are irrelevant topics. Any concentration index, such as the Gini coefficient, calculated on the topic term and to a lesser extent the topic document vectors will show very high concentration, where most of the probability is owned by a few elements.</p>
<p>We can test for some of these expectations of bias. A document’s topic assignment may be considered suspect if its term distribution from that topic derives from the low and long tail of the topic, rather than from the select high probabilty terms normally associated with the topic’s meaning.</p>
</div>
</div>
<div id="qualitative-cross-validation" class="section level3">
<h3><span class="header-section-number">1.2.4</span> Qualitative Cross Validation</h3>
<p>To be sure, topic model parameters may be biased by misspecification, and if we are being fair, by the gargantuan task we ask of them. In part because topic models, notwithstanding their decades of development, remain difficult to validate statistically, and in part because educated people scoff at the idea of machine reading, many researchers ultimately rely on qualitative interpretation to evaluate model quality. Goodness of fit means that topics pass a sniff test upon inspection. A list of a words either does or does not inspire a theory of meaningful content, and this theory either is or is not confirmed upon inspection of document with a highly ranked topic probabilty.</p>
<p>The same scholars who promote qualitative cross validation (QCV) would presumably have bet on John Henry rather than the steam drill. The arguments against the machine, which excels only at recognition, is that it is a ham fisted intruder into the delicacies of sensemaking, semantics, and interpretation. Meaning operates very differently from information namely by bringing grounding to the response to information. One example of grounding is spreading activation, that when information is presented to the mind by sensation, the mind responds by representing not only a construct of the stimulus but also a network of constructs adjacent in memory to the stimulus. Humans always see more than what they perceive.</p>
<p>Some advocates for the machine go so far as to claim that the topic model actually recovers this “semantic context” in that a large collection of terms defines the topic while only a sample of these terms will be observed in a particular document <span class="citation">(DiMaggio et al. <a href="#ref-DiMaggio2013Exploiting" role="doc-biblioref">2013</a>:578)</span>. In this sense the topic model fills in missing information in the way that meaningful interpretation works. This notion rests on a very strong assumption, however, which is that information tacit in a particular case is explict in a different case, indeed a quorom of different cases, and that the cases overlap enough to become included under the same topic. This feature would seem to compete against the other feature of polysemy, that a term may appear in multiple topics each with a different context. How does the machine know that a particular term distribution (document) is a case of missing grounding within the same topic as another document, or is in fact a different topic with a different context?</p>
<p>Of course the machine knows nothing other than how to maximize an objective function. It will select random solutions for max for them; indeed, it is the hyperparamter choices of the researcher that decide often decide which tendencies will win out. The question of whether or not a topic model detects polysemy is governed by the choice of the sigma prior, which controls topic correlations, where a constraint toward low topic correlations prohibits detection of polysemy. The current state of software discourages an understanding of how hyperparamter tuning relates to a particular research agenda, and this opacity to the method is a strong driver toward QCV.</p>
<p>Cheap computing does make grid searching across hyperparameter settings possible, if not cost effective, but until this approach is usefully automated it is safe to assume that models will be misspecified in an unknown way, that the model is tuned in a particular arbitrary theoretical direction that is unknown to the researcher. Why would one believe that QCV would inoculate against the hidden bias imposed by the model? To be clear, a biased model is one that will present a vocabulary that <em>does not</em> represent the text accurately. In the conventional use of topic models, the researcher is eager to use the topic as a lens that both arranges documents into relevant subsets (a particular draw from the archive) and primes her interpretation of the documents content by a suggestive list of terms. We may enumerate a list of errors that are possible by this method:</p>
<p>Continuous document by topic probabilities are interpreted categorically. Imagine an urchin shaped quality distribution A large portion of the sample is categorized in the sample.</p>
</div>
</div>
<div id="data" class="section level2">
<h2><span class="header-section-number">1.3</span> Data</h2>
<p>We will use the JSTOR Data for Research service to download a bag-of-words text corpus for topic modeling. I take the following steps to develop a corpus:</p>
<ol style="list-style-type: decimal">
<li>Search dfr.jstor.org using the query <code>(ta:genr* OR ab:genr*) AND la:eng</code> and requesting 1grams.</li>
<li>To cull documents for which genre is not an important term, exclude documents containing fewer than five variants of the term genre (1grams matching the regular expression <code>^genr</code>: genre, genred, and genres).</li>
<li>Remove ngrams appearing fewer than three times, which often includes optical character recognition errors.</li>
<li>Remove ngrams shorter than three characters and longer than 25 characters, again often OCR errors but also stopwords that will be removed anyway.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></li>
<li>Remove ngrams longer than three characters that are all the same letter, often OCR errors but sometimes real, as in Roman numerals.</li>
<li>Compile baseline word counts for each document assuming that at this step the documents contain only valid terms, and no OCR errors.</li>
<li>Remove SMART stopwords.</li>
<li>Remove numbers.</li>
<li>Remove punctuation, except intraword hyphens.</li>
<li>Lemmatize or stem English words.</li>
<li>Remove lemma with fewer than three characters.</li>
<li>Aggregate 1grams defined by a single lemma and, for ease of interpretation, name the sum after the most common 1gram.</li>
<li>Remove terms appearing in fewer than 20 documents.</li>
<li>Remove documents that, after the above filters, have a word count of fewer than 500 words.</li>
<li>Remove documents that are identical in content to another document even if metadata differ, i.e. reprints.</li>
</ol>
<p>The initial query returned 7,695 articles from 1,205 different journals, as well as 6,485 book chapters from 4,427 books. After the above processing steps, the sample was reduced to 3,547 articles and 2,797 chapters, or 6,344 total texts.</p>
<p>It is fair to ask what is lost during the pre-processing of texts. Many are included in error due to JSTOR’s internal translation of abstracts; where “genre” is the French translation of the English “kind” the text will be included even if the term genre does not actually appear in the English title or abstract. While I do not carefully look at the content of the excluded documents, assuming they were not texts that made important use of the term genre, I do retain some information about what components of a text were lost of those documents that were not cut. This is a measure called idiosyncrasy, or the proportion of terms in a document eliminated during pre-processing. I call it idiosyncracy because the pre-processing condition was that terms would be eliminated if they did not appear in at least 20 other texts. Texts that lost a large volume of words to this filter are drawing on a vocabulary that almost no other texts use. It would not be surprising if these were ethnographic or content analytic studies of non English materials.</p>
<p>Figure <a href="int.html#fig:idi-hist">1.4</a> shows the right-skewed distribution of idiosyncracy. The median text lost about one tenth (10.19 percent) of its words, while 90 percent of texts are within two tenths, and outliers begin at about three tenths as can be seen in the boxplot. The 153 (2.41 percent of) texts above three tenths vary across a range as wide as the rest of the distribution. The most idiosyncratic text, at 60.4 percent of its vocabulary lost, is Pelli’s “The Revival of the Literary Genre of Religious Disputation in Hebrew Haskalah: Isaac Satanow’s <em>Divrei Rivot</em>.”<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> The article, from the journal <em>Hebrew Studies</em>, is a single page introduction in English to a 12 page essay reprinted in the original Hebrew. By page count alone we would expect the idiosycrasy to be 12/13 or 92.3 percent, which also illustrates how terms that are not in the Roman alphabet may be discarded as OCR errors even prior to the idiosyncracy measurement.</p>
<div class="figure" style="text-align: center"><span id="fig:idi-hist"></span>
<img src="ambrose_dissertation_files/figure-html/idi-hist-1.svg" alt="Distribution of idiosyncracy, the proportion of document vocabularly dropped during pre-processing. Pluses indicate outliers." width="672" />
<p class="caption">
Figure 1.4: Distribution of idiosyncracy, the proportion of document vocabularly dropped during pre-processing. Pluses indicate outliers.
</p>
</div>
<p>Figure <a href="int.html#fig:gp-hist">1.5</a> shows the logarithm of the count of the term genre as a proportion of the total term count of a text. This distribution is much more highly skewed but contains fewer outliers. In the median text a genre variant accounted for about 6 in 1,000 terms, while at the 90th percentile the rate is 27 in 1,000. 44 texts (0.69 percent) are outliers where one in ten or more words is a genre variant. The text with the largest genre proportion, at 35.7 percent of its words, is Welsh’s “Editorial: The Genre Revival”<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>, a single page introduction in a special issue of <em>Literature/Film Quarterly</em> on genres.</p>
<div class="figure" style="text-align: center"><span id="fig:gp-hist"></span>
<img src="ambrose_dissertation_files/figure-html/gp-hist-1.svg" alt="Distribution of $log_{10}$ of the count of the term &quot;genre&quot; as a proportion of all terms in a text. Pluses indicate outliers." width="672" />
<p class="caption">
Figure 1.5: Distribution of <span class="math inline">\(log_{10}\)</span> of the count of the term “genre” as a proportion of all terms in a text. Pluses indicate outliers.
</p>
</div>
</div>
<div id="estimation" class="section level2">
<h2><span class="header-section-number">1.4</span> Estimation</h2>
<p>I use the <code>stm</code> package in R to estimate a series of topic models from K=2 to K=10 <span class="citation">(<span class="citeproc-not-found" data-reference-id="Robertsstructuraltopicmodel2013"><strong>???</strong></span>; Roberts et al. <a href="#ref-Roberts2018stm" role="doc-biblioref">2018</a>)</span>.</p>
</div>
<div id="results" class="section level2">
<h2><span class="header-section-number">1.5</span> Results</h2>
<div class="figure" style="text-align: center"><span id="fig:sankey"></span>
<div id="htmlwidget-3240b2a27d64fbaa99e9" style="width:100%;height:480px;" class="sankeyNetwork html-widget"></div>
<script type="application/json" data-for="htmlwidget-3240b2a27d64fbaa99e9">{"x":{"links":{"source":[35,36,40,42,39,38,41,43,37,39,43,42,37,35,41,36,40,38,41,42,38,36,37,39,43,40,35,38,41,36,37,39,40,42,43,35,43,39,36,37,42,40,35,41,38,35,37,40,36,41,43,38,42,39,38,42,37,39,41,36,35,43,40,40,36,38,39,41,42,37,43,35,36,41,40,43,42,37,38,39,35,36,42,43,40,37,35,38,39,41,0,2,0,2,0,2,1,3,4,3,4,1,4,3,1,4,1,3,5,6,7,8,7,8,6,5,7,8,5,6,5,7,6,8,5,6,8,7,9,11,12,13,10,9,13,11,12,10,9,12,13,10,11,12,13,10,11,9,11,9,10,12,13,11,10,12,9,13,14,16,19,18,17,15,18,14,17,15,16,19,14,16,19,15,17,18,18,14,16,15,17,19,18,17,16,14,19,15,16,18,17,15,14,19,19,15,17,16,18,14,20,23,22,24,25,26,21,25,20,22,21,23,26,24,21,25,26,20,22,24,23,25,21,22,20,24,23,26,20,24,26,21,25,23,22,22,23,25,24,20,21,26,21,22,20,24,25,23,26,26,23,25,24,22,21,20,27,28,33,31,30,32,34,29,31,30,33,28,29,27,34,32,34,29,31,33,27,28,30,32,34,30,29,32,28,27,31,33,34,32,28,29,33,27,31,30,32,30,28,31,34,33,29,27,31,29,33,30,34,28,32,27,31,29,27,32,33,34,30,28,32,27,29,31,30,28,33,34],"target":[44,44,44,44,44,44,44,44,44,53,53,53,53,53,53,53,53,53,45,45,45,45,45,45,45,45,45,46,46,46,46,46,46,46,46,46,47,47,47,47,47,47,47,47,47,48,48,48,48,48,48,48,48,48,49,49,49,49,49,49,49,49,49,50,50,50,50,50,50,50,50,50,51,51,51,51,51,51,51,51,51,52,52,52,52,52,52,52,52,52,1,1,3,3,4,4,5,5,5,6,6,6,7,7,7,8,8,8,9,9,9,9,10,10,10,10,11,11,11,11,12,12,12,12,13,13,13,13,14,14,14,14,14,15,15,15,15,15,16,16,16,16,16,17,17,17,17,17,18,18,18,18,18,19,19,19,19,19,20,20,20,20,20,20,21,21,21,21,21,21,22,22,22,22,22,22,23,23,23,23,23,23,24,24,24,24,24,24,25,25,25,25,25,25,26,26,26,26,26,26,27,27,27,27,27,27,27,28,28,28,28,28,28,28,29,29,29,29,29,29,29,30,30,30,30,30,30,30,31,31,31,31,31,31,31,32,32,32,32,32,32,32,33,33,33,33,33,33,33,34,34,34,34,34,34,34,35,35,35,35,35,35,35,35,36,36,36,36,36,36,36,36,37,37,37,37,37,37,37,37,38,38,38,38,38,38,38,38,39,39,39,39,39,39,39,39,40,40,40,40,40,40,40,40,41,41,41,41,41,41,41,41,42,42,42,42,42,42,42,42,43,43,43,43,43,43,43,43],"value":[10.1363289632975,0.325078237986267,0.371869101349065,0.0690433342915592,0.449854531636005,0.501943757871881,0.326625323739309,0.748636660839481,0.233801149287655,0.288581040521418,0.493864336408381,1.57216293048457,0.31626447959283,0.233615103007508,0.649695471414078,0.834134435459633,0.1875976264962,2.64556482900161,0.751109276026251,0.513882510457403,0.430031402940871,7.46216680300419,0.132376415073022,0.134251237935692,1.12711900378364,0.248065421204397,0.232929442377462,0.268621250427415,0.380827511901085,0.181439039302619,4.17707305521377,0.260443427966661,0.289472949145803,0.082338118948304,0.379303550964821,0.162910916668874,1.93420631498433,1.44658884855161,0.360416454164215,0.287188389577035,0.148973684392575,0.516798831336274,0.601556434615904,1.2715712884181,6.29033038914273,0.436322398868328,0.232641273920763,0.348812491724708,0.0873615922786659,1.30811759140346,0.564916348927739,0.401811154753397,0.0425601698912091,7.6557643301401,0.176808812498667,0.222332743357525,0.25758585194161,0.323004579122334,0.395877548694641,0.225250881035693,0.280388188074235,0.408714139283217,5.97043609587765,0.381389245436638,0.998158447043141,1.20811732275884,1.45555525148502,5.84586208693967,0.357703962975828,0.38024878502697,1.09778619023099,0.293664510768046,0.489682497863068,0.479955408871397,0.354469121750925,0.318275201597861,1.94787754765386,0.0462168725521877,0.279267180577126,0.0508956866992121,0.0959067705611936,1.74526281497539,0.570128936248663,7.77712146250865,0.410986253969636,0.417402688014105,0.397105008285943,1.15826524055288,0.34706455183678,1.30027147981205,33.3878507816827,4.35548151707546,9.00315577233663,33.3497560410543,8.68336076829222,11.2203951195587,19.9114561168076,9.70754409868989,3.50829568245549,25.7595835361351,3.47979872516652,1.72785389255154,11.6191815805882,3.39673624007692,2.35272823799632,1.35143335841556,13.8785270336774,3.30686149743952,1.63580852944194,1.38793469836312,1.00126491179455,12.2130727766342,1.95198231703438,1.09768131993583,17.9799863537766,1.53457166986086,9.95706740453116,0.973448721511054,1.7317626249327,2.35753696826761,11.2296984975272,1.86051331186339,7.67971005631857,2.79028738102753,17.322714398333,1.24643930441848,1.65305615335279,2.39546260107521,11.3783637598566,0.615384520368705,1.18884266950458,1.24789409283914,0.800917357941796,0.897116516803835,0.814561213055782,1.32012192328226,2.42207525406944,15.1895167497133,0.438905424734533,1.08769158658184,0.686469228668083,2.04719823806882,5.89440854467358,14.8657067553412,2.43360390664507,2.29544565450708,0.934212828517056,1.46379196940464,0.992515704525613,0.90191836173371,1.29468006828491,3.68344285112294,14.167900473735,4.93576228535202,0.670241486557899,0.920292124098317,1.12436838730026,3.28665006271199,10.9591183692296,0.34408585303041,0.689032199569446,0.707386595508998,0.974975162809142,0.693983734715988,0.632109995009685,0.619211125596254,1.71343376108983,13.0259298047725,1.30721344253792,0.468043744437309,0.39169319226492,5.99752997942707,0.628879042104638,1.21661916471529,0.848274939508047,0.636148567099813,2.19807405919467,1.30980395790137,0.855145915344877,1.87200372740072,12.6256484602187,0.760354815190973,9.66472259108231,1.42049705561907,0.408840771277285,0.867938221587816,1.00121222941746,0.126908705637607,0.477631056951547,0.859538989434635,0.580239558425571,0.572216377658341,0.640120159189766,6.93294333508826,0.666247827520839,3.13738313846456,3.57688176087472,0.724678867683823,6.33968679552069,0.527612949887495,10.7231540861526,0.958658755601488,0.232885721076506,0.574218969254198,0.582438216958469,0.487532571857219,0.514048383914878,0.350039544568102,0.514029334420955,0.612535124803867,11.8750902718462,1.49582896922731,1.63275845236167,0.133397169761732,1.01384658768413,0.834425369952433,0.463095661331555,0.464390060708716,4.00563477674451,0.247006404470207,0.49583139181499,0.519675467653853,1.61162746475069,0.690161386246071,1.06323868159565,1.1717597736227,12.1413012383664,2.32146023349161,0.579455245132219,9.20674504719407,1.88216755857039,0.144442384549925,0.433045283814222,1.25917302622023,0.200717061665064,0.350188487306707,0.719650220303829,6.52953189493075,0.588260658807028,0.612113895424836,0.34385775615704,0.659523648430955,1.71990005561557,0.626090360692895,0.440840475307138,1.33370075535411,0.57946750202058,1.90579188956914,7.18380889021498,0.499745184943097,0.582041028144149,0.182864302036161,0.163710567701407,2.98600585003742,0.485235373077023,0.0718555265404628,10.1423219052498,0.268554218015591,0.318531228297519,0.490080042563688,0.634555748820824,0.50182094597332,0.050926681747525,0.275949134811022,0.136225661333389,0.872796085671086,1.19558311229908,9.06155428887127,0.479936159991961,0.357753138297997,0.381664330571813,0.331376830965922,0.180752289544327,3.78423908496682,0.367411189688162,0.417046117750784,0.222507048118638,0.346393128938209,0.562387761499171,0.334539395000332,0.589746108255203,8.13324927304071,0.247002120681252,0.902128823709626,0.688730912626168,0.571610909256544,1.11558559083137,1.36111845033341,0.141481671303826,0.512148268588696,0.123689672709367,0.172259837107799,1.26982749347577,0.539566223273041,8.78886854074172,0.895733769431645,6.03454969184939,0.28232908144196,0.254556226042638,0.398340415335144,0.211331985240447,0.490578429252401,1.06183008682969,0.471697937101059,1.65796668590704,0.35937915151384,6.38230441461818,1.89269607148394,0.339291002190715,1.28916099855507,0.555410059730668,0.381702039789606,0.0643320037740882,0.833028442648339,0.0854276444114757,0.156317867374189,0.481795965752732,2.72589570063952,0.32691205951781,0.849448016825235,0.376968660132773,1.42115824473633,0.651356479448209,0.791831094447803,5.7817902389208,3.4898866864687,1.75015479006868,0.38291863359716],"group":["3","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","2","0","0","0","0","0","0","0","0","5","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","7","0","0","0","0","0","0","0","0","9","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","1","0","0","2","0","0","1","0","0","2","0","0","4","0","0","0","3","0","0","0","0","3","0","0","2","0","4","0","0","0","0","0","0","0","6","0","0","0","3","0","0","0","0","0","0","0","0","2","0","0","0","0","0","8","0","0","0","0","0","0","0","0","6","0","0","0","0","0","3","0","0","0","0","0","0","0","0","2","0","0","0","0","0","0","0","0","0","0","0","0","8","0","7","0","0","0","0","0","0","0","0","0","0","9","0","0","0","0","0","0","3","0","0","0","0","0","0","0","0","0","2","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","8","0","0","7","0","0","0","0","0","0","0","9","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","3","0","0","0","0","0","0","0","0","0","0","2","0","0","0","0","0","5","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","7","0","9","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0","0"]},"nodes":{"name":["2k1","3k1","2k2","3k2","3k3","4k1","4k2","4k3","4k4","5k1","5k2","5k3","5k4","5k5","6k1","6k2","6k3","6k4","6k5","6k6","7k1","7k2","7k3","7k4","7k5","7k6","7k7","8k1","8k2","8k3","8k4","8k5","8k6","8k7","8k8","9k1","9k2","9k3","9k4","9k5","9k6","9k7","9k8","9k9","10k1","10k2","10k3","10k4","10k5","10k6","10k7","10k8","10k9","10k10"],"group":["0","1","0","1","2","2","1","1","1","1","1","2","2","1","1","1","2","1","2","2","1","1","2","1","1","1","2","1","1","2","2","1","1","2","2","1","1","1","2","1","1","2","2","2","0","0","0","0","0","0","0","0","0","0"]},"options":{"NodeID":"name","NodeGroup":"group","LinkGroup":"group","colourScale":"d3.scaleOrdinal(d3.schemeCategory20);","fontSize":7,"fontFamily":null,"nodeWidth":15,"nodePadding":0,"units":"","margin":{"top":null,"right":null,"bottom":null,"left":null},"iterations":5000,"sinksRight":true}},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 1.6: Sankey diagram of document overlap between topic models of increasing values of K.
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:top-gini"></span>
<img src="ambrose_dissertation_files/figure-html/top-gini-1.svg" alt="Topic concentrations (TC) within documents by TC within terms. Circles proportional to term frequency explained by each topic" width="672" />
<p class="caption">
Figure 1.7: Topic concentrations (TC) within documents by TC within terms. Circles proportional to term frequency explained by each topic
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:elbow"></span>
<img src="ambrose_dissertation_files/figure-html/elbow-1.svg" alt="Cumulative distribution of within topic term probabilities." width="672" />
<p class="caption">
Figure 1.8: Cumulative distribution of within topic term probabilities.
</p>
</div>
<pre><code>Topic index maps to probability index like so:
      viz
 [1,]   9
 [2,]   4
 [3,]   1
 [4,]   7
 [5,]   2
 [6,]   5
 [7,]  10
 [8,]   6
 [9,]   3
[10,]   8</code></pre>
<pre><code>Warning in dir.create(out.dir): &#39;ldaviz/viz&#39; already exists</code></pre>
<div class="figure" style="text-align: center"><span id="fig:genr-mod-viz"></span>
<iframe src="https://mercury.dlab.berkeley.edu:2018" width="100%" height="865px">
</iframe>
<p class="caption">
Figure 1.9: Topic Term Explorer, K=10
</p>
</div>
<pre><code>Read the 6344 x 10 data matrix successfully!
Using no_dims = 2, perplexity = 30.000000, and theta = 0.500000
Computing input similarities...
Normalizing input...
Building tree...
 - point 0 of 6344
Done in 0.74 seconds (sparsity = 0.018194)!
Learning embedding...
Iteration 50: error is 92.959216 (50 iterations in 3.04 seconds)
Iteration 100: error is 80.855616 (50 iterations in 2.72 seconds)
Iteration 150: error is 79.770183 (50 iterations in 2.09 seconds)
Iteration 200: error is 79.653421 (50 iterations in 2.10 seconds)
Iteration 250: error is 79.638770 (50 iterations in 2.13 seconds)
Iteration 300: error is 2.551761 (50 iterations in 1.84 seconds)
Iteration 350: error is 2.082206 (50 iterations in 1.92 seconds)
Iteration 400: error is 1.836306 (50 iterations in 2.01 seconds)
Iteration 450: error is 1.684480 (50 iterations in 2.02 seconds)
Iteration 500: error is 1.582583 (50 iterations in 2.08 seconds)
Iteration 550: error is 1.511369 (50 iterations in 2.08 seconds)
Iteration 600: error is 1.459578 (50 iterations in 2.12 seconds)
Iteration 650: error is 1.422133 (50 iterations in 2.10 seconds)
Iteration 700: error is 1.395800 (50 iterations in 2.10 seconds)
Iteration 750: error is 1.379151 (50 iterations in 2.11 seconds)
Iteration 800: error is 1.368309 (50 iterations in 2.10 seconds)
Iteration 850: error is 1.360719 (50 iterations in 2.16 seconds)
Iteration 900: error is 1.353961 (50 iterations in 2.13 seconds)
Iteration 950: error is 1.347436 (50 iterations in 2.16 seconds)
Iteration 1000: error is 1.341263 (50 iterations in 2.17 seconds)
Fitting performed in 43.18 seconds.</code></pre>
</div>
<div id="discussion" class="section level2">
<h2><span class="header-section-number">1.6</span> Discussion</h2>

</div>
</div>
<h3>Bibliography</h3>
<div id="refs" class="references">
<div id="ref-Michel2011Quantitative">
<p>Michel, Jean-Baptiste, Yuan Kui Shen, Aviva Presser Aiden, Adrian Veres, Matthew K. Gray, The Google Books Team, Joseph P. Pickett, Dale Hoiberg, Dan Clancy, Peter Norvig, Jon Orwant, Steven Pinker, Martin A. Nowak, and Erez Lieberman Aiden. 2011. “Quantitative Analysis of Culture Using Millions of Digitized Books.” <em>Science</em> 331(6014):176–82.</p>
</div>
<div id="ref-James2019ecp">
<p>James, Nicholas A. and Wenyu Zhang and David S. Matteson. 2019. “Ecp: Non-Parametric Multiple Change-Point Analysis of Multivariate Data.”</p>
</div>
<div id="ref-Matteson2013Nonparametric">
<p>Matteson, David S. and Nicholas A. James. 2013. “A Nonparametric Approach for Multiple Change Point Analysis of Multivariate Data.” <em>arXiv:1306.4933 [Stat]</em>.</p>
</div>
<div id="ref-2012Google">
<p>Google. 2012. “Google Ngram Viewer.”</p>
</div>
<div id="ref-Nelson2017Computational">
<p>Nelson, Laura K. 2017. “Computational Grounded Theory: A Methodological Framework.” <em>Sociological Methods &amp; Research</em> 0049124117729703.</p>
</div>
<div id="ref-Moretti2005Graphs">
<p>Moretti, Franco. 2005. <em>Graphs, Maps, Trees : Abstract Models for a Literary History</em>. London: Verso.</p>
</div>
<div id="ref-DiMaggio2013Exploiting">
<p>DiMaggio, Paul, Manish Nag, and David Blei. 2013. “Exploiting Affinities Between Topic Modeling and the Sociological Perspective on Culture: Application to Newspaper Coverage of U.S. Government Arts Funding.” <em>Poetics</em> 41(6):570–606.</p>
</div>
<div id="ref-Roberts2018stm">
<p>Roberts, Margaret, Brandon Stewart, Dustin Tingley, and Kenneth Benoit. 2018. “Stm: Estimation of the Structural Topic Model.”</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>TODO, I did not, but should take a random draw of the same size to serve as a control.<a href="int.html#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>What a scandal it would be if Weber’s lionizers discovered that he had only read text indices! Surely they would bury such a fact. But the point would remain that even if a scholar were able to consumer an entire corpus, the sheer scale of contemporary publication is now beyond even a genius’s capacity.<a href="int.html#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>The Freudian “id” is an unfortunate casualty of this step, as well as some footnotes, endnotes, and captions containing small text where word boundaries were not detected during OCR and a series of words was concatenated.<a href="int.html#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>www.jstor.org/stable/10.2307/27909026<a href="int.html#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>www.jstor.org/stable/10.2307/43795866<a href="int.html#fnref5" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="abstracts.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="gen.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": "https://github.com/brooksambrose/portfolio/blob/draft/01.Rmd",
"text": "Edit"
},
"download": ["ambrose_dissertation.pdf", "ambrose_dissertation.docx"],
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
