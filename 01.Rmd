# Knowledge Development {#kd}

#### Abstract {-}

(ref:abs-kd)

#### Keywords {-}

topic modeling, history of social science

## Topics in time

Topic modeling refers to a variety of approaches to the statistical modeling of texts that blurs the distinction between qualitative and quantitative analysis. Texts are merely collections of terms (usually words) that are counted, and such counts may describe a text. In the same way that a civil census reduces communities to counts of the people who live in them, topic modeling reduces texts are texts to a count of words, to diction. And just as a census of people fails to capture the nuanced interactivity of human settlements found in their culture, politics, and economic activity, the meanings and intentions behind words are washed away.  At a very general level this term census paints a lexographic picture of texts, analogous to the demographic picture gained by a survey census of cities and towns. This approach to document description is sometimes called a "bag of words".

To explain the contents of the bag-of-word, topics are proposed. Topics can be thought of as catelogs out of which words are ordered and placed into the shopping cart that is the document. Different catelogs, different word availabilities, will produce different documents. The final bit of inference that makes topic models so practically useful is the idea that documents may be composed of multiple topics.

The surprising qualities of texts are explained to be how authors draw on regular and commonplace topics to say something different.

## Method

### Data Quality

#### JSTOR journals

```{r master2jstorm}
f<-'d/q/jstorm.RData'
if(file.exists(f)) {
  load(f)
} else {
  jstorm<-master2jstorm.f()
  save(jstorm,file=f)
}
rm(f)
```

```{r jstorm2jclu}
f<-'d/q/jclu.RData'
if(file.exists(f)){
  load(f)
} else {
  jclu<-jstorm2jclu.f(jstorm)
  save(jclu,file=f)
}
rm(f)
```

```{r jstorm2tab,include=T}
#jtab<-jstorm2tab.f(jstorm,beg.bef = 1900,end.aft = 2000)
#kable(jtab,caption='20th Century Social Science Journals in JSTOR')
```

```{r}
f<-'d/q/nces.RData'
if(file.exists(f)){
  load(f)
} else {
  system(paste('wget -qO',f,'https://nces.ed.gov/programs/digest/d14/tables/xls/tabn301.20.xls'))
  nces<-readxl::read_xls(f)
  save(nces,file=f)
}
rm(f)
phd<-nces[c(1,grep('Doctor',nces[[1]])),grep('[0-9]{4}',nces[1,])] %>% t %>% data.table %>% setnames(c('year','N'))
phd[,year:=sub('-.+','',year) %>% as.integer %>% `+`(1)]
phd[,N:=as.numeric(N)]
spl<-phd[,.(year=year[match(do.call(seq,range(year) %>% as.list),year,nomatch = NA)])]
spl[!is.na(year),N:=phd$N]
spl[,`:=`(
  year=zoo::na.approx(year)
  ,N=zoo::na.spline(N,method='hyman')
  #,N=zoo::na.approx(N)
  )]

myth(qplot(x = year,y = N,geom='line',data=spl) + geom_point(data=phd,aes(x=year,y=N)))
```


```{r jstorm2fig,include=T}
f<-'d/q/jfig.RData'
if(file.exists(f)){
  load(f)
} else {
  jfig<-jstorm2fig.f(jstorm,jclu)
  save(jfig,file=f)
}
rm(f)
jfig$p
```

```{r}
rat<-jfig$d[between(year,1800,2000)&super=='Social Sciences'] %>% setkey(year)
setkey(spl,year)
rat<-merge(rat,spl)
rat<-rat[,.(year,ratio=N.y/N.x,dif1g)]
r<-rat[,range(ratio)]
n<-diff(r)*.1

myth(
  ggplot(data=rat) +
    annotate('rect',xmin=1888,xmax=1922,ymin=r[1]-n,ymax=r[2],alpha=.1) +
    geom_line(aes(x=year,y=ratio,color=dif1g),size=1.5) 
) + theme(legend.position="none")
rat[between(year,1888,1922)]
```

This period represents one of stable growth, as the size of the field grows with the number of players on it.

```{r zot2bib}
f<-'d/q/zot2bib.RData'
if(file.exists(f)){
  load(f)
} else{
  zot2bib<-zot2bib.f(
    users='2730456'
    ,collections = fread('d/q/zotcollections.txt')$URL
    ,key = 'qMxyytAhp07W9jNOGssIQasg'
  )
  save(zot2bib,file=f)
}
rm(f)
d<-lapply(zot2bib,function(x) x[,.(publicationTitle,date,volume,issue,pages,bp,ep,title,creators,dateModified,url)]) %>% rbindlist %>% `[`(!duplicated(.[,!9]))
setorder(d,publicationTitle,dateModified)
##### find corrupt
d[is.na(publicationTitle),]
d<-d[!is.na(publicationTitle),]
```

Every record for every journal was downloaded manually, including front and back matter, articles, and book reviews.

```{r z2b-dups}
# find duplicates
d[duplicated(d[,!9:10]),.(dateModified,pub=publicationTitle,tit=substr(title,1,50),date,volume,issue,pages)][,cat(date,' ',volume,' ',issue,' ',tit,' ',pages,'\n',sep=''),by=dateModified]
```

```{r z2p-pgap}
# find page gaps
p<-d[!(duplicated(d[,!9:10])|is.na(bp)),.(
  t=publicationTitle
  ,d=date
  ,v=volume
  ,i=issue
  ,p=mapply(function(b,e) b:e,b=bp,e=ep)
),by=.I]
p[,.(gap=setdiff(
  p %>% unlist %>% range %>% as.list %>% do.call(seq,.)
  ,p %>% unlist %>% unique %>% sort
)),by=.(t,d,v,i)][,.(gap=list(gap)),by=.(t,d,v,i)]
```

```{r z2b-vgap}
# find volume gaps
p[,table(t,i)]
cat('',sep='\n')
vg<-p[,table(d,factor(i,levels=1:6),t)]
vg[vg==0]<-NA
h<-hist(vg)
h<-which(vg<=h$breaks[2],arr.ind = T)
check<-cbind(h[,-1],vg[h])[order(h[,3],h[,1]),]
colnames(check)<-c('i','j','n')
cat('\nDouble check these volumes:\n\n')
check
```

```{r z2b-short}
# find short runs
setorder(d,publicationTitle,date,volume,issue,bp)
lt<-d[!is.na(bp+ep),.(title=title[.N],pages=pages[.N],dateModified=dateModified[.N],url=url[.N]),by=.(publicationTitle,date,volume,issue)]
ltn<-lt[,.N,by=title][N<3,title]
setkey(lt,title)
# will cause autoban, but this will test automatically
if(F) w<-lt[ltn,list({
  Sys.sleep(rnorm(1) %>% abs %>% `+`(1.5))
  cat('.')
  try(url %>% read_html %>% html_text %>% grepl('Next Item',.))
}),by=url]
lt[ltn][,cat(publicationTitle,' ',date,' ',volume,' ',issue,' ',title,' ',pages,'\n',url,'\n\n',sep=''),by=dateModified] %>% invisible
```

#### Sampling Frame

```{r jpdf2imp}
f<-'d/d/jpdf.RData'
if(file.exists(f)){
  load(f)
} else{
  # seed<-.Random.seed %>% sample(1)
  # cat('Seed: ',seed,'\n',sep='')
  # set.seed(seed)
  jpdf<-jpdf2imp.f(
    dir('d/d',full.names = T,recursive = T,pattern = '\\.pdf$') # %>% sample(100) # readLines('test.txt')
    ,ocr = T) # write feedback for long imports
  save(jpdf,file=f)
}
rm(f)
```

```{r imp2ftx,eval=F}
f<-'d/p/ftx.RData'
if(file.exists(f)){
  load(f)
} else {
  seed<-.Random.seed %>% sample(1)
  cat('Seed: ',seed,'\n',sep='')
  set.seed(seed)
  s<-sample(jpdf$met[,doc],10)
  ftx<-imp2ftx.f(jpdf$imp[s])
  save(ftx,file = f)
}
rm(f,jpdf)
```

```{r ftx2pre}
f<-'d/p/pre.RData'
if(file.exists(f)){
  load(f)
} else {
  pre<-ftx2pre.f(ftx,frq = 2,nch=2)
  save(pre,file = f)
}
rm(f,ftx)
```

#### Unit of Analysis

```{r pre2mlc}
f<-'d/p/mlc.RData'
if(file.exists(f)){
  load(f)
} else {
  mlc<-pre2mlc.f(pre)
  save(mlc,file = f)
}
rm(f)
```

```{r pre2des,include=T}
des.com<-pre2des.f(pre)
des.stm<-pre2des.f(pre[!is.na(stm)])
knitr::kable(list(des.com,des.stm,data.table(tot=round(des.stm$cnt/des.com$cnt*100,2))),caption = 'Full Text Descriptives', booktabs = TRUE)
```

#### Sampling

### How many topics?

```{r pre2tel}
f<-'d/p/tel.RData'
if(file.exists(f)){
  load(f)
} else {
  tel<-pre2tel.f(pre[!is.na(stm)],stop = 3)
  save(tel,file = f)
}
rm(f)
```

```{r tamK}
f<-'d/p/clu.RData'
if(file.exists(f)){
  load(f)
} else {
  clu<-tel2clu.f(tel)
  save(clu,file=f)  
}
rm(f)
str(clu)
```

```{r mlc2mlk}
f<-'d/p/mlk.txt'
if(file.exists(f)) {mlk<-fread(f)} else {
  mlc2mlk.f(mlc) %>% fwrite(f)
  pbapply::pbreplicate(999,mlc2mlk.f(mlc) %>% fwrite('d/p/mlk.txt',append = T),cl = 6)
}
rm(f)
mlk<-melt(mlk,measure.vars = names(mlk),variable.name = 'level',value.name = 'K') %>% setkey(level)
```

```{r mlk2sim}
f<-'d/b/sim.RData'
if(file.exists(f)) {load(f)} else {
  sim<-mlk2sim.f(mlk)
  save(sim,file=f)
}
rm(f)
```

```{r sim-fig,include=F,fig.cap='Distribution of K by convex hull'}
sim$fig
```

```{r mlk2k}
k<-mlk[,.(k=pbapply::pbreplicate(1e4,psych::kurtosi(K %>% sample(replace = T)),cl = 1)),by=level][,.(e=mean(k),se=sd(k),l99=quantile(k,.005),u99=quantile(k,.995),`p>=0`=mean(k>=0)),by=level]
```

```{r k-fig,include=F,eval=F}
kable(k)
```

```{r mlk-tab,include=F} 
kable(sim$tab)
```

### Model selection

```{r mlc2stm}
# mod<-stmbow2lda.f(list(documents=mlc$par,vocab=mlc$voc),k=50,out.dir='d/p',verbose=F,visualize.results = T)
# mod$top.word.phi.beta[mod$top.word.phi.beta==0] <-.Machine$double.eps
# debugonce(lda2viz.f)
# viz<-lda2viz.f(mod,'d/b')
```

### Making sense

## An end.

Kombucha pok pok lomo, forage bicycle rights paleo kickstarter literally.  Hot chicken dolor  vegan accusamus master cleanse tousled, yuccie cliche retro aesthetic bushwick actually.  Ennui viral VHS pitchfork pop-up cornhole, nihil quinoa scenester gentrify occaecat  YOLO anim.  Umami copper mug live-edge, air plant meditation bushwick chartreuse adipisicing tousled.  Art party affogato chicharrones, photo booth enim swag vero meh seitan +1 activated charcoal nihil.  Etsy gluten-free authentic mixtape.  Shabby chic duis  90's pop-up pinterest, lumbersexual mollit  cillum.
