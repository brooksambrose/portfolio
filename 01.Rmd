# Knowledge Development {#kd}

#### Abstract {-}

(ref:abs-kd)

#### Keywords {-}

topic modeling, history of social science

## A beginning.

Nulla  sint  art party ex, mlkshk disrupt selvage 90's.  Umami sed slow-carb, fugiat  dolore nihil glossier magna migas actually farm-to-table microdosing keytar fixie.  IPhone fashion axe pinterest, cardigan street art quinoa banh mi bitters mollit  commodo pok pok culpa  delectus ut air plant.  Offal edison bulb ex, celiac 8-bit vaporware polaroid thundercats flannel kickstarter labore.  Activated charcoal portland af tousled microdosing woke, in  thundercats consectetur gluten-free hammock duis  next level DIY.  Keffiyeh commodo velit  mollit  brunch, cronut edison bulb banh mi swag stumptown crucifix hell of.  Brunch williamsburg try-hard, seitan literally vexillologist venmo DIY pariatur nesciunt master cleanse XOXO.

## Topics in Time

### JSTOR journals

```{r master2jstorm}
f<-'d/q/jstorm.RData'
# jstor throttles if too many calls
if(file.exists(f)){
  load(f)
} else {
  u<-paste0('https://www.jstor.org/action/showJournals?contentType=journals&letter=',c('0-9',LETTERS))
  jstorm<-list()
  for(i in u[which(u%in%i):length(u)]){
    jstorm[[i]]<-try(master2jstorm.f(i))
    Sys.sleep(rnorm(1) %>% abs %>% `+`(.5))
  }
  jstorm<-jstorm[!sapply(jstorm,inherits,'try-error')] %>% rbindlist
  save(jstorm,file=f)
}
rm(f)
```

```{r jstorm2tab,include=T}
jtab<-jstorm2tab.f(jstorm,beg.bef = 1900,end.aft = 2000)
cat('latex:',is_latex_output())
kable(jtab,caption='20th Century Social Science Journals in JSTOR')
```

```{r zot2bib}
f<-'d/q/zot2bib.RData'
if(file.exists(f)){
  load(f)
} else{
  zot2bib<-zot2bib.f(
    users='2730456'
    ,collections = fread('d/q/zotcollections.txt')$URL
    ,key = 'qMxyytAhp07W9jNOGssIQasg'
  )
  save(zot2bib,file=f)
}
rm(f)
d<-lapply(zot2bib,function(x) x[,.(publicationTitle,date,volume,issue,pages,bp,ep,title,creators,dateModified,url)]) %>% rbindlist %>% `[`(!duplicated(.[,!9]))
setorder(d,publicationTitle,dateModified)
# find corrupt
d[is.na(publicationTitle),]
d<-d[!is.na(publicationTitle),]
```

```{r z2b-dups}
# find duplicates
d[duplicated(d[,!9:10]),.(dateModified,pub=publicationTitle,tit=substr(title,1,50),date,volume,issue,pages)][,cat(date,' ',volume,' ',issue,' ',tit,' ',pages,'\n',sep=''),by=dateModified]
```

```{r z2p-pgap}
# find page gaps
p<-d[!(duplicated(d[,!9:10])|is.na(bp)),.(
  t=publicationTitle
  ,d=date
  ,v=volume
  ,i=issue
  ,p=mapply(function(b,e) b:e,b=bp,e=ep)
),by=.I]
p[,.(gap=setdiff(
  p %>% unlist %>% range %>% as.list %>% do.call(seq,.)
  ,p %>% unlist %>% unique %>% sort
)),by=.(t,d,v,i)][,.(gap=list(gap)),by=.(t,d,v,i)]
```

```{r z2b-vgap}
# find volume gaps
p[,table(t,i)]
cat('',sep='\n')
vg<-p[,table(d,factor(i,levels=1:6),t)]
vg[vg==0]<-NA
h<-hist(vg)
h<-which(vg<=h$breaks[2],arr.ind = T)
check<-cbind(h[,-1],vg[h])[order(h[,3],h[,1]),]
colnames(check)<-c('i','j','n')
cat('\nDouble check these volumes:\n\n')
check
```

```{r z2b-short}
# find short runs
setorder(d,publicationTitle,date,volume,issue,bp)
lt<-d[!is.na(bp+ep),.(title=title[.N],pages=pages[.N],dateModified=dateModified[.N],url=url[.N]),by=.(publicationTitle,date,volume,issue)]
ltn<-lt[,.N,by=title][N<3,title]
setkey(lt,title)
# will cause autoban, but this will test automatically
if(F) w<-lt[ltn,list({
  Sys.sleep(rnorm(1) %>% abs %>% `+`(1.5))
  cat('.')
  try(url %>% read_html %>% html_text %>% grepl('Next Item',.))
}),by=url]
lt[ltn][,cat(publicationTitle,' ',date,' ',volume,' ',issue,' ',title,' ',pages,'\n',url,'\n\n',sep=''),by=dateModified] %>% invisible
```

```{r jpdf2imp}
f<-'d/d/jpdf.RData'
if(file.exists(f)){
  load(f)
} else{
  seed<-.Random.seed %>% sample(1)
  cat('Seed: ',seed,'\n',sep='')
  set.seed(seed)
  jpdf<-jpdf2imp.f(
    dir('d/d',full.names = T,recursive = T,pattern = '\\.pdf$') # %>% sample(100) # readLines('test.txt')
    ,ocr = T) # write feedback for long imports
  save(jpdf,file=f)
}
rm(f)
```

```{r imp2ftx,eval=F}
f<-'d/p/ftx.RData'
if(file.exists(f)){
  load(f)
} else {
  seed<-.Random.seed %>% sample(1)
  cat('Seed: ',seed,'\n',sep='')
  set.seed(seed)
  s<-sample(jpdf$met[,doc],10)
  ftx<-imp2ftx.f(jpdf$imp[s])
  save(ftx,file = f)
}
rm(f,jpdf)
```

```{r ftx2pre}
f<-'d/p/pre.RData'
if(file.exists(f)){
  load(f)
} else {
  pre<-ftx2pre.f(ftx,frq = 2,nch=2)
  save(pre,file = f)
}
rm(f,ftx)
```

```{r pre2mlc}
f<-'d/p/mlc.RData'
if(file.exists(f)){
  load(f)
} else {
  mlc<-pre2mlc.f(pre)
  save(mlc,file = f)
}
rm(f)
```

```{r pre2tel}
f<-'d/p/tel.RData'
if(file.exists(f)){
  load(f)
} else {
  tel<-pre2tel.f(pre[!is.na(stm)],stop = 3)
  save(tel,file = f)
}
rm(f)
```

```{r pre2des,include=T}
des.com<-pre2des.f(pre)
des.stm<-pre2des.f(pre[!is.na(stm)])
knitr::kable(list(des.com,des.stm,data.table(tot=round(des.stm$cnt/des.com$cnt*100,2))),caption = 'Full Text Descriptives', booktabs = TRUE)
```

```{r tamK}
f<-'d/p/clu.RData'
if(file.exists(f)){
  load(f)
} else {
  clu<-fam2clu.f(tel)
  save(clu,file=f)  
}
rm(f)
str(clu)
```

### How many topics?

```{r mlc2mlk}
f<-'d/p/mlk.txt'
if(file.exists(f)) {mlk<-fread(f)} else {
  mlc2mlk.f(mlc) %>% fwrite(f)
  pbapply::pbreplicate(999,mlc2mlk.f(mlc) %>% fwrite('d/p/mlk.txt',append = T),cl = 6)
}
rm(f)
mlk<-melt(mlk,measure.vars = names(mlk),variable.name = 'level',value.name = 'K') %>% setkey(level)
```

```{r mlk2sim}
f<-'d/b/sim.RData'
if(file.exists(f)) {load(f)} else {
  sim<-mlk2sim.f(mlk)
  save(sim,file=f)
}
rm(f)
```

```{r sim-fig,include=T,fig.cap='Distribution of K by convex hull'}
sim$fig
```

```{r mlk2k}
k<-mlk[,.(k=pbapply::pbreplicate(1e4,psych::kurtosi(K %>% sample(replace = T)),cl = 1)),by=level][,.(e=mean(k),se=sd(k),l99=quantile(k,.005),u99=quantile(k,.995),`p>=0`=mean(k>=0)),by=level]
```

```{r k-fig,include=T,eval=F}
kable(k)
```

```{r mlk-tab,include=T}
kable(sim$tab)
```

### Model selection

```{r mlc2stm}
# mod<-stmbow2lda.f(list(documents=mlc$par,vocab=mlc$voc),k=50,out.dir='d/p',verbose=F,visualize.results = T)
# mod$top.word.phi.beta[mod$top.word.phi.beta==0] <-.Machine$double.eps
# debugonce(lda2viz.f)
# viz<-lda2viz.f(mod,'d/b')
```

### Making sense

## An end.

Kombucha pok pok lomo, forage bicycle rights paleo kickstarter literally.  Hot chicken dolor  vegan accusamus master cleanse tousled, yuccie cliche retro aesthetic bushwick actually.  Ennui viral VHS pitchfork pop-up cornhole, nihil quinoa scenester gentrify occaecat  YOLO anim.  Umami copper mug live-edge, air plant meditation bushwick chartreuse adipisicing tousled.  Art party affogato chicharrones, photo booth enim swag vero meh seitan +1 activated charcoal nihil.  Etsy gluten-free authentic mixtape.  Shabby chic duis  90's pop-up pinterest, lumbersexual mollit  cillum.
