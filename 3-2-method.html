<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Knowledge of the U.S. Social Sciences</title>
  <meta name="description" content="Knowledge of the U.S. Social Sciences">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Knowledge of the U.S. Social Sciences" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Knowledge of the U.S. Social Sciences" />
  
  
  

<meta name="author" content="Brooks Ambrose">


<meta name="date" content="2019-08-28">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="3-1-genre.html">
<link rel="next" href="3-3-data.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.8.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.41.3/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.41.3/plotly-latest.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<link href="libs/wordcloud2-0.0.1/wordcloud.css" rel="stylesheet" />
<script src="libs/wordcloud2-0.0.1/wordcloud2-all.js"></script>
<script src="libs/wordcloud2-0.0.1/hover.js"></script>
<script src="libs/wordcloud2-binding-0.2.0/wordcloud2.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.4/datatables.js"></script>
<link href="libs/dt-core-1.10.16/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.16/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.16/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<script src="libs/d3-4.5.0/d3.min.js"></script>
<script src="libs/sankey-1/sankey.js"></script>
<script src="libs/sankeyNetwork-binding-0.4/sankeyNetwork.js"></script>
<script src="libs/forceNetwork-binding-0.4/forceNetwork.js"></script>



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Brooks Ambrose</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Getting Started</a><ul>
<li class="chapter" data-level="" data-path="abstracts.html"><a href="abstracts.html"><i class="fa fa-check"></i>Abstracts</a><ul>
<li class="chapter" data-level="" data-path="abstracts.html"><a href="abstracts.html#what-to-read-how-the-archive-pits-profession-against-education"><i class="fa fa-check"></i>What to Read? How the Archive pits Profession against Education</a></li>
<li class="chapter" data-level="" data-path="abstracts.html"><a href="abstracts.html#social-science-disciplines-today"><i class="fa fa-check"></i>Social Science Disciplines Today</a></li>
<li class="chapter" data-level="" data-path="abstracts.html"><a href="abstracts.html#genre-and-the-literature"><i class="fa fa-check"></i>Genre and the Literature</a></li>
<li class="chapter" data-level="" data-path="abstracts.html"><a href="abstracts.html#vocabularies-of-anthropology-and-sociology-1888-1922"><i class="fa fa-check"></i>Vocabularies of Anthropology and Sociology, 1888-1922</a></li>
<li class="chapter" data-level="" data-path="abstracts.html"><a href="abstracts.html#the-social-science-citation-landscape-1900-1940"><i class="fa fa-check"></i>The Social Science Citation Landscape, 1900-1940</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-int.html"><a href="1-int.html"><i class="fa fa-check"></i><b>1</b> What to Read? How the Archive pits Profession against Education</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-status-exclusion.html"><a href="1-1-status-exclusion.html"><i class="fa fa-check"></i><b>1.1</b> Status exclusion</a></li>
<li class="chapter" data-level="1.2" data-path="1-2-generic-isomorphism.html"><a href="1-2-generic-isomorphism.html"><i class="fa fa-check"></i><b>1.2</b> Generic isomorphism</a></li>
<li class="chapter" data-level="1.3" data-path="1-3-influence.html"><a href="1-3-influence.html"><i class="fa fa-check"></i><b>1.3</b> Influence</a></li>
<li class="chapter" data-level="1.4" data-path="1-4-discipline.html"><a href="1-4-discipline.html"><i class="fa fa-check"></i><b>1.4</b> Discipline</a></li>
<li class="chapter" data-level="1.5" data-path="1-5-history.html"><a href="1-5-history.html"><i class="fa fa-check"></i><b>1.5</b> History</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-dis.html"><a href="2-dis.html"><i class="fa fa-check"></i><b>2</b> Social Science Disciplines Today</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-kd-dq1.html"><a href="2-1-kd-dq1.html"><i class="fa fa-check"></i><b>2.1</b> JSTOR Journals</a></li>
<li class="chapter" data-level="2.2" data-path="2-2-network-mode-projection.html"><a href="2-2-network-mode-projection.html"><i class="fa fa-check"></i><b>2.2</b> Network Mode Projection</a></li>
<li class="chapter" data-level="2.3" data-path="2-3-network-community-detection.html"><a href="2-3-network-community-detection.html"><i class="fa fa-check"></i><b>2.3</b> Network Community Detection</a></li>
<li class="chapter" data-level="2.4" data-path="2-4-network-visualization.html"><a href="2-4-network-visualization.html"><i class="fa fa-check"></i><b>2.4</b> Network Visualization</a></li>
<li class="chapter" data-level="2.5" data-path="2-5-results.html"><a href="2-5-results.html"><i class="fa fa-check"></i><b>2.5</b> Results</a></li>
<li class="chapter" data-level="2.6" data-path="2-6-discussion.html"><a href="2-6-discussion.html"><i class="fa fa-check"></i><b>2.6</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-gen.html"><a href="3-gen.html"><i class="fa fa-check"></i><b>3</b> Genre and the Literature</a><ul>
<li class="chapter" data-level="3.1" data-path="3-1-genre.html"><a href="3-1-genre.html"><i class="fa fa-check"></i><b>3.1</b> Genre</a></li>
<li class="chapter" data-level="3.2" data-path="3-2-method.html"><a href="3-2-method.html"><i class="fa fa-check"></i><b>3.2</b> Method</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-2-method.html"><a href="3-2-method.html#distant-sampling"><i class="fa fa-check"></i><b>3.2.1</b> Distant sampling</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-2-method.html"><a href="3-2-method.html#no-cigar"><i class="fa fa-check"></i><b>3.2.2</b> No cigar</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-2-method.html"><a href="3-2-method.html#topic-models"><i class="fa fa-check"></i><b>3.2.3</b> Topic Models</a></li>
<li class="chapter" data-level="3.2.4" data-path="3-2-method.html"><a href="3-2-method.html#qualitative-cross-validation"><i class="fa fa-check"></i><b>3.2.4</b> Qualitative Cross Validation</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-3-data.html"><a href="3-3-data.html"><i class="fa fa-check"></i><b>3.3</b> Data</a></li>
<li class="chapter" data-level="3.4" data-path="3-4-estimation.html"><a href="3-4-estimation.html"><i class="fa fa-check"></i><b>3.4</b> Estimation</a></li>
<li class="chapter" data-level="3.5" data-path="3-5-diagnostics.html"><a href="3-5-diagnostics.html"><i class="fa fa-check"></i><b>3.5</b> Diagnostics</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-5-diagnostics.html"><a href="3-5-diagnostics.html#lower-tail-probabilities"><i class="fa fa-check"></i><b>3.5.1</b> Lower tail probabilities</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-5-diagnostics.html"><a href="3-5-diagnostics.html#topic-graphs"><i class="fa fa-check"></i><b>3.5.2</b> Topic Graphs</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-5-diagnostics.html"><a href="3-5-diagnostics.html#summary"><i class="fa fa-check"></i><b>3.5.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3-6-topic-interpretation.html"><a href="3-6-topic-interpretation.html"><i class="fa fa-check"></i><b>3.6</b> Topic interpretation</a><ul>
<li class="chapter" data-level="3.6.1" data-path="3-6-topic-interpretation.html"><a href="3-6-topic-interpretation.html#journals"><i class="fa fa-check"></i><b>3.6.1</b> Journals</a></li>
<li class="chapter" data-level="3.6.2" data-path="3-6-topic-interpretation.html"><a href="3-6-topic-interpretation.html#belwether-texts"><i class="fa fa-check"></i><b>3.6.2</b> Belwether texts</a></li>
<li class="chapter" data-level="3.6.3" data-path="3-6-topic-interpretation.html"><a href="3-6-topic-interpretation.html#terms"><i class="fa fa-check"></i><b>3.6.3</b> Terms</a></li>
<li class="chapter" data-level="3.6.4" data-path="3-6-topic-interpretation.html"><a href="3-6-topic-interpretation.html#dossiers"><i class="fa fa-check"></i><b>3.6.4</b> Dossiers</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="3-7-topic-clusters.html"><a href="3-7-topic-clusters.html"><i class="fa fa-check"></i><b>3.7</b> Topic clusters</a></li>
<li class="chapter" data-level="3.8" data-path="3-8-reading-strata.html"><a href="3-8-reading-strata.html"><i class="fa fa-check"></i><b>3.8</b> Reading strata</a><ul>
<li class="chapter" data-level="3.8.1" data-path="3-8-reading-strata.html"><a href="3-8-reading-strata.html#document-composition"><i class="fa fa-check"></i><b>3.8.1</b> Document composition</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="3-9-discussion-1.html"><a href="3-9-discussion-1.html"><i class="fa fa-check"></i><b>3.9</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-voc.html"><a href="4-voc.html"><i class="fa fa-check"></i><b>4</b> Vocabularies of Anthropology and Sociology, 1888-1922</a><ul>
<li class="chapter" data-level="4.1" data-path="4-1-kd.html"><a href="4-1-kd.html"><i class="fa fa-check"></i><b>4.1</b> Knowledge Development</a></li>
<li class="chapter" data-level="4.2" data-path="4-2-kd-dq2.html"><a href="4-2-kd-dq2.html"><i class="fa fa-check"></i><b>4.2</b> Social Science Journals</a></li>
<li class="chapter" data-level="4.3" data-path="4-3-topics-ideas.html"><a href="4-3-topics-ideas.html"><i class="fa fa-check"></i><b>4.3</b> Topics ≟ Ideas</a></li>
<li class="chapter" data-level="4.4" data-path="4-4-kd-lit.html"><a href="4-4-kd-lit.html"><i class="fa fa-check"></i><b>4.4</b> Prior Work</a></li>
<li class="chapter" data-level="4.5" data-path="4-5-information.html"><a href="4-5-information.html"><i class="fa fa-check"></i><b>4.5</b> Information</a></li>
<li class="chapter" data-level="4.6" data-path="4-6-meaning.html"><a href="4-6-meaning.html"><i class="fa fa-check"></i><b>4.6</b> Meaning</a></li>
<li class="chapter" data-level="4.7" data-path="4-7-communication.html"><a href="4-7-communication.html"><i class="fa fa-check"></i><b>4.7</b> Communication</a></li>
<li class="chapter" data-level="4.8" data-path="4-8-full-text.html"><a href="4-8-full-text.html"><i class="fa fa-check"></i><b>4.8</b> Full-Text</a></li>
<li class="chapter" data-level="4.9" data-path="4-9-kd-dd.html"><a href="4-9-kd-dd.html"><i class="fa fa-check"></i><b>4.9</b> Data</a></li>
<li class="chapter" data-level="4.10" data-path="4-10-kd-dp1.html"><a href="4-10-kd-dp1.html"><i class="fa fa-check"></i><b>4.10</b> Sampling</a></li>
<li class="chapter" data-level="4.11" data-path="4-11-units-of-analysis.html"><a href="4-11-units-of-analysis.html"><i class="fa fa-check"></i><b>4.11</b> Units of Analysis</a></li>
<li class="chapter" data-level="4.12" data-path="4-12-kd-dp2.html"><a href="4-12-kd-dp2.html"><i class="fa fa-check"></i><b>4.12</b> Topics</a><ul>
<li class="chapter" data-level="4.12.1" data-path="4-12-kd-dp2.html"><a href="4-12-kd-dp2.html#how-many-topics"><i class="fa fa-check"></i><b>4.12.1</b> How many topics?</a></li>
</ul></li>
<li class="chapter" data-level="4.13" data-path="4-13-model-selection.html"><a href="4-13-model-selection.html"><i class="fa fa-check"></i><b>4.13</b> Model selection</a></li>
<li class="chapter" data-level="4.14" data-path="4-14-cit.html"><a href="4-14-cit.html"><i class="fa fa-check"></i><b>4.14</b> The Social Science Citation Landscape, 1900-1940</a><ul>
<li class="chapter" data-level="4.14.1" data-path="4-14-cit.html"><a href="4-14-cit.html#scholarly-communication-vs-knowledge-terrain"><i class="fa fa-check"></i><b>4.14.1</b> Scholarly Communication vs Knowledge Terrain</a></li>
<li class="chapter" data-level="4.14.2" data-path="4-14-cit.html"><a href="4-14-cit.html#wok"><i class="fa fa-check"></i><b>4.14.2</b> Mapping Knowledge Terrain</a></li>
<li class="chapter" data-level="4.14.3" data-path="4-14-cit.html"><a href="4-14-cit.html#disciplines-as-a-large-world-co-reference-network"><i class="fa fa-check"></i><b>4.14.3</b> Disciplines as a Large World Co-reference Network</a></li>
</ul></li>
<li class="chapter" data-level="4.15" data-path="4-15-methods.html"><a href="4-15-methods.html"><i class="fa fa-check"></i><b>4.15</b> Methods</a></li>
<li class="chapter" data-level="4.16" data-path="4-16-data-1.html"><a href="4-16-data-1.html"><i class="fa fa-check"></i><b>4.16</b> Data</a></li>
<li class="chapter" data-level="4.17" data-path="4-17-results-1.html"><a href="4-17-results-1.html"><i class="fa fa-check"></i><b>4.17</b> Results</a><ul>
<li class="chapter" data-level="4.17.1" data-path="4-17-results-1.html"><a href="4-17-results-1.html#continents"><i class="fa fa-check"></i><b>4.17.1</b> Continents</a></li>
<li class="chapter" data-level="4.17.2" data-path="4-17-results-1.html"><a href="4-17-results-1.html#peaks"><i class="fa fa-check"></i><b>4.17.2</b> Peaks</a></li>
<li class="chapter" data-level="4.17.3" data-path="4-17-results-1.html"><a href="4-17-results-1.html#valleys"><i class="fa fa-check"></i><b>4.17.3</b> Valleys</a></li>
<li class="chapter" data-level="4.17.4" data-path="4-17-results-1.html"><a href="4-17-results-1.html#do-reference-lists-describe-author-knowledge"><i class="fa fa-check"></i><b>4.17.4</b> Do reference lists describe author knowledge?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/brooksambrose" target="blank">GitHub</a></li>
<!--<li><a href="https://www.ischool.berkeley.edu/people/brooks-ambrose" target="blank">UC Berkeley</a></li>-->

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Knowledge of the U.S. Social Sciences</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="method" class="section level2">
<h2><span class="header-section-number">3.2</span> Method</h2>
<p>As I have said, the first consequence of eschewing disciplinary limitations is to bloat the size of the “literature” on genre, since no uses of the term would be excluded. An empirical approach to the standard academic convention of a literature review will help reign in the scale and complexity of the task. My aim, however, remains practical rather than scientific. The methods need to be good enough to yield results that offer something new above a traditional literature review relying on library search and disciplinary wisdom about what is important. This is not because a scientific approach is undesirable, it is that it is not yet demanded of “the literature”. Sociologists are not expected to take a sociological orientation toward the history of their fields. Rather the literature review serves the social purpose of taking a position in a field of cultural production. It is a listing of a roster of political support and rivalry, and an advertisement to attract a desired audience.</p>
<p>To take an empirical approach to the literature review would be subversive were it not the first function of disciplinary genres to render atypical draws from the archive irrelevant. Disciplinary subfields, genres, are credentialed by secret sets of references, and most comers are held at the door. This in and of itself can be subversive of even more arbitrary club rules, namely those of educational pedigree, such that anyone willing to invest in a presentation of the genre definition will be granted access to the venues, if not the invisible colleges, of the subfield. To be admitted to the arena is no guarantee of achievement within it, but it is a start. Nevertheless, the scale of the archive will always supply entropy enough to create a deterrent of flotsam and jetsam around subfields composed of projects and persons who either never cracked the code or who willfully eschewed it.</p>
<div id="distant-sampling" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Distant sampling</h3>
<p>The research strategy here attempts to parry the entropic tendency of the archive by substituting human for machine limits. The methodological premise of a meta-analysis of genre is that the Gordian knot of the global cultural complexity of the archive can be cut by stratified sampling. I use a large digital archive of texts, JSTOR, to represent the whole of the academic archive. Though clearly a toy representing only a fraction of all networked scholarly produce, JSTOR is large enough to easily surpass individual cognition and compel the equivalent types of complexity reduction facing any researcher approaching the real archive via their local university library portal.</p>
<p>I use a simple term search of the keyword “genre” to define half of a sampling frame.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> I could then take a simple random sample of texts, analyze how each uses the term genre, develop a classification scheme, and enumerate the different uses of the term. Unfortunately, a small sample in a statistical sense may be larger than a poor researcher can handle. 1,000 texts is not large statistically, but it is huge from a content analysis perspective. What’s worse, 1,000 texts may still exclude, by random chance, small subcultures of the term. Stratification within a more or less global sampling frame resolves this issue by delineating those subcultures so none would be left out.</p>
<p>Alas, the JSTOR digital archive lacks subject labels at the article level, though it does include them for book chapters and for journals. While not foolish, inheriting a journal label to the articles included within it may be a coarse approximation if within-journal content variation exceeds between-journal variation. We can use text analytic classification methods to cluster articles directly and discover latent groups of articles, and in so doing we can have an independent standard to compare to the discipline labels given to journals. It is an open question whether such methods align with what we have discussed above as disciplinary and subdisciplinary groupings, for us whether regularities in vocabulary correspond to regularities in the meaning of the term genre. If they do not, then the study will only be a stop en route to a true census of the uses of the term genre, and the contribution will be to have interrogated the quality of the methods used, though this would be a small consolation indeed! Even so, for a new method to claim to be able to improve on conventional wisdom, I behoove myself to proceed methodically.</p>
<p>The choice in computational text analysis (CTA) about how to represent texts as data hinges on whether word order is preserved. The older and more tested approach is to not preserve word order. The name given to this “bag of words” format reminds one of its inelegance. A bag of words is a frequency table for each document counting up the number of times particular words are used, a representation that effectively reduces a text to its vocabulary. It is the analyst’s crude operational decision to treat vocabularies as indicators of meaning, but social scientists conventionally insist on cross validation via qualitative analysis. While the ambitions of computational text analysis may start with a replacement of, for instance, the standard literature review, the conventional distrust, at least in sociology, of mathematical models of text makes CTA more of a sampling method than an analytic method. The study will culminate in a reading of texts, albeit one that is different than traditional qualitative analysis because the CTA researcher welcomes the introduction of interpretive bias from an understanding of the mathematical model before, during, or after the texts are read. In the game of “choose your influence”, CTA is one choice while disciplinary wisdom is another.</p>
<p>There are two types of classification methods in text analysis, direct document clustering and topic modeling. Direct document clustering treats the bag of words as a vector space and calculates distance or similarity metrics between documents, which are then clustered. In a topic model, the relationship between documents is mediated by an unobserved but latently modeled representation of their content; documents are similar because they are formed from the same topics.</p>
<p>Whichever approach one takes, and both may be used, recall that the goal is to organize the texts into strata for the purpose of stratified sampling. We said that we wish to typify and enumerate the different uses of the term genre. By qualitative analysis, we could read every text in a simple random sample and come up with a theory of the use of genre in that text. The demerits of this approach are several <span class="citation">(c.f. Nelson <a href="bibliography.html#ref-Nelson2017Computational" role="doc-biblioref">2017</a>:5)</span>. It would take longer than we want even for too small a sample. We are not humanists and have not been trained in text analysis (this will hound us no matter what). Fatigue will set in, and accuracy and consistency will suffer. We may limit our set of theories to spare us the agony of complexity. It will be hard to reproduce our results. There may be path dependency with a different reading order producing different theories. On the upside, we would be more educated for it.</p>
<p>Instead, we will stratify the sample, and it is in the configuration of the strata that much of the work will be done. The strata impose upon our interpretation of the texts the assumption of sameness.</p>
</div>
<div id="no-cigar" class="section level3">
<h3><span class="header-section-number">3.2.2</span> No cigar</h3>
<p>The popular yet maligned distant reading approach taken by digital humanists <span class="citation">(e.g. Moretti <a href="bibliography.html#ref-Moretti2005Graphs" role="doc-biblioref">2005</a>)</span> is being taken up with gusto by social scientists who are less skeptical of quantitative methods <span class="citation">(e.g. DiMaggio, Nag, and Blei <a href="bibliography.html#ref-DiMaggio2013Exploiting" role="doc-biblioref">2013</a>)</span>. Following Nelson <span class="citation">(<a href="bibliography.html#ref-Nelson2017Computational" role="doc-biblioref">2017</a>)</span> I employ a quantitative analysis of texts not to replace human reading with machine reading but to support reproducibility in traditional qualitative content analysis. While CTA makes it possible to dispense with reading altogether, knowledge, understanding, and the cultural logics of arguments–especially their ontologies–are still only obtainable by reading primary texts, closely or not. The most radical interpretive CTA method would involve deep neural net supervised machine learning, which may be able to predict how a particular human reader would classify a text without their needing to read it, though this has never been demonstrated. What I gain from CTA is guidance in answering the question of what to read, and perhaps in what order to do so.</p>
<p>As we know, the question of what to read is answered institutionally for scholars already by way of canon, curriculum, word of mouth, and digital reference term search services. These are their own forms of distant reading, because they each make obsolete the archaic image, true of figures like Weber, of a scholar buried in library stacks reading everything they come across (and so it has been said of Weber, forgetting nothing).<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> These contemporary shortcuts are historically arbitrary, but what is important is first that they serve the function of reducing the overwhelming cognitive complexity of published scholarship, and second that they structure that reduction in the same way for all scholars. An arbitrary reduction needs to be consistent to act as an infrastructure for subdisciplinary scholarship, otherwise scholars would find themselves located in different literatures.</p>
<p>If distant reading is a criticism of close reading then it has a big hill to climb especially among humanists who are trained to deal very carefully with texts. In the social sciences a type of customary distant reading is that of ritual citations, those that have developed a meaning that may be oblique to their content or at odds with the intentions of the the original authors. A ritual citation is simply one that is cited but not read, but also one that is so often used that its socially acceptable usages are known from other secondary accounts. For all the lack of due diligence in the use of ritual citations, their socially understood meanings are better than the thoroughly perfunctory citation, those included because they were returned by a digital reference service and never read by anyone.</p>
<p>What are the social patterns of the traditional literature review are topics for the sociology of knowledge and science and for the information sciences. This is not the task of the current study. What we take from the traditional approach is the consequences of excluding large segments of intellectual history. What CTA makes possible for the first time is a nonarbitrary, inclusive analysis of <em>all</em> content in a digitized corpus. It will not necessarily be a good analysis, but what it will lack in quality it will make up for in coverage. A CTA approach to the literature review will at least make clear what lacuna would be left by the traditional approach. They also reduce the potential idiosyncrasy of a particular author’s literature review because, unlike a personal reading, a CTA model can be communicated precisely.</p>
<p>Of course the cognitive limitation of how much any scholar can actually read and understand remains. There will be an exclusion mechanism no matter what, therefore a chief assumption of a CTA literature review is that corpus segmentation is both possible and that some reduced form of reading, some sampling procedure, can be said to be representative of the unread portion in each segment. These representative texts will be subjected to a close reading, but their interpretation will be generalized to unread documents. Hence I call this a “no cigar” approach to reading, as in “close but”. If on the contrary to the assumption no two snowflakes are alike, then the enterprise of knowing more than we have before is fraught, and CTA becomes yet another arbitrary reducer.</p>
<p>What is worse, or perhaps better, is that there is reason to believe that idiosyncrasy itself is an historically variable feature of disciplines. If institutional isomorphism has proceeded to some high level in contemporary disciplines, then the assumption that reading the bellwether texts is as good as reading the entire herd may hold. If this is true, however, it raises as many questions about the process of institutionalization in cultural production as it answers about the potential to learn truer versions of intellectual history.</p>
</div>
<div id="topic-models" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Topic Models</h3>
<p>We have referred generically to computational text analysis, and now we can discuss the topic model as our technique of choice. There are many ways of estimating a topic model (e.g. the famous Latent Dirichlet Allocation (LDA) estimator) but the model itself is simple. It is a latent variable model that decomposes a document-by-term matrix–in which every document is represented as a frequency distribution over every term appearing in the corpus–into two unobserved matrices:</p>
<ul>
<li>a topic-by-term matrix, and</li>
<li>a document-by-topic matrix.</li>
</ul>
<p>Topics are directly represented by they topic-by-term matrix. A topic is a probability distribution over a vocabulary. To draw on a topic means to choose vocabulary as a random draw from this distribution, where words with higher probabilities will be chosen more often. In the case of genres we might imagine a topic about film and a topic about music. Some words may be important (highly probable), to both topics, such as the word “genre”, while others would be distinct, such as the words “movie” (probable for film but improbable for music) and “band” (vice versa).</p>
<p>Note the usual distributional bait-and-switch of categorical statistical analysis, where observed count data are operationalized as the outcomes of unobserved probabilities. The probabilities are what will be estimated, not the counts. The importance of this will be explored in the sections on estimation and diagnostics, but suffice to say that the differences between probabilities and counts encapsulate many of the difficulties applied researchers encounter when using topic models.</p>
<p>Given topics as term distributions, a document can be represented not as a distribution over terms, but as a distribution over topics. The topic mediates the relationship between documents and terms. In order to generate diction for a document, all that need be understood is the ratio of topics out of which it is composed. This is sometimes explained as a generative mechanism; to ask what word will be chosen next in composing a document, one first samples from the document’s own topic distribution to decide which topic the word will be drawn from, and given that topic, one then samples from the topic’s word distribution to decide which word will be included in the document. A document’s topic probabilities also create the expectation of how many words are attributed to each topic. A document with topic probabilities .7 from music and .3 from film would be expected to be 70 percent about music and 30 percent about film, making for a parsimonious albeit reductive description of document content.</p>
<p>It is important not to overinterpret a topic model. To describe a topic model as “generative” implies that it explains how documents are written. Such a generative metaphor reveals the absurdity of a topic model as a representation of writing. Not to mention the fact that punctuation tends not to be represented (though it could be), the terms chosen would be in a random order incapable of making meaningful sentences. Hence it is best to avoid the generative metaphor as an explanation of texts. If topic models touch on the generation of real, meaningful documents, it is only in a very limited sense. What the topic model really represents is how vocabularies are organized to condition an author’s diction. A vocabulary can be thought of as an infrastructure of meaning more trivial than grammar or syntax and much more trivial than concepts or ideas. A topic is a simple list of words that is known or knowable across all authors in a field. Topics do not tell stories; authors tell stories in part by making diction choices that are conditioned by topics.</p>
<p>From a sense or meaning making perspective topics are trivial; this is because so little is known about what an author says by knowing the topic or even the term distribution of a document. What topics are useful for, however, is the segmentation or cartography of a corpus. Topics are really a global feature, perhaps a cultural feature, of a corpus of texts that is itself meaningfully selected. If indeed a field of texts is oriented to common if not always overlapping vocabularies, then topics can represent this well.</p>
<p>A topic model could be posited based on the domain knowledge of an expert, and this would be a form of estimation. The practical value of statistical topic modeling is that the unobserved topics can be induced, with a raft of statistical assumptions, directly from the observed document-by-term matrix to arrive at a model with the features just described. An estimated topic model will contain several other parameters filling in assumptions necessary to make it possible to identify the unobservable topic probabilities in each of the two matrices of the model. For instance, in LDA models the concentration parameter commonly called alpha makes an assumption about how many topics tend to comprise each document. Alpha values close to zero make it very likely that documents are composed of only one topic, while an alpha value greater than one increase the chance that a document will be decomposed into several topics. Alpha equal to one creates no tendency, so concentrated and diffuse mixtures are all equally likely to occur. It would behoove a researcher to make an informed decision about this parameter, yet software often sets an arbitrary default that the user may or may not be fully aware of.</p>
<div id="choosing-k" class="section level4">
<h4><span class="header-section-number">3.2.3.1</span> Choosing K</h4>
<p>Finally, topic models require the analyst to choose the number of topics K. The approach we take to guiding this decision is not to expect one correct specification of K but rather to see it as a changing resolution. A K=2 model usefully bifurcates the sample and is not simply wrong because it is too restrictive. As K increases we expect the samples to continue to divide as new parameter spaces become available to partition the sample. While this is not strictly a hierarchical design, since each K model is fit independently, we should expect to see aspects of hierarchical topics as well as some degree of stability in the relationships among topics.</p>
<p>Between model cross-validation means that document and term groupings should be relatively stable as K increases. The document overlaps between, say, a three topic model and a four topic model should not be random. By graphing the document overlaps between pseudo hierarchically organized models, it should be clear which topics are the most stable and which are constituted partly by chance or by spurious association. An ensemble approach would then recommend itself; if the content of a topic is stable across different specifications of K, within limits, then we should have even more confidence in that topic.</p>
<p>When parameter space is limited the content with the strongest signal will come to define the topic, but the document by term vector will be contaminated with content that would be separated given more space. For sets of documents that are constituted by multiple true topics, we expect to see splitting of larger topics as the resolution increases to meet the real diversity. Hierarchy will reveal itself as topics with stronger topic signals subsume weaker ones until K reaches a point where there is enough space to separate them. On the other hand, in the classic trade-off between variance and bias, where K overshoots the true number of topics, we expect to see random splitting and possibly “dust bin” effects where spare topics allow larger topics to prune their weaker term associations. Indeed dust bins may appear even before the true K is reached. Where the term proportions explained by topics are very unequal, it may pay during estimation to treat a true smaller topic as a dust bin for a larger topic, because the optimization gains of clarifying a larger topic may be greater than the losses of confusing a smaller one.</p>
<p>Another interesting feature of this approach is that it shows when and how topics are able to appear given the parameter space constraints. We expect the most dominant topics, those that appear at low K and remain stable as K increases, to derive from vocabularies that are both distinctive and used often. The content with the strongest signal will be “FREX” terms, terms that are both frequent and exclusive <span class="citation">(Bischof and Airoldi <a href="bibliography.html#ref-Bischof2012Summarizing" role="doc-biblioref">2012</a>)</span>. Frequent means they have high counts in the overall corpus either due to occurrence across many texts or to very large counts in a few texts. Exclusivity (or monosemy, the opposite of polysemy) means that terms co-occur with an invariable set of additional terms. Exclusivity is related to the notion of anchor words that are maximally exclusive, appearing in only one topic, but likely very infrequent.The exclusivity of terms relates to the separability of topics <span class="citation">(Arora et al. <a href="bibliography.html#ref-Arora2018Learning" role="doc-biblioref">2018</a>)</span>, while the topic frequency of terms relates to the topic’s contribution to explaining global corpus frequencies, that is, to maximizing model likelihood during estimation.</p>
<p>It should be possible to predict a priority for topic emergence as models increase parameter space for topics. First, we expect topic model estimators would be very tuned to picking out even a handful of texts written in a different language than the main corpus, as terms within those documents would be both frequent and exclusive. We should expect technical jargon to also send a strong signal for it’s high exclusivity. Indeed, these special vocabularies are salient for both humans and machines for the same reason; they are easy to disassociate from the rest of the text. The priority, however, for the estimators will be to explain global term frequencies, so jargon will likely be behind frequent terms that appear across multiple topics, as in the case of polysemy or the more common case of simple language ambiguity. Trailing the pack and the last to emerge will be, as we have discussed, idiosyncrasy.</p>
<p>Let us remind ourselves of what badness means, because a bad model in a statistical sense may very well be the correct model for the analytical purpose of the researcher. A human reader with an interpretive goal in mind can be quite apt at scanning text content and ignoring what she finds to be irrelevant. Some of this seeming irrelevance has to do with the syntactic structure of language, while others a reader knows by experience to be elements of style and rhetoric in their field. The interpretive goal becomes like a flashlight that darkens much more content than it illuminates.</p>
<p>While human readers tend to make sense of only small portions of texts, the machine is not so lucky as to have the human capacity for selective ignorance. The topic model estimator sees and makes sense of everything at once. This is sometimes at cross purposes to the researcher’s hope of complexity reduction, because in interpreting the model rather than the text she will be told by the model that something is important even if she would have easily ignored the same context in the natural setting. A topic model that is both correctly specified and accurately fit on a large corpus will likely have dozens or even hundreds of topics. Such a variegated classification scheme is likely to contain some topics that a reader would consider to be redundant, for instance, because they are about the same thing yet differ for an irrelevant stylistic vocabulary. Many others will simply be irrelevant to her research agenda. The task of sorting through the topics is supposed to be easier than sorting through the original texts, yet the researcher is sure to find many inscrutable lists of FREX terms in a that can only be understood with reference to classified articles.</p>
<p>In the case that a correct model of vocabulary clustering is actually too complicated to be helpful, the correct research decision may be to deliberately underspecify the model. We can imagine the real topics as guests standing in a line of priority, and the model is like a wedding with a limited number of tables. The guests with the strongest relations among them sit at the first table, the next strongest at the second and so on until all of the tables are full. In their munificence the happy couple still lets the remaining guests in, and what can they do but pull up a chair at the tables where perhaps they already know one of the more honored guests. However, if an additional table, or several, were to be found, the crashers could look among themselves for close relationships, even perhaps peeling away a priority guest, to form a separate group. Prior to there being room, that group would be unrecognizably distributed among several tables. The group would not exist, but the individuals would, and they would find a seat somewhere.</p>
<p>Just as the arrival of wedding crashers at the tables does not alter the identity of the core group that constituted them to begin with, a model where K is set too low will serve to highlight those vocabularies that send the strongest signals, even if the tails of these topic distributions are contaminated by unidentified topics. From a frequency and exclusivity standpoint the unidentified topics are the less important ones. Smaller and less distinct groups will be occluded in an underspecified model, and whether these are substantively important is a theoretical decision.</p>
<table style="text-align:center">
<caption>
<span id="tab:frex">Table 3.1: </span><strong>Content priority across frequency and exclusivity</strong>
</caption>
<tr>
<td colspan="3" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
exclusivity
</td>
<td>
frequency
</td>
<td>
</td>
</tr>
<tr>
<td colspan="3" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
low
</td>
<td>
high
</td>
</tr>
<tr>
<td style="text-align:left">
low
</td>
<td>
<ol start="4" style="list-style-type: decimal">
<li>idiosyncrasy
</td>
<td>
<ol start="2" style="list-style-type: decimal">
<li>polysemy/ambiguity
</td>
</tr>
<tr>
<td style="text-align:left">
high
</td>
<td>
<ol start="3" style="list-style-type: decimal">
<li>jargon
</td>
<td>
<ol style="list-style-type: decimal">
<li>foreign language
</td>
</tr>
<tr>
<td colspan="3" style="border-bottom: 1px solid black">
</td>
</tr>
</table></li>
</ol></li>
</ol></li>
</ol></li>
</ol>
<p>Indeed we may never expect idiosyncrasy to emerge as its own topic except in the limiting case. Presumably K can be set so high as to approach the saturation point of a topic for each document. In this event topics that would otherwise appear in common may alter to represent the uncommon parts of a document, and the topic would merely reproduce the term distribution of a particular document. Thus there is a transition from content in common to content idiosyncratic to groups of trivial size and to individual texts in the limiting case. The model is unable to ignore supposedly idiosyncratic content, and will thus find a way to classify it among topics in common, effectively distorting the term vector of those topics. There may be no objective point at which the content in common is neatly separable from the idiosyncratic content; indeed common content evolves only by idiosyncratic innovation. An ensemble approach allows us to observe how particular content moves among topics as parameter space opens up.</p>
<p>Finally, there may be hope that sparse model estimation techniques would ameliorate some of the considerations above. Sparse model regulation, such as those using the L1 or LASSO constraint, bias parameters downward and thus may set trivial regression coefficients nearer to zero. Such an approach may well fail to represent idiosyncrasy at all, which is either a benefit or a hazard. Such a biased model would, by effacing the idiosyncratic portions, yield topics representing only the common portions of documents. This avoids what we have termed contamination at the cost of losing information that we may care about. Thus for sparse model techniques to be used responsibly document residuals would need to be calculated to help recover the unmodeled portions of the texts. The model diagnostics we explore below attempt to separate model parameters into common and idiosyncratic elements, the difference being whether the idiosyncrasy is located in the topic model or in the residuals.</p>
</div>
<div id="bias" class="section level4">
<h4><span class="header-section-number">3.2.3.2</span> Bias</h4>
<p>Before documenting the data preparation below, it is important to keep in mind several sampling and modeling considerations that tend to be overlooked. First, idiosyncrasy is assumed to be unmodelable. A flaw of traditional topic models is that, at one level, all documents are generic. Originality exists only in novel admixtures of vocabularies held in common. Vocabularies that are limited to trivially small sets of works, be they idiosyncrasies of content or style, become sources of bias to topic model estimators. Because idiosyncratic vocabulary is by definition rare, it lacks both the mass of frequency and distribution across documents to be reliably picked up as a topic. Indeed, if each document were expected to contain some idiosyncrasy, then the number of topics needed to catch all of the idiosyncrasy would be equal to the number of texts in the corpus. Each document would then be a combination its own idiosyncratic topic (of which it would account for 100 percent of topic content) and a distribution over other topics held in common. The real number of topics would then be K+N where N is the number of texts and practically always much greater than K. Researchers would balk at including such a large set of extraneous topics, while estimators would both be strained by the greater parameters space and would collide with hyperparameters designed to militate against estimating topics distributed only over a single document.</p>
<p>The impracticality from a modeling perspective of representing idiosyncrasy coincides with the undertheorized tendency among researchers for extreme pruning of idiosyncrasy during data preparation. A more parsimonious modeling solution would be to allow a single extra topic designed to catch all idiosyncrasy. Yet this would tend to violate the assumptions behind construction of the other topics for two reasons, first because one topic would have significant distribution across all documents and second because terms within the topic would never be estimated together as they would really be a mixture of N uncorrelated subtopics.</p>
<p>Idiosyncrasy tends to be pruned in a desire to limit the length of the vocabulary to bring it within the bounds of computational power and the chances of a successful parameter optimization. Depending on the task, however, the researcher may not be so concerned with performance, and may leave plenty of idiosyncrasy in the sample. What then is the effect on the topic estimation of such idiosyncrasy, since the idiosyncrasy must end up somewhere?</p>
<p>First, there will be a tendency to muddy the content of common topics with the particular idiosyncrasies of the documents that happen to draw on them. This in part explains the long, non-zero tails of topic by term distributions, which are usually filtered out during post-estimation and interpretation of the models. We would however expect them to corrupt the error structure of the topic they contaminate, leading to suboptimal estimates of the true terms in the topic.</p>
<p>Second, the document proportion of the contaminated topic will be inflated in the contaminating document. After all, the idiosyncrasy of the document was represented, erroneously, in the contaminated topic. Because of the length of the term vector it is not difficult to imagine the truly pathological case wherein the probability sum of the false portion of the topic is greater than that of the true portion. In this event, a document could be categorized within a topic due more to the false content than to the true content, especially if the idiosyncrasy was placed in topics randomly. Contrary to the effect of random error in an explanatory variable in ordinary least squares linear regression, which is to bias the regression coefficient downward, in a topic model the effect will be to bias the topic probability of a document upward.</p>
<p><span class="citation">DiMaggio et al. (<a href="bibliography.html#ref-DiMaggio2013Exploiting" role="doc-biblioref">2013</a>)</span> represent a typical albeit conservative approach to topic modeling as distant reading. Their data preparation of a newspaper corpus about U.S. arts policy in the 1980s and 1990s resulted in 54,982 unique terms and 7,598 documents <span class="citation">(<a href="bibliography.html#ref-DiMaggio2013Exploiting" role="doc-biblioref">2013</a>:582)</span>. This incredible dimensionality in the term vector, which eliminated only stopwords and a few hand-picked terms and did no stemming, represents a very conservative approach to term filtering admitting to no performance based truncation. They chose a model with 12 topics. Thus in a strict interpretation of their 12-topic model, we are to believe that the extreme idiosyncrasy of news, with all of its historical specificity, is contained in a noise or junk topic rather than creating bias on the estimation of the signal topics. With such a huge term mass to classify and so few topics in which to do it, it is incredible to think that the algorithm would alight on a junk topic rather than using that spot for a signal topic. It is plausible that the noise (and so offensive a term to those reporters trying to say something new!) is distributed across signal topics rather than being safely tossed in the dust bin. To wit, their choice of a low alpha parameter of 0.1, which assumes that each document is generated from relatively few topics, makes it even less likely that the estimator would spend precious parameter space on a noise rather than on a signal topic.</p>
<p><span class="citation">DiMaggio et al. (<a href="bibliography.html#ref-DiMaggio2013Exploiting" role="doc-biblioref">2013</a>)</span> attempt to placate statistical criticism by substituting quantitative, statistical forms of validation for qualitative cross validation of topics. This may be more treacherous than the authors admit. Their analytical approach is:</p>
<ol style="list-style-type: decimal">
<li>Fit the topic model.</li>
<li>Sort the topic by term vectors in decreasing order.
<ol style="list-style-type: lower-alpha">
<li>Split the fat head from the skinny tail.</li>
<li>Interpret the terms in the fat head.</li>
</ol></li>
<li>Sort the topic by document vectors in decreasing order.
<ol style="list-style-type: lower-alpha">
<li>Split the fat head from the skinny tail.</li>
<li>Classify those documents in the fat head according to 2.b.</li>
</ol></li>
<li>Interpret the documents according to 3.b.</li>
</ol>
<p>The sorting procedures are a typical low-hanging fruit use of the model. Even though the model is a much simpler ball of string than the original full text corpus, it is still a very complicated statistical equation with, in this case, 12 * 54,982 + 12 * 7,598 = 750,960 estimated parameters. Sorting the term and document vectors allows the analyst to proceed from an interpretation of the strongest signals toward the weakest, stopping when the author feels satisfied that the research question is addressed. The assumption here is that the strongest statistical signals are unbiased, that when parameters are converted to ranks, and the ranks are converted to truncated lists of words and documents, that those lists are correct.</p>
<p>The specter that I raised above applies to the document ranking more than to the term ranking. A formal feature of topic models is that each topic is composed of all terms in the corpus. Of course this is an artifact rather than an intention of the model, as the goal is to separate relevant from irrelevant terms in the constitution of topics. Similarly, all documents are distributions over all topics, but this is not (necessarily) the intention; again we expect an elbow in the sorted topic document vector in front of which are relevant and after which are irrelevant topics. Any concentration index, such as the Gini coefficient, calculated on the topic term and to a lesser extent the topic document vectors will show very high concentration, where most of the probability is owned by a few elements.</p>
<p>We can test for some of these expectations of bias. A document’s topic assignment may be considered suspect if its term distribution from that topic derives from the low and long tail of the topic, rather than from the select high probability terms normally associated with the topic’s meaning.</p>
</div>
</div>
<div id="qualitative-cross-validation" class="section level3">
<h3><span class="header-section-number">3.2.4</span> Qualitative Cross Validation</h3>
<p>To be sure, topic model parameters may be biased by misspecification, and if we are being fair, by the gargantuan task we ask of them. In part because topic models, notwithstanding their decades of development, remain difficult to validate statistically, and in part because educated people scoff at the idea of machine reading, many researchers ultimately rely on qualitative interpretation to evaluate model quality. Goodness of fit means that topics pass a sniff test upon inspection. A list of words either does or does not inspire a theory of meaningful content, and this theory either is or is not confirmed upon inspection of document with a highly ranked topic probability.</p>
<p>The same scholars who promote qualitative cross validation (QCV) would presumably have bet on John Henry rather than the steam drill. The arguments against the machine, which excels only at recognition, is that it is a ham-fisted intruder into the delicacies of sensemaking, semantics, and interpretation. Meaning operates very differently from information namely by bringing grounding to the response to information. One example of grounding is spreading activation, that when information is presented to the mind by sensation, the mind responds by representing not only a construct of the stimulus but also a network of constructs adjacent in memory to the stimulus. Simply, humans see more than they perceive, but machines cannot.</p>
<p>That machines are dumb because they recognize rather than interpret is not entirely fair. In machine learning the analog to memory, be it treated as semantic grounding or anything else, is mathematical model representation, and the analog to learning is a Bayesian updating of old models with new data. A machine seeing new data with an old model can indeed see more than it perceives. At this moment in the era of computational social science, however, researchers train models for the first time on the data they wish to explain. It is theoretically possible to communicate and transport models from past to present researchers, however this is not done in practice for lack of infrastructure and more importantly because social scientists rarely study the same thing twice. Where data are ample it is possible to simulate a history of memory for the machine using hold out techniques where a model is trained on one sample of the data and applied to predict another sample. Where the goal is to maximize prediction, training and hold out samples are randomly selected. A different approach <span class="citation">(e.g. Nay <a href="bibliography.html#ref-Nay2017Predicting" role="doc-biblioref">2017</a>)</span> involves selecting training and hold out as a process in time. This is a closer approximation to human memory, as humans always approach the present only armed with a memory of the past. In this sense a time ordered model training process may create the same kind of errors on new data that a social institution would.</p>
<p>As clever as the time sorted hold out strategy is, it is unlikely to outperform a supervised approach to model validation wherein human judgements serve either as diagnostics or training materials for model fitting. Human culture is far too expansive to be modeled by a computer for no simpler reason than the data of human memory are always rapidly lost and what is retained is selected for arbitrary historical reasons. What makes the contest between John Henry and the steam drill interesting in the modern era is the social problem of cultural reproduction. Machines will outperform humans only where human history is made more accessible to machines than to humans, which may be a join function of the success of digital archiving coupled with the deterioration of human education.</p>
<p>In the case of topic models, some advocates for the machine go so far as to claim that the topic model actually recovers semantic context <span class="citation">(DiMaggio et al. <a href="bibliography.html#ref-DiMaggio2013Exploiting" role="doc-biblioref">2013</a>:578)</span> or what we have called grounding. Semantic context is a more specialized notion than memory, and it refers to the human capacity for reproducing common meaning. In language viewed through a topic model a large collection of terms defines the topic while only a sample of these terms will be observed in a particular document. In this sense the topic model fills in missing information in the way that meaningful interpretation does. This notion rests on a very strong assumption, however, which is that information tacit in a particular case is explicit in a different case, indeed a quorum of different cases, and that the cases overlap enough to become included under the same topic. With big variation in document length topic models may take grounding, which is properly a community resource, arbitrarily from the longer documents within a corpus thus giving them undue influence over sensemaking. In real sociocultural interaction, a large, exogenous influx of novel term associations would not determine meaning at the margin. Real meaning has legitimacy enforced by interested actors, such that deviant term associations are negatively sanctioned. Topic models only learn from cultural expression and are ignorant of social processes that condition expression. If novel terms are associated in one text with a core of common terms found in many texts, they too will be added to the topic. This is a corruption of the grounding that would not occur in real life.</p>
<p>The estimation of grounding would seem to compete against the other feature of polysemy, that a term may appear in multiple topics each with a different context. How does the machine know that a particular term distribution (document) is a case of missing grounding within the same topic as another document, or is in fact a different topic with a different context? Of course the machine knows nothing other than how to maximize an objective function. Estimators are designed to start from a more or less arbitrary guess and update parameters in the direction of models that are more likely given the data. Indeed, it is the hyperparameter choices of the researcher that often decide which research approaches will win out. For example, the question of whether or not a topic model detects polysemy is operationalized as topic correlation and governed by the choice of the sigma prior, which controls the diagonalization of the correlation matrix, where a constraint toward low topic correlations prohibits detection of polysemy. The current state of software discourages an understanding of how hyperparameter tuning relates to a particular research agenda, and this opacity to the method is a strong driver toward QCV.</p>
<p>Cheap computing does make grid searching across hyperparameter settings possible, if not cost effective, but until this approach is usefully automated it is safe to assume that models will be misspecified in an unknown way, that the model is tuned in a particular arbitrary theoretical direction that is unknown to the researcher. Why would one believe that QCV would inoculate against the hidden bias imposed by the model? To be clear, a biased model is one that will present a vocabulary that <em>does not</em> represent the text accurately. In the conventional use of topic models, the researcher is eager to use the topic as a lens that both arranges documents into relevant subsets (a particular draw from the archive) and primes her interpretation of the documents content by a suggestive list of terms. We wish to keep two forms of QVC error in mind.</p>
<p>The first is classification error. Continuous document by topic probabilities are interpreted categorically according to an explicit or tacit threshold of classification. Explicitly, one could analyze the global decay of topic probabilities and attempt to find natural empirical separations at threshold values. More commonly, the tacit satisficing criterion is met as one walks down the ranked list of documents and eventually decides that they have understood the topic. The error arises in the within-class generalization where classification quality has degraded in a continuous fashion (and past the point reached by our satisficed reader) yet such errors have been effaced by the hard classification rule. In short, by understanding the bellwethers, the researcher only partially understands the corpus and indeed only further mystifies the poorly classified stragglers.</p>
<p>It will help to visualize the statistical situation leading to this error. In the expected case of model misspecification, usually too few topics, we should also expect an urchin shaped quality distribution where on each topic spine are bellwether documents drawn out by their strong signal to be representative of the topic. As one descends the spine of each topic we will begin finding the poorly classified documents collected on the body of the urchin. These documents are representative of no topics, that is, equally representative of all or several topics. For a misspecified model, it is possible that a collection of these stragglers would be given a home in a model with an extra spine, that is, new parameter space for an extra topic. But without a topic to represent them, the analyst may make the mistake of a false generalization from bellwether to straggler documents. Such stragglers may even be halfway up the spine, assuring their classification but for the wrong reason: bellwether documents achieve their topic probability by virtue of words at the head of the sorted topic by term vector, whereas stragglers achieve their lesser but still above threshold topic probabilities from the meaningless long tail of the topic by term vector. This long tail, we must recall, contains terms that may have trivially small topic probabilities when considered separately, but when considered together, because the term vector is so long, their cumulative probability of the false segment of the vector may rival in classification power that of the true segment.</p>
<p>The second is confirmation bias. Readers tend to skim and scan documents more quickly and less carefully when they are told what they are about ahead of time. It is natural for researchers to want to examine the document by term vectors of the topics in order to understand the results of the model and apply the findings to solve research problems. These lists may be very evocative of theoretical assumptions and practical expectations about the corpus, which has not normally been read ahead of time. Theories of the meaning of the term lists are very likely to establish confirmation bias in the reading of the texts. This means that documents that have been classified by a satisficing or threshold rule will be read differently with a theory of the topic in mind than they would have otherwise. Confirmation bias means that the analyst will have a tendency to focus on content that appears to conform to the topic theory while discounting content that contradicts it. Sometimes this will be warranted; after all, a feature of the model is the ability to classify documents into multiple topics. In the pathological case, however, the meaning of the document will be distorted to fit the theory of the topic. A model that causes the reader to misread a document is certainly not helpful, and the pull of confirmation bias tends to be strong even when one is aware of it.</p>
<p>Fortunately we may adjust our research strategy to avoid each of these errors. First, to ameliorate the effects of misclassification, a simple concentration metric such as the Gini coefficient applied to the vector will help discriminate between documents classified strongly into only a few topics (highly concentrated probabilities) from documents that are classified weakly into all (that is none) of the topics (unconcentrated probabilities). To assess a particular topic classification it should be possible to decompose the portion of a document’s text that is estimated to derive from a particular topic. That portion can then be scored according to its weighted average rank of the terms actually contained in the document, with poorly classified texts having lower scores. The utility of this quality scoring is to shine a light on the yet to be correctly classified texts, which may give an indication of when it is warranted to increase the parameter space of the model, and which may substantively reveal the less dominant (perhaps dominated) vocabularies.</p>
<p>Second, it is a simple enough procedure to forestall interpretation of the topic by term vectors until after a direct inspection of documents grouped by their topic classification. Indeed, this may promote a more accurate theory of the topic since terms will be interpreted within context.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="2">
<li id="fn2"><p>TODO, I did not, but should take a random draw of the same size to serve as a control.<a href="3-2-method.html#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>What a scandal it would be if Weber’s lionizers discovered that he had only read text indices! Surely they would bury such a fact. But the point would remain that even if a scholar were able to consume an entire corpus, the sheer scale of contemporary publication is now beyond even a genius’s capacity.<a href="3-2-method.html#fnref3" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="3-1-genre.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="3-3-data.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": "https://github.com/brooksambrose/portfolio/blob/draft/03.Rmd",
"text": "Edit"
},
"download": ["ambrose_dissertation.pdf", "ambrose_dissertation.docx"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
