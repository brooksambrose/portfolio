# `r {load('_init.RData');kopi();tvoc}` {#voc}

*Brooks Ambrose and Lynne Zucker*

#### Abstract {-}

(ref:abs-voc)

#### Keywords {-}

(ref:key-voc)

***

```{r}
if(!'latex'%in%ls()) source('_init.R')
```

What were the dominant ideas in the social sciences at their formation as professions in the postbellum United States? What was the course of their development over a generation of scholarship?  In this study we answer these questions <!--inductively--> through a distant reading of the original journals of two disciplinary cases, anthropology and sociology. Though the goal is substantive, the methodological challenges of consuming a large quantity of text will feature prominently in the story that unfolds. Along the way, we will demonstrate the usefulness of the computational text analysis that is being explored in the humanities and how it can be combined with <!--traditional textual analysis--> quantitative series of economic and demographic data to address two theoretical concerns of organizational and institutional theory, as well as cultural sociology: what are the preconditions that allow cultural industries to emerge and become lasting institutions; and, what scope of cultural change can such institutions sustain? 

U.S. higher education has been simultaneously a field of tamed tradition and wild innovation. While institutional theory has classically focused on predicting the stability of norms [see especially @DiMaggio1983Iron], the more intriguing side of the theory focuses on the processes that cause change. These two sides of institutions are connected in surprising ways [see @Zucker2019Institutional]. Calcified institutional structures, where formal relationships are hierarchical and knowledge is heavily codified, are particularly susceptible to sudden redefinition, especially if the elements are interlinked so that one change can easily spread. Some institutions that experience such redefinitions are strengthened for it, while others may be hobbled and brought closer to their end.

Scholarly institutions have an unusually high degree of adaptation to their own sources of cultural instability. Academic disciplines have excelled at gaining ground both in the longue durée (secular trend) and in histoire événementielle (momentous event). Where periods of social upheaval disrupted scholarly business as usual, scholarly fields reorganized on new footings. However the adaptability of scholarship as a whole did not entail the endurance of all of its parts. Coequal to theories of paradigm shift describing the endogenous reasons for scholarly decline [@Kuhn1970Structure;@Lodahl1972Structure;@Rzhetsky2006Microparadigms], we apply the population ecology and resource dependency perspectives to elucidate the embedding of scholarship in wider resource networks [@Tolbert1985Institutional;@Tolbert1983Institutional;@Morphew2002Using] that are particularly vulnerable to exogenous shocks .

These classic concerns about the social and economic structure of organizations have typically been explored empirically by counting the financial resources available to organizations as well as indicators of their internal growth and differentiation. To these resource-level measures we develop new data series that draw directly on the cultural activity of scholars, joining social inputs to cultural outputs that have evaded accurate counting due to the complexity of cultural data sources, in our case scholarly texts. While there has been tremendous growth in quantitative approaches to counting and classifying text for the purposes of longitudinal analyses of scholarship [e.g. @Hall2008Studying;@Roose2018Trends], these have rarely been conducted with careful consideration of historical and institutional context [see for an exception @DiMaggio2013Exploiting].

We address the interplay between stability and change in two different ways. We begin by outlining key changes in the U.S. economy that we associate with concomitant developments in U.S. higher education. Such developments affected the quotidian activities of scholars by developing the resources and opportunities necessary for them to "make culture" in their organizational roles. Meanwhile, we periodize these micro-level trends by bracketing them between exogenous and often cataclysmic upheavals in U.S. society due to war and economic depression. Finally, we look at the actual "culture made" in an attempt to reveal the linkages between the day's work, or the publication of articles, and the year's news, or the social problems that both create a broader interest in social science and that disrupt the lives of social scientists as they did every person during periods of crisis.

## <!--B--> Social science history in context

Below we will attempt a distant reading of journals in the disciplines of sociology and anthropology. First, however, it will be helpful to put the development of these professions into some historical context. We take a coarse view of national history as the history of wars because of their downstream effects on government activity and institutional investments. The first period is between the end of the American Revolution (1783) and the end of the American Civil War (1865) and is the national context for the origin of U.S. anthropology. The second period is after the Civil War until the end of World War I (1918) and is the context for the origin of U.S. sociology and of modern U.S. higher education generally. Wars of territorial expansion are waged regularly during both periods against native peoples and rival colonial empires, and social research was recruited to solve attendant problems of population and to provide rationales for the relationships with and understandings of conquered or would-be conquered people.

In the interwar periods the leading structural changes tended to be economic. Where wars were ruptual moments that stalled development across many sectors of society, the attendant growth of military capacity also tended to lead to expansions of government, infrastructure, and even education via the training remaining with veterans. The tremendous strain on civilian life in terms of labor shortages during the Civil War thrust women into labor roles outside of the home, expanding the scope of their integration into the economy and public life even after returning to domestic roles as traditional family life was reconstituted. As the organizational form diffused, the old bases of trust in kinship networks were supplanted by mechanisms of communication and control oriented toward formal procedures rather than personal commitments [@Zucker1986Production]. This new kind of trust was based in the perceived fairness of rules, procedures, and contracts as well as in the new intermediary role of managers.

Drawing on economic historians and institutional theory Zucker [-@Zucker1983Organizations] has shown a secular trend in the late 1800s through the 1950s of a transition of American labor from self and often family-based employment to wage or salaried employment by a firm. This great transformation stirred a number of related elements that had been taken for granted. The population shifted from rural toward urban, primarily by leaving agriculture and taking up salaried employment. While before work was organized around individuals, it now became organized around corporate entities [-@Zucker1983Organizations\:Figure 3]. This shift started during WWI, with wives going to work for munitions and aircraft factories in the cities. After the war ended, politicians expected a reverse flow back to the rural areas and waited 20 years to count this new urban population to determine representation in the U.S. House of Representatives. This reverse flow never happened. 



(ref:zuck1) Shift from self-employment to wage-labor in United States. Source: Table 4, Zucker [-@Zucker1983Organizations\:15]. Illiteracy Source: Table 6, Snyder [-@Snyder1993120\:21]

```{r zuck1,include=T,fig.cap='(ref:zuck1)'}
war<-rbindlist(list(
      data.table(text='Long Depr.',xmin=1873,xmax=1879,dodge=.94,col='blue')
      ,data.table(text='Sample',xmin=1888,xmax=1922,dodge=.5,col='darkgray')
      ,data.table(text='Great Depr.',xmin=1929,xmax=1939,dodge=.94,col='blue')
      ,data.table(text='Civil War',xmin=1861,xmax=1865,dodge=.99,col='red')
      ,data.table(text='WWI',xmin=1914,xmax=1918,dodge=.99,col='red')
      ,data.table(text='WWII',xmin=1939,xmax=1945,dodge=.99,col='red')
    ))[,x:=mean(c(xmin,xmax)),by=text][,N:=text][,year:='']
warp<-function(gg,r,ann=war) gg+geom_rect(data=ann,aes(xmin=xmin,xmax=xmax,ymin=r[1],ymax=r[2]),fill=ann$col,alpha=.1,inherit.aes = F) +
    geom_text(data=ann,aes(x=x,y=ann$dodge*r[2],label=text),color=ann$col,inherit.aes = F)
emp<-data.table(
           Date=c(1800,1860,1910,1940,1950,1960,1970)
           ,'Self-Employed'=c(57,37,22,26.1,21.2,15.7,10.2)
           ,'Wage & Salary Workers'=c(12,40,78,71,77.2,83.8,89)
           ) %>% melt(id.vars='Date',variable.name=' ',value.name='Percent Labor Market')
ill<-fread('d/q/snyder1993illiteracy.txt',fill = T) %>% {setnames(.,make.unique(names(.)))} %>% .[!.N]
rbindlist(list(emp,ill[,.(Date=Year,` `='Illiteracy','Percent Labor Market'=Total)])) %>%  {plt(warp(myth(ggplot(data=.
         ,aes(x=Date,y=`Percent Labor Market`,color=` `))) + geom_line() + geom_point() + theme_bw() 
      ,r=emp[,range(`Percent Labor Market`) %>% replace(1,0)])+tl+ scale_x_continuous(breaks=seq(1800,1970,20)))}
```

```{r per-cond}
acs<-fread('d/q/booksmapsmagazinesnewspaperssheetmusic.txt',fill = T,sep='\t') %>% melt(id.vars='Year',variable.name='Product',value.name='Millions of dollars')
```

The development of procedural trust to organize production also heightened the penalty to illiteracy; whereas orientations to personal loyalty and family obligations were based in tradition and enduring social ties, procedural trust required learning new relationships quickly in novel on-the-job contexts. Contracts, manuals, bookkeeping, and even posters and signage on factory floors presumed what is now referred to as functional literacy but was then still an emerging feature of occupations. More quickly than in any other nation the literacy rate in the United States rose due to public investments in education, fueled by and fueling a growing demand for literate labor. Though data for earlier periods are unavailable, in aggregate personal consumer spending on private education grew from `r {w<-acs[.(c(1909,1929)),on='Year']['G492 Education',on='Product',.SD,.SDcols='Millions of dollars'] %>% unlist;w[1] %>% nn}` million dollars in 1909 to `r w[2] %>% nn` million dollars in 1929, staying strong during WWI only to enter a decline during the Great Depression. Consumer literate spending would recover during WWII giving seeming unlimited opportunity for cultural industries like publishing and education to turn the public's ability to read into consumer demand and an engine for their own growth.

(ref:per-con) Personal Consumption Current Dollar Expenditures on Literate Products ("G456 Books, maps, magazines, newspapers, sheet music", "G457 Private education and research", and "G492 Education (private)"). Source: US Census Bureau [-@Bureau1975Historical\:316-319]

```{r per-con,include=T,fig.cap='(ref:per-con)'}
acs %>%   {myth(warp(ggplot(.,aes(x=Year,y=`Millions of dollars`,color=Product)) + geom_line(),ann=war[ec('Sample,Great Depr.,WWI,WWII'),on='text'],r=range(.$`Millions of dollars`,na.rm = T)))+theme(
      legend.position = c(.7, .98),
      legend.justification = c("left", "top"),
      legend.box.just = "left"
  ) + tl + scale_y_continuous(trans='identity') + scale_x_continuous(breaks=seq(1870,2010,10))} %>% plt
```

Meanwhile on the supply side U.S. higher education was growing steadily. After the Civil War two federal laws called the Morrill Acts of 1862 and 1890 allowed states to sell land and raise funds for the building of colleges and universities. As institutions of higher education were founded, scholars who may have been located in other organizations like museums and learned societies were able to join much larger communities of peers. This was a precondition for graduate training, and it was primarily through graduate programs that the disciplines constituted themselves as professions. Graduate students and new Ph.D.s fueled the growth of the internal audience necessary for the establishment of new journals. Figure \@ref(fig:t196) makes apparent the different phases of investment in founding universities. From 1870 about ten new institutions were founded every year until net change stopped at about 1,000 in 1890. For the next 30 years the number of institutions stayed very stable, even declining slightly. At no other time in American history did an initial organizational investment remain so constant.

(ref:t196) Decennial change in the number of U.S. colleges and universities. Source: [@NCES2010Table]

```{r t196,include=T,fig.cap='(ref:t196)'}
f<-'d/b/t196.RData'
if(file.exists(f)){
  load(f)
} else {
  t196<-fread('d/d/tabn196.txt',fill=T,header = T)
  t196 %<>% {.[,.SD,.SDcols=grep('^[^V]',names(.))]} %>% .[2:.N]
  t196[,`Selected characteristic`:=`Selected characteristic` %>% gsub('[^A-Za-z]+',' ',.) %>% sub(' s','s',.) %>% sub(' $','',.)]
  t196[,names(t196)[-1]:=lapply(.SD, function(x) gsub('[^0-9]+','',x) %>% as.integer),.SDcols=names(t196)[-1]]
  t196<-t196[grep('total',`Selected characteristic`,ignore.case = T)] %>% melt(id.vars='Selected characteristic',variable.name='d',value.name='N')
  t196[,yr:=d %>% sub('-.*','',.) %>% as.integer]
  save(t196,file=f)
}
rm(f)
t196 %>% setnames('Selected characteristic','sc')
t196[.(
  #ec('Total institutions,Total faculty,Total fall enrollment')
  'Total institutions'),on='sc'] %>%
{warp(myth(ggplot(data=.,aes(x=yr,y=N,color=sc))) ,r=range(.$N,na.rm = T)) + geom_line() + geom_point(show.legend = F) + lr +theme(legend.title = element_blank()) + xlab('Date') + scale_x_continuous(breaks=seq(1860,2010,10))} %>% plt
```


Figure \@ref(fig:nces2phd) shows the aggregate number of doctorate degrees awarded by any university. While in absolute terms the Ph.D. output in the period before 1960 appears minimal compared to the explosive growth during the 1960s and 1970s, for its time it actually grew faster in percentage terms. By observing the logarithm of the count in the bottom panel it is clear that the system tended to grow by a constant multiple decade to decade. From 1870 to 1900 growth was as rapid as it ever was, an indication that the genesis of the university system was a special time of rapid expansion as a surplus of facilities was justified by the recruitment of personnel as was the intention behind the Morrill Acts. Once on this firm footing growth remained at very comparable rates until the present, save for two exogenous upward shifts of the curve. The second shift, as we have already mentioned, occurs between 1960 and 1970 when the baby boom generation begins to fill the ranks of academia. But a lesser known but equally profound earlier shift occurred for the WWI generation, after the Great War and before the beginning of the Great Depression. This interwar period was one of national investment in higher education that served the United States well for responding to the challenges posed by the Great Depression and eventually by WWII.

(ref:nces2phd) Decennial growth in the number of Ph.D. degrees conferred in the U.S. Source: [@NCES2010Table]


```{r nces2phd,fig.asp=1,include=T,fig.cap='(ref:nces2phd)'}
f<-'d/q/nces.RData'
if(file.exists(f)){
  load(f)
} else {
  nces<-nces2phd.f()
  save(nces,file=f)
}
rm(f)
egg::ggarrange(
  warp(nces$fig,r=nces$tab$N %>% range,ann=war[ec('Sample,WWI,Great Depr.,WWII'),on='text']) + scale_x_continuous(breaks=seq(1870,2010,10))
  ,warp(nces$fig+scale_y_continuous(trans='log10') + ylab('log10(N)'),r=nces$tab$N %>% range,ann=war[ec('Sample,WWI,Great Depr.,WWII'),on='text'][,text:='']) + scale_x_continuous(breaks=seq(1870,2010,10))
) 
```

The view from within academia can be described as a field of published research housed largely within the ecology of journals. Consistent with the rapid growth in Ph.D.s before the turn of the century, the number of new journals appearing every year more than doubled between 1880 and 1890, as evidenced by the journals included in the JSTOR archive (see Figure \@ref(fig:jbdp)). This rapid period of growth was fueled by the wave of college and university foundings in the wake of the Civil War. The tempo of this surge, which we will call the sowing period, abated in the period starting in 1895 for unclear reasons. The most likely scenario is that the demand for publishing space created by the first generation of American Ph.D.s was met. Economically, the so called Panic of 1896 was an economic recession that may have chilled the financial prospects for journals though Americans had already been experiencing similar events almost every other year since the end of the American Civil War. The Spanish American War of 1898, while a major political event of the time, was not so large as to impact the daily lives of most Americans, though more than 100,000 men volunteered for military service in the week after the detonation of the USS Maine in Havana Harbor. The ability for an event to be disruptive is a function of foresight rather than hindsight, and it is possible that propaganda and yellow journalism exaggerating the threat caused panic enough to delay investments for a time. 

```{r jbd}
f<-'d/b/jbd.RData'
if(file.exists(f)){
  load(f)
} else {
  source('lyn.R')
  jbd<-environment(pli$x$visdat[[1]])$data[,year:=cd %>% as.character %>% as.numeric]
  save(jbd,file=f)
}
rm(f)
```

```{r pdi,fig.asp=.25}
library(MASS)
load('d/b/pdi.RData')
pdi<-pdi['birth',on='s'][between(d,1883+2,1982-12)]
grwm<-pdi[,MASS::rlm(log10(N)~d)] 
grw<-grwm %>% coef %>% exp %>% .['d'] %>% {.-1} %>% `*`(100) %>% round(3)
pdi[,ggplot(aes(x=d,y=N),data=.SD)+geom_point()+geom_smooth(method='rlm',formula=y~x) + scale_y_continuous(trans='log10')]
prd<-prediction::prediction(grwm,pdi) %>% data.table
prd[,N:=10^fitted]
bin<-5
prd[,cd:=range(d) %>% round(-1) %>% {do.call(seq,list(from=.[1],to=.[2],by=bin))} %>% {cut(d, breaks = ., labels = .[-length(.)], include.lowest = T,right=F)}]
prd<-prd[,.(N=sum(N)),by=.(s,cd)]
prd<-prd[,.(s,d=cd %>% as.character %>% as.integer,N)]#[!c(1,.N)]
```

```{r jbdp,include=T,fig.cap='Birth and death rates for JSTOR journals, 5 year intervals. Dotted line is linear prediction of logarithm of N from 1885 to 1970.'}
{myth(warp(
  ggplot(data=data.table(jbd),aes(x=year,y=N,color=s)) + geom_step(data=prd,aes(x=d,y=N),inherit.aes = F,linetype='dotted',color='black') + geom_step() +  scale_y_continuous(trans='identity') + theme_bw()
    ,r = jbd$N %>% range)) + 
    #scale_y_continuous(trans='log10') +
   scale_x_continuous(breaks=seq(1780,2000,20))+tl+theme(legend.title = element_blank()) + xlab('Date')} %>% plt
```

Here as well the more granular data make apparent the effects of the World Wars. Though births stalled only moderately during WWI (1914-1918) an unprecedented number of journals came to an end. The end of the Great War also coincides with the Spanish Flu that was even more devastating in terms of loss of life. In the interwar period the trend in births recovers rapidly, until a stall at the beginning of the Great Depression followed again by a recovery. While depressions put financial strain on investment capital that also affects journals, the attendant rise in social problems also raises demand for creative solutions from scholarship. Thus during both the Long (1873-1879) and Great (1929-1939) depressions the rate of new journals actually accelerated by the end of the downturn. Some of this effect may have been due to journal starts being postponed rather than outright canceled. The consequences of WWII (1939-1945) are visible not in a die off of existing journals but in a precipitous fall in new journals. WWII was especially famous for draft and volunteer enlistment that cut across social classes. Thus scholars, who have always come disproportionately from the wealthy class, served as soldiers alongside poor and middle class Americans. The direct service of scholars, however, was not the only cause of disruption; in a profession where concentration counts, attention itself was siphoned off to following the war news.

Immediately after WWII the journal field returned rapidly to business as usual, recouping all of its losses and returning to an exponential growth trajectory. Indeed if one inspects the logarithm of the count of births (not shown) the trend is very linear with no exogenous shifts as were visible in the Ph.D. series. In each of 85 years between 1885 and 1970 the rate of journal births grew by `r grw` percentage points on average, as illustrated by the dotted line in Figure \@ref(fig:jbdp).^[Calculated by a robust linear regression, which penalizes the effect of the outliers, of the logarithm of the annual count. Regression table available upon request.] The only substantial exceptions to the exponential growth occurred in the crest before the turn of the century and in the trough caused by WWII. 

### The social sciences

We will begin to hone in on the cases of anthropology and sociology by observing the social sciences in particular as represented by the JSTOR archive. Figure \@ref(fig:jstorm2fig) shows the net growth of social science journals over a 200 year period. The curve is broken into periods of roughly similar slope. To choose the breakpoints we apply change point analysis, which detects significant differences in time series data [@Matteson2013Nonparametric; @James2019ecp], to the first difference of the curve. We find turning points in the expected locations. 1885 marks the start of the sowing period and the end of the long prehistory of modern scholarship, while 1919 and 1944 mark the post war recoveries. Repeating the analysis on the second difference of the trend reveals that 1939, the end of the Great Depression and the beginning of WWII, marks a singular turning point where the acceleration of journal growth shifted into a higher gear.

```{r jstorm2fig,include=T,fig.cap='Periods in the Growth of the Number of Social Science Journals in the JSTOR Archive.'}
f<-'d/q/jfig.RData'
if(file.exists(f)){
  load(f)
} else {
  jfig<-jstorm2fig.f(
    jstorm,jclu,series='Social Sciences',
    ann=war
  )
  save(jfig,file=f)
}
rm(f)
plt(jfig$p + scale_x_continuous(breaks=seq(1800,2000,20),minor_breaks = seq(1810,2010,20)),tooltip=ec('text'))
```

<!--
```{r jstorm2tab,eval=F,include=F}
if(!'jstorm'%in%ls()) load('d/q/jstorm.RData')
jtab<-jstorm2tab.f(jstorm,beg.bef = 1900,end.aft = 1900)
sg(jtab[,.(
  `Title History`=publication_title
  ,Discipline=discipline
  ,Start=start %>% as.character
  ,Stop=stop %>% as.character
)],tit='20th Century Social Science Journals in JSTOR',new.col.align = 'p{0.4\\\\linewidth}p{0.3\\\\linewidth}rr'
#,rplc = ec('\\{tabular\\},\\{longtable\\}')
)
```
-->

The ratio of new Ph.D.s to extant journals gives a good indication of the range of opportunities or the degree of competition facing individual scholars. While not strictly comparable, the aggregate number of Ph.D.s conferred divided by the number of social science journals gives a glimpse of the evolving opportunity structure in the social sciences. Figure \@ref(fig:nces/jstorm) shows that this ratio remained very constant for the 50 years between 1870 and 1920 indicating that the field of professional opportunities grew about as quickly as the number of newcomers playing on it. This began to change as the upward shifts in Ph.D. output outpaced the growth of new journals. While the increase in the ratio may have been partially offset by technical improvements allowing journals to fit more scholars into the same issues, there is no mistaking the tectonic shift between 1960 and 1970 where the number of new Ph.D.s competing over each journal quadrupled from 42 to 166.

```{r nces/jstorm,include=T,fig.cap='Number of PhDs conferred in the United States per Social Science Journal'}
rat<-jfig$d[between(year,1800,2000)&super=='Social Sciences'] %>% setkey(year)
setkey(nces$int,year)
rat<-merge(rat,nces$int)
rat<-rat[,.(year,ratio=N.y/N.x,g1)]
r<-rat[,range(ratio)]
n<-diff(r)*.1

plt(warp(myth(
  ggplot(data=rat,mapping = aes(x=year,y=ratio)) +
    ggplot2::annotate('rect',xmin=1888,xmax=1922,ymin=-Inf,ymax=Inf,alpha=.1) +
    geom_line(aes(color=g1),size=1.5) +
    geom_point(data=rat[.(nces$tab$year)] %>% na.omit,mapping = aes(x=year,y=ratio),shape=21,size=1.5,stroke=1,fill='white')
) + theme(legend.position="none",axis.title.x = element_blank())
,r = rat$ratio %>% range))
wrk<-rat[between(year,1888,1922),summary(ratio)]
```


We take the relatively flat trend visible in the sowing period to represent one of stable growth and what Swidler [-@Swidler1986Culture] has called settled lives. Between 1888 and 1922 there tended to be about `r nn(wrk['Median'],0)` new Ph.D.'s in the U.S. for every social science journal even as each population grew year over year. These growth patterns begin to diverge around `r jfig$d[super=='Social Sciences'][g1==3][1,year]` as a decades long acceleration of personnel begins, relatively slowly between 1920 and 1960 at an average acceleration rate of `r rat[between(year,1920,1960,F),nn(ratio %>% mean)]` PhDs per journal per year, and then quite precipitously in the 1960s at an average acceleration rate of `r rat[between(year,1960,1980,T),nn(ratio %>% mean)]`. This makes the period between the sowing years and the onset of WWI a good choice for studying the development of scholarly publication at a time where internal trends take precedence over external shocks, and to observe how these tendencies are ultimately affected by an historical crisis. 


<!--TODO: I use intellectual histories of anthropology to characterize the antebellum period, and the same for the postbellum period including sociology.-->

```{r jclu-sub}
if(!'jstorm'%in%ls()) load('d/q/jstorm.RData')
jclu_sub<-jstorm[sapply(discipline,function(x) 'Social Sciences'%in%x)][,discipline:=lapply(discipline,setdiff,'Social Sciences')] %>% jstorm2jclu.f
setnames(jclu_sub$tab,'super','Subdiscipline')

if(!'jstorm'%in%ls()) load('d/q/jstorm.RData')
jclu_sub_econ<-jstorm[sapply(discipline,function(x) 'Business & Economics'%in%x)][,discipline:=lapply(discipline,setdiff,'Business & Economics')] %>% jstorm2jclu.f
jsube<-jclu_sub_econ$tab
setnames(jsube,'super','Subdiscipline')
```


```{r pd}
load('d/p/pd.RData')
pd[,y:=lubridate::year(d)]
pts<-merge(pd['birth',on='s'][,.(title_history,y)],pd['death',on='s',.(title_history,y)],by='title_history') %>% setnames(ec('title_history,b,d')) %>% .[is.na(d),d:=2010]
pts<-pts[rbindlist(list(jclu_sub$super,jclu_sub_econ$super))[grep('^Labor',super),super:='Labor & Employment Relations'],on='title_history',s:=super][!is.na(s)]
pts<-pts[,.(y=b:d,s),by=.(title_history)][,.N,by=.(y,s)]
d2<-function(d='Economics') pts[.(d,2),on=ec('s,N'),{x<-min(y);names(x)<-d[1];x}]
```

Anthropology is a rare example of a discipline that has a long prehistory prior to the U.S. university system, whereas sociology's birth very much coincided with the university as a new organizational context of the kind described by Zucker [-@Zucker1983Organizations]. Tables \@ref(tab:jclu-tab-sub) and \@ref(tab:jclu-tab-sub-econ) show a classification of social science journals, given by the JSTOR archive, and their ranking in terms of size as it stands today. These disciplinary labels are more valid today than than they were in the 1880s at the start of the sowing period for scholarly journals, as it was not always obvious that these took priority over other fields whose names have receded from the scholarly imagination. The differentiation of business and economics away from the other disciplines had yet to occur.

As can be seen in Figure \@ref(fig:subdisc), which plots the net journal counts over time, archaeology is the oldest discipline and has always had more journals. Of the more theoretical social sciences that appeared after 1880 the oldest to receive a second journal in the field, a marker of growth, was economics (`r d2('Economics')`), followed in order of emergence by education (`r d2('Education')`), anthropology (`r d2('Anthropology')`), geography (`r d2('Geography')`), political science (`r d2('Political Science')`), and sociology (`r d2('Sociology')`). After WWI two clear trajectories emerge. On one hand, the bundle of larger disciplines grows steadily before receiving the baby boomer generation that super charged their growth after 1960. On the other hand, a handful of fields, among them the most recent with the exception of the older anthropology, continue on the older, slower acceleration curve. Two fields, sociology and business, cross from the slower to faster track at later points.

```{r}
if(!'jclu'%in%ls()) load('d/q/jclu.RData')
```

```{r jclu-tab-sub,include=T}
sg(jclu_sub$tab[,!ec('Labeled,LPct')],tit = "JSTOR Social Sciences Journal Counts")
```

```{r jclu-tab-sub-econ,eval=T,include=T}
sg(jsube[,!ec('Labeled,LPct')],tit = "JSTOR Business & Economics Journal Counts")
```

```{r subdisc,include=T,fig.cap='JSTOR journal counts by subdisciplines in Social Sciences and Business and Economics'}
p<-pts[between(y,1851,2000)] %>% {plt(warp(myth(
  pal = 'Paired',
  ggplot(.,aes(x=y,y=N,color=s)) + 
    #geom_vline(xintercept=1950,color='lightgray') + geom_text(x=1950,y=.89*max(.$N),label='NSF',color='darkgray') +
    geom_line() 
)+ theme(legend.title=element_blank())+ml
,r=.$N %>% range) + scale_x_continuous(breaks=seq(1860,2000,20)) #+ scale_y_continuous(trans='log10')
,hovinf = 'text')}
if((latex|docx)) p else {p %>% 
    plotly::style(visible="legendonly", traces = setdiff(1:11,c(1,4,8,10))) %>% 
    plotly::style(hoverinfo='skip',traces=12:17)
}
```

Anthropology is thus revealed to be a discipline whose fate was very different than the rest. For most fields an early head start gave an advantage in size that was seldom surpassed by newcomers. Stranger still is that anthropology did not receive the same boost that others did during the Great Depression, where it was unique in experiencing a net decline, and after WWII where most disciplines succeeded in siphoning off at least some Cold War investments in science and technology. It seems that anthropology did not court the new National Science Foundation and its equivalents as vigorously as did sociology [@Abbott2007Hot].

Anthropology's turning away from government investment was a reversal of its historical relationship with power. U.S. nation building after the American Revolution had enrolled researchers in the projects of westward expansion against native peoples, the consolidation of slave economies against Africans, and the legitimation of the American experiment against European detractors. These were pressing problems to the intellectuals among government leaders at different levels, and they worked to make investments in new knowledge to resolve them. Such new knowledge was initially an extension of older "theories of man" in theology and enlightenment natural philosophy. These had a foothold in the private education of the American so-called natural aristocracy as well as in urban colonial institutions like the American Philosophical Society, which served as meeting places for intellectual elites and scholars. After the British burned the Library of Congress in 1814 Thomas Jefferson famously sold his personal library to Congress to restore it, an illustration that secular arts and sciences were produced and maintained by and under the patronage of private elites.

In the antebellum United States anthropology, the study of man, was synonymous with what was also called ethnology, the study of the races of man. Analogously to how interwoven economics became with the federal government during the Great Depression, prior to the Civil War ethnology was bankrolled by elites who were themselves implicated in the projects of nation building, which at this time meant westward expansion and the domination of indigenous peoples. Jefferson paid prizes to amateur ethnologists to collect lists of words from the languages of the eastern tribes in an attempt to recognize their potential to be civilized, taking what was then the radical position that native people could be eventually integrated into American society. Inspired by Jefferson's thinking, for a brief time in the 1840s the Virginia legislature used tax incentives and educational programs to promote intermarriage between male settlers and native women with the goal of their assimilation [@Patterson2001Social\:9]. This policy experiment, avowedly racist by today's standards, was considered by colleagues of Jefferson to be a civilized alternative to war with native people, and it illustrates a kind of intellectual product that was in demand by the polity.

The professionalization of scholarship attending the origins of the university system put an end to these patronage relationships while at the same time government attention shifted from problems of pacifying native peoples to those of managing an industrializing and globalizing society. Figure \@ref(fig:ttseas) shows that prior to the sowing period anthropology and ethnology, as terms appearing in published books, were joined in an almost identical trend, but that at the onset of the university system the trends diverged favoring anthropology. Meanwhile, sociology rose from relative obscurity to overtake anthropology as if riding the wave of university founding. It is possible that in the new organizational environment of the university the younger generations were empowered to sweep away some of the work of earlier generations. Sociology, wedded more forcefully to a scientific epistemology, carried the aura of a modern discipline oriented to the problems of a modern society, while anthropology was treated as mired in the problems of a dwindling age. Anthropology may very well have attempted to redefine itself precisely by shedding the terminology of ethnology in order to improve its footing on the new institutional terrain.

(ref:ttseas) Anthropology and ethnology diverge during the sowing time, while each is overtaken by sociology. Source: [@2012Google]

```{r ttseas,include=T,fig.cap='(ref:ttseas)'}
f<-'d/b/ttseas.RData'
if(file.exists(f)){
  load(f)
} else {
  ttseas<-gbng2tts.f(query = ec('ethnology,anthropology,sociology'),out = 'd/b',ys = 1700,ye = 2000,cfso = T)
  save(ttseas,file=f)
}
rm(f)
plt(ttseas[between(Year,1850,1950)] %>% {myth(warp(ann=copy(war)[,text:=''],r=.[,range(Count)],ggplot(data=.,aes(x=Year,y=Count,color=Phrase))))} + geom_point(size=.5) + scale_y_continuous(trans='log10') + geom_smooth(method='loess',se = T,size=.5)+ scale_x_continuous(breaks=seq(1850,1950,10))+lr+ylab('log10(Count)'),output='latex')
```

The most important journals in anthropology and sociology date from the postbellum period, and the appearance of each is implicated in the project of professionalization for each discipline. The 1920s marked the end of war with the last of the militating American Indian tribes, and a reckoning with the darkest sides of industrialization laid bare by WWI. Social research had by this time completed a shift from colonial to industrial problems and enjoyed a golden decade of development as a profession, punctuated by the next great historical crisis in the Great Depression. With the 1920s begins the adolescence of social research, which is beyond the present scope. In the next section we will study the childhood of a new profession of social science, which ends with the Great War. We however draw the study out until 1922 to observe some of the transition into the next phase of development. As we will discuss presently, the journal data that form the basis of the cultural analysis of scholarship are endogenous to the sowing period. The journals literally did not exist, at least for very long, before the profession established itself in universities. 1922 also happens to be the end of the public domain in U.S. copyright, which will aid in the reproducibility of the analysis and will allow all readers to recover the texts in question without difficulty.

## <!--M1--> A census of words

<!--Topics `r if(latex) '\\normalfont{≟}' else '≟'` Ideas-->

> "To master the whole meaning of the discovered truths and to understand all that is summarised in them, one must have looked closely at scientific life whilst it is still in a free state, that is, before it has been crystallised in the form of definite propositions" [@Durkheim1893division\:299].

Given a relatively stable epoch in American society, one free of the most extreme exogenous shocks of war and economic depression, how is it possible to trace the development of the discursive structures of American social science scholarship? We will use the digitized texts of articles taken from the earliest journal in anthropology and in sociology in the JSTOR archive to represent a time series of discourse. The methodological challenge is how to count such an empirical source as text, which is seldom considered to be data at all. It is now unavoidable to delve into methodological considerations that will distract momentarily from the historical portrait painted above. Once we have results we will be able to rejoin the historical discussion with new evidence of scholarly development.

Texts are after all for reading rather than counting. Above we have shown how counting is possible within certain categories that are given historically, namely the journal sources of texts and the institutional resources that were their context. Theoretically, however, there is a concern that discursive formations appear as a conversation among texts that may not be visible in the exterior labels given by discipline boundaries, or at least that such social or institutional labels are lagged behind their cultural origins, if indeed they ever break through from the cultural to the social. A genre category is the example par excellence of a socialized culture, one whose relevance to a society at large, however narrowly society may be conceived, is at least debatable because it is visible in the language as a category. The conjecture here is that direct analysis of texts may reveal the cultural currents, as Durkheim referred to them, before the crystallize into firmer social formations.

To do that kind of counting at scale does require a humble approach to text. Text is the bearer of meaning when read by humans. When read by machines text are impoverished. Rather than claim to have access to the meanings of texts, we instead make the more conservative claim to have access to the vocabularies with which those meanings are communicated. Thus the strategy of the study occurs in four steps.

1. Sort text into categories of similar vocabulary.
2. Describe the vocabularies that define category membership.
3. Describe vocabulary prevalence across time and discipline.
4. Validate category contents by a traditional qualitative reading of texts.

We will spend considerable effort on solving the problem presented by step 1, as here everything depends on the computational methods employed. Steps 2 and 3 are straightforward given a successful mathematical model of texts. Step 4 is seldom attempted, and may be the hardest of all, because it is here that machine and human learning must be integrated. If through these steps we may operationalize the notion of disciplines and fields of scholarship as conformity to vocabularies, then we believe a new horizon of intellectual history is possible. If on the other hand we find that machine-learned vocabularies do not correspond to human-learned understandings of the texts drawing on those vocabularies, then the discovery will be negative, that distant reading is not a scientific, historical, or hermeneutic method.

The statistical tool we will rely on in step 1 is called topic modeling, which refers to a variety of computational approaches to text data that blur the distinction between qualitative and quantitative analysis. The topic model paints a lexicographic picture of texts, analogous to the demographic picture gained by a census survey of cities and towns. To a topic model, texts are merely collections of terms (usually words) that are counted to create the so-called "bag of words" description of a text. In the same way that a census reduces communities to counts of the names of people who live in them, topic modeling reduces texts to the frequency of word choices in texts, to their diction or vocabulary. Just as a census of people fails to capture the nuanced interactivity of human settlements found in their culture, politics, and economic activity, the topic model washes away the meanings and intentions behind the words that are enumerated.

A population census would not be very helpful were it only a count of the names of respondents, and of course the really helpful data derive from the demographic and economic survey attached to the name. Text data do not usually come with such a collection of rich covariates, nevertheless topic models promise to discern helpful patterns from counts alone. The trick behind the estimation of a topic model is that it attempts to learn the demographic information (topics) without asking, by merely looking at how the names alone (terms) are distributed across geographies of interest (texts). If it can keep its promise, a topic model applied to census data might recover the cultural patterns latent in the distribution of names. It might, for instance, learn different groupings of names that in turn correspond to markers like age, race, national origin, or gender, so long as membership in those categories was related to geography. It might, for instance, successfully separate a category of Hmong names out from among the names of all people living in St. Paul because the non-Hmong names appeared in other regions where no Hmong names appeared.

To call the category of names "Hmong" requires an interpretation of the model, which by itself is just lists of names. This is the work of step 2, and requires a little bit of shoe leather by trying to make sense of what a list of names refers to.  Here reading texts is like a census taker knocking on a door, and a topic model's latent analysis saves on this effort. Sometimes bringing domain knowledge to bear on the list itself will suggest a category label, but often choosing a small sample of texts as exemplars of the category. Still this requires much less shoe leather than a traditional qualitative analysis in which each text is studied directly. Of course the census is much more informative because it asks about demographic categories directly thereby avoiding the need for a latent analysis. In domains where rich covariates are not yet available or are prohibitively expensive to acquire, latent analysis provides promising clues of patterns that already exist. What is even more interesting, and something that might surprise even census analysts, is when latent categories do not correspond to known survey items. In either event the power of topic modeling for inductive analysis is to reveal structure in how names hang together that was hidden.

Even without conducting the second labeling step, in step 3 it will already be possible from the output of the model to inspect the distribution of topics across available covariates, especially time. These are the patterns that will help validate the topic models against what is already known about intellectual history. For instance, the power of institutional and generational change may well be apparent in the historical distribution of topics. This step leads naturally into step 4 by suggesting anomalies that can only be explained by a closer look at the texts, the chore that the entire preceding analysis punts on. In step 4 we learn either that our understanding of history was wrong, or that our topic model was wrong, and there may be no method other than one's judgement to decide.

## Digital full-text archives

In order to develop time series from a census of words, we rely on computational text analysis (CTA) or "distant reading" as it it known in the humanities. It will be helpful to consider the epistemological opportunities and challenges presented by the distant reading approach. While controversial in humanistic circles that emphasize the primacy of the reader’s novel interpretive work when consuming text, distant reading fits comfortably within a social science epistemology that aims to achieve an objective description of intellectual history. Indeed, computational methods offer a useful backstop to the subjectivity of a particular person’s reading of history. Topic modeling, one method of CTA, promises to automate a particular slice of what hermeneutic methods accomplish. Hermeneutics claims that through historical methods it is possible to reconstruct the interpretive context of texts such that they can be understood in the same way that contemporary historical actors understood them. Establishing such context is a laudable yet arduous feat of historical research to uncover the social and intellectual milieu of a particular text. This is the gold standard approach, but one that restricts the field to specialists with the training and resources necessary for the undertaking.

Computers cannot study history in this way. What they can do, however, is mine source material for limited kinds of contexts. The kind we are concerned with below are the historical vocabularies that writers used to construct texts in the past. Vocabularies are glyphs without grammar; they do not mean anything, but nothing meaningful can be said without them in the present or in the past. They are the mediated form of language, and in communicating with each other historical actors leave traces that survive perfectly in time so long as texts themselves survive. 

While computers cannot read meaning in texts, and can barely recognize it, they are almost as good as humans at recognizing the glyphs of texts, and vocabularies are nothing but glyphs. What computers lack in smarts, they make up in speed and memory. The quantitative scale of their recognition makes for a qualitative shift because vocabularies can be enumerated across immense corpora of texts. Immense, at least, by human standards as there are limits to even computer memory and speed. Yet such enumeration of texts into objective historical categories is a profound resource for the intellectual historian. That one could begin a reading with such context would be a transformative research tool. Vocabulary enumeration, by which we mean simply the counting and classifying of texts according to the vocabularies they contain, invites a population studies approach to intellectual history. Where sense-making is driven by comparisons, a reader’s arbitrary combination of texts is guaranteed to lead to anachronism. But if we can know that texts are relevant to each other without knowing why, we have done some small amount of hermeneutic work by supplying texts as historically correct context to each other.

And even going so far as abandoning the project of reading texts in a historically correct way, vocabulary enumeration can still lend objectivity to a novel construction, a productive anachronism, of textual meaning. Because vocabularies, the problems solved by computers, are mathematically, algorithmically, or stochastically determined, they may provide an immutable description of corpora that, like a map, enables individual and collective exploration within a common framework. Such maps may become the parameters of interpretive methods, which we may use to surface and control some of our subjectivity.

### Quality of evidence 

Computational text analysis requires that text corpora be transformed from a human to a machine readable format. Several efforts to digitize paper archives have made historical research designs possible, notably the Google Books project, HathiTrust, and the ITHAKA JSTOR archive. Digital storage devices like the portable document format (PDF) have also enabled texts to be represented in both a digital version and as a reasonable facsimile of paper originals. Reasonable, we should say, for most sociological purposes, but not for other historical questions where materiality of culture is important [@Schreibman2014NonConsumptive\:149].

Digital archives make research into the production of culture difficult, precisely because they misrepresent several aspects of the means of production. Researchers should be mindful that digitization of texts abstracts some qualities of texts and renders many others invisible. The importance of physical space and material qualities of libraries is illegible when working with digital archives, while the verbal content of texts is highlighted. We must keep in mind that we are not viewing what historical actors saw. Digital texts are almost perfectly fungible, while, variability in the format and material condition of historical texts may have been common. We are liable, for instance, to underestimate the search costs to locate texts, and the fungibility of texts themselves.

Simply put, if the texts we analyze are not the ones that historical actors read, can we be said to really be doing something historical? There are reasons to believe that digital text archives provide not just a useful but an historically valid abstraction from the material texts. If we want to understand how an individual scholar understood a particular text, better to have her personal copy, margin notes and all. Yet how would that scholar have treated the text as a cultural item? She would abstract her own copy to a format credibly held in common, the more antiseptically clean version that we see in digital archives. These are the ghosts of the texts, so to speak, but they are what would be left when all idiosyncrasies were removed, the version that one would assume colleagues thought of when declaring that text publicly.

<!-- This comment on evidentiary foundations introduces the theme of this paper, which is the difference between personal and social meanings of texts, and more generally how tendencies toward idiosyncracy attenuate those toward conformity, and vice versa. -->

This is by way of saying that the texts we compile below are not the same that were read by the historical actors under consideration. They are the texts that historical actors would assume their contemporaries were reading, that is, the sanitized, fungible, original published form of the text. By getting at these texts, we are getting at the real historical infrastructure for scholarly communication.


```{r zot2bib}
f<-'d/q/zot2bib.RData'
if(file.exists(f)){
  load(f)
} else{
  zot2bib<-zot2bib.f(
    users='2730456'
    ,collections = fread('d/q/zotcollections.txt')$URL
    ,key = 'qMxyytAhp07W9jNOGssIQasg'
  )
  save(zot2bib,file=f)
}
rm(f)
d<-lapply(zot2bib,function(x) x[,.(publicationTitle,date,volume,issue,pages,bp,ep,title,creators,dateModified,url)]) %>% rbindlist %>% `[`(!duplicated(.[,!9]))
setorder(d,publicationTitle,dateModified)
##### find corrupt
d[is.na(publicationTitle),]
d<-d[!is.na(publicationTitle),]
```

### Data

The optical character recognition that computers require in order to store text digitally depends critically on the hard work of creating quality scans of journal archives. JSTOR has done a commendable job of this. The entire record for two journals, American Anthropologist and the American Journal of Sociology, was downloaded manually in PDF format, including front and back matter, articles, and book reviews.^[The PDFs were downloaded one at a time over several weeks during July of 2018 in full compliance with the JSTOR terms of use which prohibit automation or web scraping.] Though JSTOR offers an automated service for downloading texts in the bag-of-words format, a frequency table of the terms within it, we opted to use the PDF version which preserves word order. This full text format may be converted into a bag-of-words, but it will also give us the option to choose a unit of analysis smaller than the document by letting us divide the texts at points of our choosing.

```{r z2b-dups}
# find duplicates
d[duplicated(d[,!9:10]),.(dateModified,pub=publicationTitle,tit=substr(title,1,50),date,volume,issue,pages)][,cat(date,' ',volume,' ',issue,' ',tit,' ',pages,'\n',sep=''),by=dateModified]
```

```{r z2p-pgap}
# find page gaps
p<-d[!(duplicated(d[,!9:10])|is.na(bp)),.(
  t=publicationTitle
  ,d=date
  ,v=volume
  ,i=issue
  ,p=mapply(function(b,e) b:e,b=bp,e=ep)
),by=.I]
p[,.(gap=setdiff(
  p %>% unlist %>% range %>% as.list %>% do.call(seq,.)
  ,p %>% unlist %>% unique %>% sort
)),by=.(t,d,v,i)][,.(gap=list(gap)),by=.(t,d,v,i)]
```

```{r z2b-vgap}
# find volume gaps
p[,table(t,i)]
cat('',sep='\n')
vg<-p[,table(d,factor(i,levels=1:6),t)]
vg[vg==0]<-NA
h<-hist(vg)
h<-which(vg<=h$breaks[2],arr.ind = T)
check<-cbind(h[,-1],vg[h])[order(h[,3],h[,1]),]
colnames(check)<-c('i','j','n')
cat('\nDouble check these volumes:\n\n')
check
```

```{r z2b-short}
# find short runs
setorder(d,publicationTitle,date,volume,issue,bp)
lt<-d[!is.na(bp+ep),.(title=title[.N],pages=pages[.N],dateModified=dateModified[.N],url=url[.N]),by=.(publicationTitle,date,volume,issue)]
ltn<-lt[,.N,by=title][N<3,title]
setkey(lt,title)
# will cause autoban, but this will test automatically
if(F) w<-lt[ltn,list({
  Sys.sleep(rnorm(1) %>% abs %>% `+`(1.5))
  cat('.')
  try(url %>% read_html %>% html_text %>% grepl('Next Item',.))
}),by=url]
lt[ltn][,cat(publicationTitle,' ',date,' ',volume,' ',issue,' ',title,' ',pages,'\n',url,'\n\n',sep=''),by=dateModified] %>% invisible
```

```{r jpdf2imp}
f<-'d/d/jpdf.RData'
if(file.exists(f)){
  load(f)
} else{
  # seed<-.Random.seed %>% sample(1)
  # cat('Seed: ',seed,'\n',sep='')
  # set.seed(seed)
  jpdf<-jpdf2imp.f(
    dir('d/d',full.names = T,recursive = T,pattern = '\\.pdf$') # %>% sample(100) # readLines('test.txt')
    ,ocr = T) # write feedback for long imports
  save(jpdf,file=f)
}
rm(f)
if(!'filt'%in%ls()) filt<-list()
filt$import<-cor2aud.f(jpdf$imp,'imported')
```

```{r imp2cln}
f<-'d/d/clnp.RData'
if(file.exists(f)){
  load(f)
} else {
  # seed<-.Random.seed %>% sample(1)
  # cat('Seed: ',seed,'\n',sep='')
  clnp<-imp2cln.f(
    imp = jpdf$imp
    ,hand = 'd/d/cln1537906742.txt'
    # ,seed=seed    
  )
  save(clnp,file=f)
}
rm(f)
if(!'filt'%in%ls()) filt<-list()
filt$clean<-cor2aud.f({setkey(jpdf$imp,doc,par);jpdf$imp[clnp$prd[(mth),.(doc,par)]]},'cleaned')
```

<!--## Sampling {#kd-dp1}-->

```{r imp2ftx,eval=F}
f<-'d/p/ftx.RData'
if(file.exists(f)){
  load(f)
} else {
  # seed<-.Random.seed %>% sample(1)
  # cat('Seed: ',seed,'\n',sep='')
  # set.seed(seed)
  # s<-sample(jpdf$met[,doc],10)
  # ftx<-imp2ftx.f(jpdf$imp[s])
  ftx<-imp2ftx.f({setkey(jpdf$imp,doc,par);jpdf$imp[clnp$prd[(mth),.(doc,par)]]})
  save(ftx,file = f)
}
if(!'filt'%in%ls()) filt<-list()
filt$token<-cor2aud.f(ftx,'tokenized')
rm(f,jpdf,clnp)
```

```{r ftx2pre}
f<-'d/p/pre.RData'
if(file.exists(f)){
  load(f)
} else {
  pre<-ftx2pre.f(ftx,frq = 5,nchr=3)
  save(pre,file = f)
}
rm(f,ftx)
if(!'filt'%in%ls()) filt<-list()
filt$pre<-cor2aud.f(pre[!is.na(stm)],'preprocessed')
```

```{r pre2sam}
f<-'d/p/sam.RData'
if(file.exists(f)){
  load(f)
} else {
  seed<-.Random.seed %>% sample(1)
  cat('Seed: ',seed,'\n',sep='')
  set.seed(seed)
  sam<-pre[sample(doc[!is.na(stm)] %>% unique
                  ,1e2 # add to reduce processing time
                  )]
  sam[,stm:=droplevels(stm,exclude=unique(sam[,.(stm,doc)])[,.N,by=stm][N==1,stm %>% as.character]) %>% droplevels %>% droplevels(exclude=NA)]
  attr(sam,'seed')<-seed
  save(sam,file=f)
}
rm(f)
if(T) {sam<-pre;rm(pre)}
if(!'filt'%in%ls()) filt<-list()
filt$sam<-cor2aud.f(sam[!is.na(stm)] %>% setkey(doc),'sampled')
```

```{r filtd}
f<-'d/b/filt.RData'
if(file.exists(f)){
  load(f)
} else {
  filt<-rbindlist(filt)
  filt<-rbindlist(list(
  lapply(filt,function(x) if(is.integer(x)) round(x/max(na.omit(x))*100,2) else x) %>% do.call(data.table,.)
  ,lapply(filt,function(x) if(is.numeric(x)) max(na.omit(x)) else '100%') %>% do.call(data.table,.)))
  save(filt,file=f)
}
rm(f)
if(latex) filt[,step:=seqinr::stresc(step)]
```

Table \@ref(tab:filt) summarizes the data preparation process. The sample included `r filt[.N,doc] %>% nn` documents containing on average `r {filt[.N,pag]/filt[.N,doc]} %>% round %>% nn` pages and `r {filt[.N,par]/filt[.N,doc]} %>% round %>% nn` paragraphs each. Upon initial importing of PDFs it was apparent that some of the digitization given by JSTOR was partial or corrupted. This was detectable as PDF documents with many pages but few characters. Such texts were re-digitized prior to analysis, and failure to do so would have left considerable data gaps that would have influenced the results.

The cleaning of these raw units is a nontrivial task, first because there are many more items of text in a document than are actually relevant body text, and second because some body text was for one reason or another not successfully digitized during optical character recognition. Rather than writing ad hoc rules to filter out particular lines of text, we take a supervised machine learning approach. We drew a random sample of 1,250 "paragraphs", which included both real syntactic paragraphs and any other elements delineated by a return, from the original PDF import, and we removed 250 for a holdout set for later model testing. We then hand coded all 1,250 training and test paragraphs according to a simple binary of "keep" or "drop". Examples of dropped paragraphs include anything from bibliography entries or partial text from numerical tables to optical character recognition (OCR) false positives, like stray marks on a page or character recognition of a photograph. Items to keep reflected any substantial body text even if it contained some OCR errors.

Given the training set, we measured a dozen features to describe the contents of the paragraph. For instance, the number of vowels, consonants, numbers, and special characters were recorded. These features, both as raw counts and as proportions of the total number of characters per paragraph, were merged with the hand coded labels. The training portion was then fed to a super learner [@Polley2019SuperLearner], which is an ensemble of several different kinds of linear and nonlinear prediction models. @vanderLaan2007Super have shown that a linear combination of the predictions of a diverse set of underlying learning models yields results that are more accurate than any model alone. Once the model was trained, it was validated on the 250 paragraphs in the test set. The model yields a probability that a particular paragraph should be dropped, and a probability threshold must be chosen to decide whether unlabeled paragraphs should be given the keep or drop label. We use a common measure of learner quality called area under the curve (AUC), which measures the performance of a learner as a ratio of the true positive to false positive rate across all classification thresholds. AUC has a possible range of 0.5 (perfect misclassification) to 1 (perfect classification). The model scored a value of `r clnp$auc %>% round(4)`.

The high AUC of our cleaning model gives us confidence that we will have a favorable rate of misclassification even when applying a very lax threshold value. We chose a low probability score of `r clnp$thr['2.5%']` as the threshold, which favors more false positives (keeping a drop) and fewer false negatives (dropping a keep). This permissive threshold is warranted because some of the false positives, which may be mixtures of good and bad text, are likely to be filtered out during subsequent data preparation steps. Roughly one third of paragraphs were dropped by this procedure, which is also an indication of how important this step is to get right. Indeed we may have more confidence in our ability to clean given access to the original full text than being supplied a bag-of-words format which offers little context for deciding what to filter out.

Once data are clean they may be tokenized, which is to split at word and punctuation boundaries. We invented a reversible bag-of-words (BOW) data format to store tokenization such that the document, page, line, and paragraph locations of each token are preserved. Conventionally this information is thrown out, but keeping it allows great flexibility to the analyst and also makes it possible to trace even a BOW style technique like topic modeling back to the originating text. It also allows the researcher to easily bin tokens differently than at the document level, providing an opportunity for multilevel analysis. To wit, by revealing punctuation, the reversible BOW format lets us count sentences as well.

The final step is called pre-processing, and involves making decisions about which tokens to keep and which to discard. In the reversible BOW format nothing is actually dropped, rather tokens are flagged by different drop criterion. For instance true/false value of whether a token is a stop-word or a number is recorded.  The researcher may then query the database differently for different research decisions. We dropped punctuation, stop-words, terms smaller than three characters, and we kept numbers especially out of a concern to preserve dates. Finally all tokens are lemmatized, which reduced words in several parts of speech to a root. For instance, the lemma for "run", "ran", and "running" may simply be "run". Lemma are often abstract or non words themselves, so to aid in later interpretation we calculated the lemma common term, which is the token of highest frequency within a lemma category.

```{r filt,include=T}
sg(filt[step!='sampled'][.N,step:='N at 100 Percent'] %>% {.[,lapply(.SD,function(x) c(as.character(x[-.N]),x[.N] %>% nn))]} %>%  setnames(ec('Step,Document,Page,Paragraph,Sentence,Token,Term,Lemma')),tit='Term filtering due to data management',digit.separator=',',type = 'latex')
```



```{r pre2mlc}
f<-'d/p/mlc.RData'
if(file.exists(f)){
  load(f)
} else {
  mlc<-pre2mlc.f(sam)
  save(mlc,file = f)
}
rm(f)
rm(sam)
```

```{r pre2des}
# des.com<-pre2des.f(pre)
# des.stm<-pre2des.f(pre[!is.na(stm)])
#kab(list(des.com,des.stm,data.table(tot=round(des.stm$cnt/des.com$cnt*100,2))),caption = 'Full Text Descriptives')
```

### An historical interlude

```{r jdpf-desc,include=T,fig.cap='Annual count of journal issue contents, 1888-1922. Mean line with 95 percent confidence interval is fit using a robust regression with a quadratic term for year.'}
jpdf$met[,year:=as.integer(year)]
jpdf$met[,.N,by=.(journal,year)] %>% {
  warp(myth(ggplot(.,aes(x=year,y=N,color=journal)) + geom_smooth(method='rlm',formula=y~poly(x,2),size=.5) + geom_line(size=.25) + geom_point(size=.5))+tl,r=.$N %>% range,ann=war['WWI',on='text']) + scale_x_continuous(breaks=seq(1880,1930,5)) + xlab('Date')
  } %>% plt
```

With some data in hand we may now describe anthropology and sociology in terms of the content of their two leading journals, American Anthropologist (AA) and the American Journal of Sociology (AJS). Figure \@ref(fig:jdpf-desc) plots the total number of items contained in the journal each year, including, in addition to research articles, front and back matter, book reviews, advertisements, errata, and other miscellaneous items. First it can be noticed that AA has almost a decade's lead before the start of AJS, but that AJS starts as a larger publication in these terms. It is sensible to expect that because anthropology was an empirical discipline that the rate of completion of research projects, which often required substantial field work, would be less than sociology, which had a greater role for social theory which was simply easier to produce. The trend for AA rose steadily but approached a plateau by 1910, after which it declined rapidly until the start of WWI. Two scenarios suggest themselves, first that these were delays in publishing received material as there was a steep correction in 1915, or second that this was an actual shift in focus precipitated by unsettled lives at the dawn of war. Sociology did not appear to experience a decline at all, but rather an acceleration of activity perhaps in anticipation of the war and continuing after.

Figure \@ref(fig:pre2ntok) paints a more granular picture and shows a sharper contrast between the two journals. Here we see the count of the total number of words appearing in the journal each year, which allows us to control for variation in the physical format of the volumes. In the decade before the turn of the century AA plodded along very steadily in the neighborhood of 125,000 words a year. Perhaps noticing that it was being eclipsed in scale by peers in the social sciences, or perhaps because of the pressure of the newly arriving Ph.D.s, AA nearly doubles its output in a single year. It could not however sustain the injection and steadily falls back to its original level in a trend that all but ignores the war, save for a flurry of activity in 1915. These trends suggest that anthropology struggled to maintain its footing at it adapted to the new university context and the growth of its peers.

```{r pre2ntok,include=T,fig.cap='Total word counts of all issues of all journal contents annually, 1888-1922.  Mean line with 95 percent confidence interval is fit using a robust regression with a quadratic term for year.'}
f<-'d/b/ntok.RData'
if(file.exists(f)){
  load(f)
} else {
  ntok<-pre[!is.na(com),.N,by=.(journal=sub('d/d/jpdf/([^/]+)/([^/]+)/.*','\\1',doc),year=sub('d/d/jpdf/([^/]+)/([^/]+)/.*','\\2',doc) %>% as.integer)]
  save(ntok,file=f)
}
rm(f)
ntok %>% {
  warp(myth(ggplot(.,aes(x=year,y=N,color=journal)) + geom_smooth(method='rlm',formula=y~poly(x,2),size=.5) + geom_line(size=.25) + geom_point(size=.5))+ ylab('Annual word count') + theme(legend.title = element_blank()) + ll
       ,r=.$N %>% range,ann=war['WWI',on='text']) +  scale_x_continuous(breaks=seq(1880,1930,5)) + xlab('Date')
  } %>% plt
```

We may now drill down further into these words. The goal of the following analysis will be to organize the text within each journal into subcategories of common vocabulary. We hesitate to call these subfields without qualitative cross validation, which is beyond the scope of the present study. What we will be able to accomplish is a prima facie argument for the scale and diversity of vocabularies at play within each journal. As we have discussed, vocabularies are not ideas, but they are good indications of who is speaking and about what.

### How many topics?

The biggest challenge for a study of historical vocabularies is discerning the scale of diversity. To borrow from Wuthnow [-@Wuthnow1989Communities] and at the risk of impoverishing the term, how many different communities of discourse were at play within the journal space and in the relatively settled time following the sowing period? In this section we leverage the rich text data at our disposal to take a data modeling approach to answering this question. Doing so will require passing a technical hurdle in the application of the particular method of topic modeling that we have discussed.

In using topic models to address historical questions researchers have found it very useful to assume an arbitrary number of topics, because the results of the model may still provide insights even when the true diversity of vocabularies is unknown. Some lines of research consider the gold standard for whether the analyst has chosen the right number to be the ease with which human readers can interpret topics [@Chang2009Reading]. In our case our goal is to capture a real phenomenon that is independent of our likely anachronistic readings of texts. We want to know the number of vocabularies that were actually at play in the historical situation, the expectation being that the number is far greater than an analyst might choose for either interpretive or computational convenience.

```{r pre2tel}
f<-'d/p/tel.RData'
if(file.exists(f)){
  load(f)
} else {
  tel<-pre2tel.f(pre[!is.na(stm)],stop = 3)
  save(tel,file = f)
}
if('pre'%in%ls()) rm(pre)
rm(f)
```

```{r tamK}
f<-'d/p/clu.RData'
if(file.exists(f)){
  load(f)
} else {
  tel<-tel[!s%in%tm::stopwords()] # remove stopwords
  tel<-tel[!r%in%tm::stopwords()]
  tel<-tel[s!=r] # remove remaining loops
  system.time(clu<-tel2clu.f(tel))
  save(clu,file=f)  
}
rm(f)
```

```{r ctr,eval=F}
ctr<-apply(clu$memberships,1,function(x) x %>% table %>% sort(decreasing = T))
View(ctr)
top<-ctr[[1]] %>% prop.table %>% `*`(100) %>% round(3) 
top %>% head(30)

cp<-tilit::ov2chpt.f(top %>% head(50),min.period = 2,drv = 1)
top %>% head(50) %>% as.vector %>%  plot(col=cp$g)

```

```{r crs,eval=F}
sm<-list()
for(i in 1:nrow(clu$memberships)) sm[[i]]<-factor(clu$memberships[i,],levels=order(table(clu$memberships[i,]),decreasing = T))
sm<-do.call(data.table,sm)
setnames(sm,paste0('h',ncol(sm):1))
sm[,cr:=clu$names]
sm[,m:=as.integer(h1)] %>% setkey(m)
sm %>% setkey(cr)

tel[,`:=`(ms=sm[s,h3],mr=sm[r,h3])]
tel[,bc:=ms==mr]
bct<-list(tel[s!=r,.(tew=sum(ew)),by=.(s,bc)] %>% setnames('s','cr')
,tel[s!=r,.(tew=sum(ew)),by=.(r,bc)] %>% setnames('r','cr')
) %>% rbindlist %>% .[,.(tew=sum(tew)),by=.(cr,bc)] %>% setkey(cr,bc)
bct[,m:=sm[cr,h3]]
bct<-dcast(bct,cr+m~bc,value.var = 'tew',fill=0)
bct[,tew:=`FALSE`+`TRUE`]
bct[,pit:=`FALSE`/tew] # proportion internal ties
bct[,`:=`(`FALSE`=NULL,`TRUE`=NULL)]
bct[,m:=as.integer(m)]
bct %>% setorder(m,-pit)
bct %>% setkey(m)
plt(
myth(
ggplot(bct[.(9)],aes(x=tew,y=pit)) + geom_text(aes(label=cr,angle=45),alpha=.5)
) + scale_x_log10())
```

```{r mlc2mlk,eval=F}
f<-'d/p/mlk.txtbkp'
if(file.exists(f)) {mlk<-fread(f)} else {
  system.time(mlc2mlk.f(mlc,verb=T) %>% fwrite(f))
  pbapply::pbreplicate(999,mlc2mlk.f(mlc) %>% fwrite('d/p/mlk.txt',append = T),cl = 1)
}
rm(f)
mlk<-melt(mlk,measure.vars = names(mlk),variable.name = 'level',value.name = 'K') %>% setkey(level)
```

```{r mlk2sim}
f<-'d/b/sim.RData'
if(file.exists(f)) {load(f)} else {
  sim<-mlk2sim.f(mlk,rep = 1e3)
  save(sim,file=f)
}
rm(f)
```

Researchers conventionally feed entire documents into the construction of term frequencies. This method treats any term in a document as being related to any other term by the same degree. The goal of any topic mixture model algorithm is to sift these terms into different topic categories by looking for clues across documents; a topic can be "seen" in a particular document to the extent that other documents include that topic as well as other topics not present in the focal article, such that the intersection of terms reveals the topic. But a much simpler assumption to reduce the attendant noise within a document is to merely feed lower level syntactic structures--paragraphs and sentences--to the algorithm. We will see that doing so greatly improves the usefulness of discovered topics.

To infer the correct number of topics we use the method described by @Mimno2014Lowdimensional and implemented in the R package `stm` [@Roberts2018stm]. The concept is to represent words as coordinates in a vector space defined by every other term in the corpus, operationally as a square matrix with height and width equal to the length of the number of unique words in the global corpus, which in this case is `r mlc$voc %>% length %>% nn`. The elements of this matrix, called $Q$, are counts of the number of texts in which both words appear. Leveraging the anchor words technique of @Arora2018Learning, which posits that for every topic there exists a term present for that topic and no others, it is possible to identify these anchor words as points on a convex hull encompassing every other term in the corpus. The calculation of the convex hull of $Q$ is nontrivial in its original high dimensional space but can be approximated. Mimno and Lee argue rather than approximate the convex hull in high dimensions the hull may be calculated exactly in an approximation of the original space in a low number of dimensions.

The value of this approach is that the exact calculation of the convex hull of an approximation of $Q$ solves two important problems for the resolution of a topic model; it makes an inference about the number of topics $K$ and identifies the anchor words that can be used to exactly calculate the topic by term matrix. Mimno and Lee recommend a combination of computationally fast algorithms--random principal component analysis [rPCA, @Rokhlin2009Randomized] and t-distributed stochastic neighbor embeddings [t-SNE, @Maaten2008Visualizing]--to achieve the low dimensional approximation of $Q$.

```{r}
res<-data.table(sim$fig1$data)
sci<-res['doc',on='level',rep(count,observed %>% {.*1000} %>% round) %>% quantile(p=c(.025,.5,.975))] %>% round
pft<-res['doc',on='level',vcd::goodfit(cbind(observed*1000 %>% round,count),'poisson')]

```

Though this is a clever solution to the choice of the number and contents of topics, in practice there is a lot of measurement error due to the randomization aspects of the $Q$ approximation. Looking at just the estimate of the number of topics, repeating the algorithm multiple times creates a count distribution of possible Ks. On our corpus the median count of the number of topics running the approximation 1,000 times was `r sci[2]`, with a 95 percent confidence interval between `r sci[-2] %>% nl`. While this is very helpful in getting the researcher into the ballpark for choosing the correct $K$, models fit with these varying assumptions for the number of topics will yield very different results.

While it may be sensible to simply choose the median outcome of 78 for $K$, it is not obvious that a notion of central tendency is appropriate given the highly nonlinear structure of $Q$. To get a sense of how the empirical distribution of $K$ compares to a plausible parametric distribution we model the error in the inference of $K$ as a Poisson, using a maximum likelihood estimate of `r pft$par$lambda %>% round(3)` for lambda. We then simulate from this distribution to obtain a range for each $K$. By and large the observed counts of $K$ fall within the ranges for each possible $K$ suggested by this distribution very well, which lends credence to choice of lambda as the correct $K$.

There are however a few notable exceptions at the extremes of the distribution that diverge considerably from the Poisson. On the low side, a $K$ of 50 occurs much more often, appearing more than `r sim$tab[.('doc',50),on=ec('level,c'),t %>% floor %>% nn]` standard deviations higher than the expectation. On the high side a $K$ of 104 appears more than `r sim$tab[.('doc',104),on=ec('level,c'),t %>% floor %>% nn]` standard deviations more often.

It may be that the task of coming up with the one true number of topics is not itself valid, that instead there may be several valid choices for $K$ that differ by an assumption about the resolution of the underlying topic structure of the documents. It may not be a coincidence that the mean of 50 and 104, 77, is almost exactly the same as the median value 78 of repeated runs of the low dimensional embedding method of Mimno and Lee. If we assume for the sake of argument that these three levels of $K$ are each valid, it implies that there may be a topic resolution assumption that helps explain the estimation variability.

We theorize that the three $K$ tiers discovered here reflect a real pattern of topic classifications. We posit that the low level of 50 describes the true seed topics out of which further combinations are routinely developed. The mid level of 78 we posit as a nonrandom pattern of combinations of the original 50 that is much lower than the thousands of combinatoric possibilities given 50 seed topics. Finally the high level of 104 we predict represents a transition from common to idiosyncratic discourses, the point at which the topic model begins to identify features of language and style that are sometimes considered junk but that are more appropriately conceptualized as personal rather than public features of discourse.

To explore the conjecture that the low tier represents elemental topics, we conducted the following test. Documents are expected to be mixtures of topics, but we may surmise that these mixtures are a result of combining many lower level syntactic units that are themselves unmixed or at least less mixed, that is, that only express one or two topics. Sentences and paragraphs are easily identifiable low level syntactic units given access to the original full text of documents. It is credible to expect sentences or paragraphs to yield a clarified topic structure. Splitting up documents introduces an assumption that proximity matters even if word order is not strictly preserved. The bag-of-words format leads to the same association between words no matter how far apart they actually appear in the original text, though we may believe that words on the same page are more relevant to each other than words several pages apart. Organizing the large bag, documents, into smaller bags effectively prunes the associations of terms that are far apart by completely ignoring them unless they are very close together.

Figures \@ref(fig:sim-fig) and \@ref(fig:sim-fig2) illustrate the results of repeating the procedure of Mimno and Lee at these different syntactic levels. In Figure \@ref(fig:sim-fig) the density of observed counts are represented as points labeled with their corresponding $K$ value. The curves represent the fitted Poisson distributions for each level, while the shaded areas represent the simulated confidence intervals for each possible value of $K$. As mentioned, most labels fall within the confidence interval, which suggests the the medians are a good operationalization of the real $K$. This however appears to be more true of the document than the sentence or paragraph levels, which seem to be systematically above the curve in the center of the distribution and below it on the tails.

```{r sim-fig,include=T,fig.cap='Distribution of K by syntactic unit calculated by repeated solution of convex hull on low-dimensional embedding of word space.'}
sim$fig1
```

This feature of a more pointed shape is called leptokurtosis and corresponds to a kurtosis score greater than zero. If the empirical distributions of $K$ are more leptokurtotic than the assumed Poisson error distribution it would suggest there is a tendency for the random search to be pulled more strongly to the median of these distributions, in a sense "pointing to" the true $K$ value, lending weight to the median as the correct value of $K$. To test whether this appearance is significant we calculate the kurtosis of all simulated distributions from each Poisson and then calculate the probability that the observed kurtosis is less than the simulation. The results are reported in Table \@ref(tab:mlk2k). First, the appearance of leptokurtotic "pointedness" is not supported by a calculation of each distribution's actual kurtosis in the case of documents or of paragraphs, and only very weakly in the case of sentences. None of the observed distributions are significantly more leptokurtotic than their associated Poisson, though it is more probable that the sentence level is more pointed than the other two.

```{r mlk2k,include=T}
f<-'d/b/k.RData'
if(file.exists(f)){
  load(f)
} else {
  ok<-mlk[,.(o=psych::kurtosi(K)),by=level] %>% {x<-.$o;names(x)<-.$level;x}
  ek<-mlk[,.(e=pbapply::pbreplicate(psych::kurtosi(rpois(1000,vcd::goodfit(K,'poisson')$par$lambda)),n = 5000,cl = 4)),by=level] %>% data.table
  prb<-ek[,mean(ok[level]<e),by=level]%>% {x<-.$V1;names(x)<-.$level;x}
  k<-data.table(level=names(ok),observed=ok,ek[,.(expected=mean(e),l99=quantile(e,.01),h99=quantile(e,.99)),by=level][,!'level'],`P(o ≤ e)`=prb)
  save(k,file=f)
}
rm(f)
#debugonce(sg)
sg(k[,level:=ec('document,paragraph,sentence')],digits=5,tit='Kurtosis permutation test')
```

Though the kurtosis test amounts to only very weak evidence that sentences represent a more stable topic structure than paragraphs or documents, when viewing the outcomes at the three levels together a more confident conclusion is apparent. Figure \@ref(fig:sim-fig2) plots only the labels that are significantly different than their Poisson errors. What is notable about the highly significant $K$ of 50 at the document level is that it is located at the center of the combined sentence and paragraph structure. This suggests that 50 is a very good choice for the documents but one that a random search is very unlikely to find. That there is correspondence between the solution at different syntactic structures makes sense since documents are nothing but their sentences and paragraphs. By using the comparison to lower level units we conclude that 50 is the best choice for $K$ for a document level topic model.

```{r sim-fig2,include=T,fig.cap='Significant Counts of K.'}
sim$fig2
#sg(sim$tab,'Significant Counts of K')
```




```{r mlc2ckm}
f<-'d/b/ckm.RData'
if(file.exists(f)){
  load(f)
} else {
  if(!'pre'%in%ls()) load('d/p/pre.RData')
  cll<-pre[!is.na(stm),.N,by=.(doc,stm)]
  # cut down vocab
  cl1<-cll[,.(N=max(N)),by=stm]
  cl2<-cl1[,table(N)] 
  n<-names(cl2)
  n<-cl2 %>% unclass %>% ov2chpt.f(drv = 2) %>% {data.table(n=n %>% as.integer,.)[!duplicated(g),]}
  # has to appear seven times in an article
  dw<-list(ldf=cl1[N<n[2,n],stm %>% as.character])

  cl1<-cll[,.N,by=stm]
  cl2<-cl1[,table(N)] %>% head(500)
  n<-names(cl2)
  n<-cl2 %>% unclass %>% ov2chpt.f(drv = 2) %>% {data.table(n=n %>% as.integer,.)[!duplicated(g),]}
  
  # too restrictive, just pick 10 docs at least
  # you know what, let's do it
  dw$lfd<-cl1[N<n[2,n],stm %>% as.character]
  dw %<>% unlist %>% unique
    
  system.time(mlc<-pre2mlc.f(pre[!dw,on='stm'][,stm:=droplevels(stm)],'doc'))
  mta<-pre[!dw,on='stm',.(doc=unique(doc))]
  mta[,so:=unique(doc) %>% sub('d/d/jpdf/([^/]+)/.+','\\1',.) %>% factor(labels=ec('A,S'))]
  rm(pre)
  mta[,ix:=.I]
  mta<-merge(mta,jpdf$met,by='doc',all.y = F)
  system.time(mod<-stmbow2lda.f(list(documents=mlc$doc$spm,vocab=mlc$voc),out.dir = 'd/p',k = 50,verbose = T,check.for.saved.output = F,save.to.disk = T,alpha=1,sig.pri = 0,it='Spectral',prevalence=~so,data=mta))
  ckm$meta<-mta
  save(ckm,file=f)
}
if('jpdf'%in%ls()) rm(jpdf)
rm(f)
```

## <!--M2--> Topic interpretation

We fit a structural topic model with $K$ equal to 50 and with one document covariate term, whether the document was from anthropology or sociology. While not strictly necessary, the discipline term provides a convenient method of ranking the topics according to the discipline in which they are most prevalent. Figure \@ref(fig:kts) shows all 50 topics as a time series by discipline, ranked from the most prevalent in anthropology to the most prevalent in sociology. Toward the middle of the grid are topics that are more equally divided between the disciplines. We will first characterize the most anthropological and most sociological topics and then discuss a few that are mixed, and in the next section we will analyze whether there are turning points that correspond to the exogenous events we have described above.

Mathematically, a topic is a pair of probability vectors, one describing the topic as a composition of terms, and the other describing documents as a composition of topics. Researchers conventionally use these probabilities to create ranked word and document lists that then stand in for the qualitative content modeled by the topics. An inspection of the underlying documents is beyond the present scope, but it would entail dividing the corpus into clusters based on the main topic classifications of each document and possibly by different patterns of topic mixing. Instead we understand documents in their disaggregated forms, with the portion they contribute to a topic being added to that of every other document participating in the same vocabulary.

Although vocabulary is an abstract concept it nonetheless represents an independent dimension of scholarship. Vocabularies are ways that scholars can connect to common conversations. They exist above and between particular documents, and in many ways vocabularies are a signal of relevance. The mass of a vocabulary can be thought of as the aggregate number of words drawn from it across the entire professional field. We assume perfect information and access to all scholarship, which at this point in history is not an unrealistic standard given that we have limited the study to the single most important journal in each discipline. This means that the average scholar was aware of most of the different conversations occurring within their own field of study, an assumption that in the present day would be untenable given the subsequent growth of the social sciences. By measuring the number of words drawn from each vocabulary we have a macroscopic view of how the attention of the entirety of each discipline was allocated and when.

Below we will characterize selected topics both by the pattern of their time series and by the words most important within each topic. There are two approaches to measuring a term's importance within a topic. The first is its raw frequency, which is its local topic probability multiplied by its global corpus frequency. These are the words that appear the most often within a topic. Second is a topic's relative share of a term's frequency, which means that a term is important to the extent that it is exclusively concentrated within a topic. In the tables that follow we provide a term list of each type, as well as a third list which is an equal weighting of both criteria. The formula we use to construct these lists is called relevancy, and it is controlled by a weighting parameter lambda that sets the ratio of frequency to exclusivity [@Sievert2014LDAvis]. When lambda is close to 0 the ranking reflects exclusivity, when it is close to 1 it reflects frequency, and when it is 0.5 it is an equal weighting. It is possible for a word to be both frequent and exclusive, and this may appropriately be considered evidence of even greater importance to a topic. Indeed as a single label to refer to topics that is more memorable than a number, we will use the most highly ranked term when frequency and exclusivity are ranked equally.

```{r ckm-mef}
f<-'d/b/mef.RData'
if(file.exists(f)){
  load(f)
} else {
  gt<-1:50 #setdiff(1:50,c(4,12,17,20,24,35,40))
  mef<-stm::estimateEffect(formula = gt~so,stmobj = ckm$model,metadata = as.data.frame(mta))
  x<-list()
  for(i in gt) x[[i]]<-try(summary(mef,i)$tables[[1]] %>% {c(.[,1],.[,4])})
  x<-data.table(do.call(rbind,x))[,t:=.I] %>% setnames(ec('int,sos,pi,ps,t'))
  x[,o:=sos>0][(o),ps:=ps*-1]
  setorder(x,-o,-ps)

  attr(x,'mef')<-mef
  mef<-x
  save(mef,file=f)
}
rm(f)
```


```{r relh}
wa<-ckm$model$beta$logbeta[[1]] %>% {which(apply(.,2,function(x) sum(x==-1000))==nrow(.)-1)}
wa<-data.table(k=ckm$top.word.phi.beta[,wa] %>% apply(2,function(x) which(!!x)),a=ckm$vocab[wa])
rel<-plagiat:::lda2rel.f(ckm,lambda.step = c(.01,.5,1))
trm<-rel[.('0.5',1),on=ec('lambda,ord'),Term]
rel<-rel[,.(Terms=paste(Term,collapse=', ')),by=.(Category,lambda)] %>% split(by='Category')
for(i in names(rel)) rel[[i]]<-rbindlist(list(
  data.table(lambda='',Terms=wa[.(sub('^T','',i) %>% as.integer),on='k',sprintf('model anchor: %s',a)])
  ,rel[[i]][,.(lambda=as.character(lambda),Terms)]
))
names(rel)<-trm
```


```{r ckm3kts}
f<-'d/b/kts.RData'
if(file.exists(f)){
  load(f)
} else {
  kts<-ckm %>% {.$doc.top.theta*.$doc.length} %>% data.table %>% {setnames(.,1:ncol(.) %>% as.character)} %>% data.table(.,so=ckm$meta$so,yr=ckm$meta$year) %>% melt(id.vars=ec('so,yr'),value.name='f',variable.name='K') %>% .[,.(f=sum(f)),by=.(so,yr,K)]
  save(kts,file=f)
}
rm(f)
kts[,K:=factor(K,levels=mef$t %>% rev)]
```

```{r chnpnt}
library(lspline)
grd<-dcast(kts,formula=yr~so+K,value.var = 'f') %>% na.omit %>% setorder(yr)
ch1<-ecp::e.divisive(grd[,!'yr'],k=1,min.size=2,sig.lvl = .01)
#grd[,cbind(yr,ch1$cluster)]
withr::with_seed(12345,chn<-ecp::e.divisive(grd[,!'yr'],min.size=2,sig.lvl = .01))
chm<-grd[,.(yr,g=chn$cluster)]
kts[chm,on='yr',g:=factor(g)]
ch<-grd[,cbind(yr,chn$cluster)][chn$estimates] %>% {.[-c(1,length(.))]}
kts[is.na(g),g:='1'] # truncated portion of anthropology, bit of a fudge
slp<-kts[,.(rlm=list(rlm(f~lspline(yr,ch,marginal = T)))),by=.(so,K)]
slp[,slpwr:=sapply(rlm,function(x) coef(x) %>% tail(1))]
slp[,mslp:=sapply(rlm,function(x) coef(x) %>% tail(2) %>% {.[2]/.[1]})]
# not as clever as I thought
# slp[slpwr<=0,g:=kmeans(slpwr,2)$cluster]
# slp[slpwr>0,g:=as.integer(kmeans(slpwr,2)$cluster+2)]
# gr<-slp[,.(m=mean(slpwr)),by=g] %>% setorder(m) %>% .[,ec('neg,zer,zer,pos')[g]]
# slp[,slg:=gr[g] %>% factor(levels=ec('neg,zer,pos'))]
# slp<-slp[order(slg,abs(slpwr)),]
slp[,g:=cluster::pam(slpwr,3) %>% {factor(ec('neg,zer,pos')[order(.$medoids)][.$clustering],levels=ec('neg,zer,pos'))}]
```

```{r kts,include=T,fig.height=8,fig.cap='Topic time series of sociology (orange) and anthropology (green). Darker topic labels indicate higher corpus frequencies. Pink lines mark beginning and end of WWI.',out.height='100%'}
pkts<-function(DAT,sl,ko,clm=5,sum=F,bly=T) {
  if(!missing(ko)) levels(DAT$K)<-ko
  if(!missing(sl)) DAT[,slg:=factor(slg,levels=c(sl,setdiff(levels(slg),sl)))]
  p<-myth(ggplot(data=DAT,aes(x=yr,y=f,color=so))) +
  geom_vline(xintercept = war['WWI',on='text',c(xmin,xmax)],color='pink') +
  geom_text(data=DAT[,.(f=max(f) %>% round,yr=min(yr)),by=K],aes(label=K,x=yr,y=f,alpha=f),inherit.aes = F,vjust='inward',hjust='inward',color='black',show.legend = F)
  if(sum) {p<-p + geom_line(show.legend = F)} else {p<-p + geom_smooth(method="rlm", formula=y~lspline(x,ch), se=FALSE,size=.5,aes(linetype=slg),show.legend = F) + geom_point(show.legend = F,size=.25)}
  p<-p + facet_wrap(vars(K),scales = 'free_y',ncol = clm) + 
theme(strip.background = element_blank(),strip.text.x = element_blank()) +
  xlab(element_blank())  + ylab(element_blank()) + ylim(0,NA) + xlab('Date')
  if(bly) p<-p+ blanky
  p
}
pkts(kts,sum=T)
```

From this population of time series we select several illustrative cases. First we compare topics that align strongly with one discipline or the other to those that are mixed. Then we compare topics according to their trajectories ahead of and during WWI.

### Anthropology

Topic 29, *`r trm[29]`*, is the largest for anthropology and encompasses the archaeology of native American civilizations. It includes research into the famous mound builders of the Adena cultures, but covers all discourse surrounding excavation. It may rightly be called the archaeological discourse within anthropology. The topic trend is consistently high over the sample period with a peak around 1907. 

(ref:t29-a) `r sprintf('Topic %s, *%s*',29,trm[29])`

```{r t29-a,include=T}
sg(rel[[29]],tit='(ref:t29-a)',tabularx = T,new.col.align = 'lY')
```

Topic 28, *`r trm[28]`*, shows a more variable pattern. It is relatively week until just before the turn of the century where it experiences a surge. Also relevant to archaeology, and even more explicitly so, the topic covers the names of important figures like Smithsonian. *`r trm[28]`* may be the archival facet of the same endeavor that *`r trm[29]`* represents in the field. The rise in the trend is mirrored by a similar decline in the second half of the period, which is consistent with expected decline of the term ethnology which is very frequent.

(ref:t28-a) `r sprintf('Topic %s, *%s*',28,trm[28])`

```{r t28-a,include=T}
sg(rel[[28]],tit='(ref:t28-a)',tabularx = T,new.col.align = 'lY')
```

Topic 31, *`r trm[31]`*, shows a different pattern, rising only slowly at first then gaining speed toward the end of the period and through the war. It represents a concern with the culture and politics of lineage in tribal social structure. The model anchor phratry is a technical term for kinship groups, and exogamy refers to marriage between groups.

(ref:t31-a) `r sprintf('Topic %s, *%s*',31,trm[31])`

```{r t31-a,include=T}
sg(rel[[31]],tit='(ref:t31-a)',tabularx = T,new.col.align = 'lY')
```

### Sociology

Topic 34, *`r trm[34]`*, is an example of a trend continuously on the rise in this period. It is also exemplary for sociology's preference for abstract theorizing, which stood in stark contrast to anthropology's deep embedding in empirical field studies. The terms are relevant to psycho-social discourse around human drives and experiences of sociality and selfhood.

(ref:t34-s) `r sprintf('Topic %s, *%s*',34,trm[34])`

```{r t34-s,include=T}
sg(rel[[34]],tit='(ref:t34-s)',tabularx = T,new.col.align = 'lY')
```

Topic 19, *`r trm[19]`*, refers again to more abstract terms in social theory and jurisprudence as well as some terms like evolutionary and equilibrium suggesting models informed by the sciences. It's trend represents one of constant attention across the period.

(ref:t19-s) `r sprintf('Topic %s, *%s*',19,trm[19])`

```{r t19-s,include=T}
sg(rel[[19]],tit='(ref:t19-s)',tabularx = T,new.col.align = 'lY')
```

Few series from sociology exhibit a decreasing trend, but topic 30, *`r trm[30]`*, is chief among them. It is a very important topic at the beginning of the period then declines rapidly to a low point around 1905 after which it meanders along never recovering its former significance. The concern with religion appears to be narrowly Christian, and it is unclear whether the discourse is about religion or is itself a *religious discourse*. If the latter then its decline may make more sense as sociology was founded on secular principals.

(ref:t30-s) `r sprintf('Topic %s, *%s*',30,trm[30])`

```{r t30-s,include=T}
sg(rel[[30]],tit='(ref:t30-s)',tabularx = T,new.col.align = 'lY')
```

### Interdisciplinarity

Most of the topics that contain a mixture of sociology and anthropology are small in frequency, though there are few exceptions that are larger. A significant one is topic 32, *`r trm[32]`*, which reflects professional engagement with humanistic fields like English or it may be conversations on the journalism of the day. Apart from a spike in sociology around 1905 the two disciplines participate evenly in this discourse.

(ref:t32-m) `r sprintf('Topic %s, *%s*',32,trm[32])`

```{r t32-m,include=T}
sg(rel[[32]],tit='(ref:t32-m)',tabularx = T,new.col.align = 'lY')
```

Another smaller mixture is topic 26, *`r trm[26]`*, which likely refers to professional communications and news around conferences. It is interesting that there is rarely a substantive overlap between disciplines, but that they mirror each other in their metadiscourse around the profession itself. It may be the case that the disciplines looked to each other to replicate patterns of professional association.

(ref:t26-m) `r sprintf('Topic %s, *%s*',26,trm[26])`

```{r t26-m,include=T}
sg(rel[[26]],tit='(ref:t26-m)',tabularx = T,new.col.align = 'lY')
```

### Better, worse, or stayed the same

While the examples above provide some cursory insights into a few of the notable topics, our goals here are not primarily qualitative. We aim to observe how fields waxed and waned in response to larger events in American society, namely the onset and duration of WWI. When inspecting individual time series it is easy to see associations with particular events that may in fact be coincidences, chance occurrences, or trends that are better explained by different happenings. If, however, multiple times series appear to shift at similar times, it is stronger evidence that the momentous event is the one that matters most.

In this section we take an inductive approach to answering this question. The change point analysis already applied above is well suited to this task. Whereas before we looked for turning points in singular times series, now we may look at all 100 trends simultaneously, one each for sociology and anthropology in each topic. The change point algorithm iteratively explores each annual transition to see whether the mean of the trend is statistically different on either side, and it returns change points only if they pass a threshold of statistical significance.

Here we set the threshold to a significance level of .99 and feed each topic time series to the change point algorithm, truncating the period of AA that predates the start of AJS to make the disciplines comparable, for a final range of `r grd[,yr %>% range %>% nl %>% sub('and','to',.)]`. Additionally we may specify the minimum distance between detected points, the smallest interval being two years. We choose this interval to allow the solution to be as granular as the data suggest. Nevertheless this procedure yielded only two significant change points, one in `r ch[1]` and one in `r ch[2]`. The earlier of these is plausibly close to the end of the sowing period of rapid institutional growth, while the second falls only two years before the onset of war.

We take these findings of two change points as some support or our central argument that the historical context matters. What remains to be seen is how the history mattered. To address this question for the onset of war we classify the trends into categories of the direction of change before and after the change point. The possibilities were that a trend could slow down, speed up, or stay the same. To model these effects we apply a robust regression using a spline term with knots set to our change points. This model produces a linear effect in the periods between the change points that is also constrained to intersect at the knots such that the predicted trend is continuous.

Table \@ref(tab:chnsum) reports the distribution of the two disciplines according to these three possibilities. In the right margin it can be observed that the majority times series exhibited no change in trend. Some of this effect is owed to the large number of series that are essentially off for one or the other discipline, hence a flat trend near zero for the entire period. The negative and positive numbers are more revealing. In the aggregate there are more than twice as many negative changes as there are positive changes. This is consistent with the expectation that war would have a depressive effect on organizational output especially where that organization, social science scholarship, is not understood to be immediately relevant to the war effort. Simply, attention shifted elsewhere.

Within disciplines there are considerable differences. The AA series were more likely to stay on an even trajectory, whereas the AJS series were more likely to experience especially negative change. It should be noted here that what we are measuring is the change in the slope of the line before and after `r ch[2]`. A series that is already sloping downward will be classed as a positive change if the decline abates to a shallower trend, even if the trend is still down. Any trend that stays on its former path, whether increasing or decreasing, will be classed as not changing.

(ref:chnsum) Proportion of topic time series by discipline and change in slope in `r ch[2]` ahead of WWI

```{r chnsum,include=T}
mt<-slp[,table(g,so) %>% prop.table] %>% addmargins %>% data.table %>% dcast(g~so,value.var='N') 
mt[,g:=factor(g,levels=ec('neg,zer,pos,Sum'),labels=ec('Negative,No change,Positive,Total'))] %>% setorder(g)
mt %>% setnames(ec('Δ Slope,Anthropology,Sociology,Total'))
kts<-merge(kts,slp[,.(so,K,slg=g)],by=ec('so,K'))
sg(mt,tit='(ref:chnsum)')
```

Figure \@ref(fig:slwdwn) illustrates the four trends with the biggest downward shifts in absolute terms. This means that the larger a series, the less it need decline to register a change, and the smaller the more significantly it needs to change to appear.

```{r slwdwn,include=T,fig.cap='Cases that slow down before WWI'}
ok<-slp[,.(s=slpwr[which.max(slpwr)]),by=K][order(s,decreasing = T),K %>% as.character]
{l<-'neg';l} %>% {kts[,.(w=any(slg==.)),by=K][,K[w] %>% as.character]} %>% 
  {kts[.,on='K']}  %>% {.[intersect(ok,.$K)[1:4],on='K']} %>% setorder(K) %>% pkts(sl=l,clm = 2,bly=F)
```

A large trend that experiences a measurable decline for AJS is topic 12, *`r trm[12]`*. It appears to be the contextual words that describe book reviews. If this topic represents a measure of the count of book reviews, a decline may reflect a momentary drop in book publishing or a shift in attention away from books. AA, which also contains book reviews, does not experience a slow down and in fact remains on an upward trend, even seeing a boost during the war.

(ref:t12-h) `r sprintf('Topic %s, *%s*',12,trm[12])`

```{r t12-h,include=T}
sg(rel[[12]],tit='(ref:t12-h)',tabularx = T,new.col.align = 'lY')
```

Topic 17, *`r trm[17]`*, is a smaller AA series that experiences a nose dive during the war. It seems to refer to terms around conferences and meetings. If this is an indication of real conference activity rather than merely its advertisement, it is surprising to think that there would be a decline in professional association for anthropologists during this time. Sociology turns the other way, increasing its mentions of meetings.

(ref:t17-h) `r sprintf('Topic %s, *%s*',17,trm[17])`

```{r t17-h,include=T}
sg(rel[[17]],tit='(ref:t17-h)',tabularx = T,new.col.align = 'lY')
```

Topic 38, *`r trm[38]`*, is a sociology series that is more difficult to interpret but may be a social theory topic including American pragmatism. Its upward trend was arrested to the point of nearly flattening out, as if theoretical concerns were less satisfying given the practical demands of the war years.

(ref:t38-h) `r sprintf('Topic %s, *%s*',38,trm[38])`

```{r t38-h,include=T}
sg(rel[[38]],tit='(ref:t38-h)',tabularx = T,new.col.align = 'lY')
```

Topic 39, *`r trm[39]`*, represents a bigger topic that is actually mixed, and that exhibits a big downturn for sociology during the war but an upturn for anthropology. The concerns with magic likely dovetail with Durkheim's *The Elementary Forms of Religious Life* [@Durkheim1915Elementary] which was current at the time and a rare bridge between the two disciplines.

(ref:t39-h) `r sprintf('Topic %s, *%s*',39,trm[39])`

```{r t39-h,include=T}
sg(rel[[39]],tit='(ref:t39-h)',tabularx = T,new.col.align = 'lY')
```

***

```{r spdup,include=T,fig.cap='Cases that speed up before WWI'}
ok<-slp[,.(s=slpwr[which.max(slpwr)]),by=K][order(s,decreasing = T),K %>% as.character]
{l<-'pos';l} %>% {kts[,.(w=any(slg==.)),by=K][,K[w] %>% as.character]} %>% 
  {kts[.,on='K']}  %>% {.[intersect(ok,.$K)[1:4],on='K']} %>% setorder(K) %>% pkts(sl=l,clm = 2,bly=F)
```

Figure \@ref(fig:spdup) highlights four topics that experience a positive change, and indeed the differences before and after 1912 seem to be more dramatic than in the cases of slowing down. The sociology topic 13, *`r trm[13]`*, relates to theories of psychology and consciousness and may be a precursor to phenomenology.

(ref:t13-h) `r sprintf('Topic %s, *%s*',13,trm[13])`

```{r t13-h,include=T}
sg(rel[[13]],tit='(ref:t13-h)',tabularx = T,new.col.align = 'lY')
``` 

Topic 22, *`r trm[22]`*, is the largest topic in sociology and seems to coincide closely with WWI, though terms suggesting a concern with civil unrest may predate the concern with militarism. It may reflect a broader interest in social problems relevant to political democracy, including the problems of industrialization and nation building. Its trend was insignificant until 1910 at which point it started a precipitous rise that continued unabated through WWI.

(ref:t22-h) `r sprintf('Topic %s, *%s*',22,trm[22])`

```{r t22-h,include=T}
sg(rel[[22]],tit='(ref:t22-h)',tabularx = T,new.col.align = 'lY')
```

Topic 3, *`r trm[3]`*, is an anthropology trend with an interesting pattern. Though it swings upward during WWI, this is due in part to the decline of an earlier ascent in the sowing period. The topic relates to the ceremony and ritual of Hopi civilization of the American southwest, of which Tusayan and Zufii were two important sites. It is possible that this series represents two generations of scholarship into this work, as its popularity waned soon enough to wax again a decade after its original prominence.

(ref:t3-h) `r sprintf('Topic %s, *%s*',3,trm[3])`

```{r t3-h,include=T}
sg(rel[[3]],tit='(ref:t3-h)',tabularx = T,new.col.align = 'lY')
```

Topic 46, *`r trm[46]`*, is large and resembles *`r trm[22]`* in its rapid growth before and during the war. Not merely limited to education, the topic appears to refer to rural sociology, which was organized as a semi-autonomous profession specializing in the social and economic challenges of small towns, and which was often a counterpoint to the emphasis on the study of cities represented by the epicenter of sociology at the University of Chicago, where AJS itself was published.

(ref:t46-h) `r sprintf('Topic %s, *%s*',46,trm[46])`

```{r t46-h,include=T}
sg(rel[[46]],tit='(ref:t46-h)',tabularx = T,new.col.align = 'lY')
```

<!--
```{r stay,include=F,eval=F,fig.cap='Cases that remain constant during WWI'}
ok<-slp[,.(s=slpwr[which.max(slpwr)]),by=K][order(s,decreasing = T),K %>% as.character]
{l<-'zer';l} %>% {kts[,.(w=any(slg==.)),by=K][,K[w] %>% as.character]} %>% 
  {kts[.,on='K']}  %>% {.[intersect(ok,.$K)[1:4],on='K']} %>% setorder(K) %>% pkts(sl=l,clm = 2,bly=F)
```
-->

The above descriptions of topics as vocabularies, as cursory as they may be, represent only a slice of the total diversity of topics predicted by the topic model. The reader is invited to explore the rest on her own by following the link to the interactive exhibit. The term explorer allows you to vary the lambda ratio between frequency and exclusivity to see how the status of particular words changes. It also shows how the topics relate to each other in a two dimensions space of term similarity, which succeeds at reproducing an axis between sociology and anthropology, albeit with other unexplored differences. Perhaps its most useful feature is the ability to easily observe how the frequency of individual words varies across topics, helping to highlight interesting cases of polysemy.

(ref:soc-mod-viz) Topic Term Explorer, $K$ = 50. [*Interactive pop-out.*](`r if(latex|docx) 'https://brooksambrose.github.io/portfolio/'`exh/viz/index.html){target="_blank"} 

```{r soc-mod-viz,include=T,fig.cap='(ref:soc-mod-viz)'}
#invisible(suppressWarnings(capture.output(lda2viz.f(ckm,out.dir = 'exh',launch = F))))
knitr::include_graphics('img/soc-mod-viz.png')
```

<!--
One final case to explore inolves the polysemy, or multiple meanings, of the word ethnology. As we have discussed ethnology was a synonym for anthropology in an earlier era, and anthropologies entrance into the modern university setting coincided in part with a casting away of the ethnology label.
-->
<!--
git the damn tables from chapter 3

#A
29 the most anthroy: mounds, mound builders, Adena culture, though perhaps archeology in general
9 minor; language linguistics Vienna inscriptions vocabulary lots of different languages but not native american
28 museums curation institutional stuff
42 art material culture
15 spanish colonial
14 small but increasing trend, north american Indian tribes place names, accounts for half ot he word "indian"
49 lodges rituals medicine witches; some correspondence with soc
31 fascinating, large, emerges at dawn of WWI; Totemism clans father brother (patriarchy?) kinship descent phratries exogamy
48 yikes phrenology or just physical anthropology? skulls, negrito!, brains frontal pigment jaws stature blonde apes homo 
37 language grammar dialects verbs nouns (other parts of speech)...stops during WWI? Money for interpreters?
1 minor; mexico copper iron bronze stone metal Aztec
47 plants animals water seeds plants boats oil seals food eat berries; secular decline, mostly anthro some mix with sociology
3 Hopi indians, smaller topic, big at turn of century then decline
24 fascinating huge rush then complete stop: etc, treats noting ibid year words Lond Globus, stuff around dates...? centennial celebrations or reflections on turn of the century? peaks just after 1905
17 interesting because professional communication, presidents, subscriptions, Franz Boas chair speakers
36 minor; chief sent war confederacy summon belt messenger friendship (diplomacy and war?)
39 
20
40
35

#M
41
6
5
32
26
11
8
23
2

#S
4
27
25
50
7
45
44
43
16
18
30
34
10
38
13
46
21
33
19
12
22

-->



## <!--E--> Discussion

The evidence of change points toward the beginning and end of our sample period, as well as the wild diversity of different topic trajectories, illustrate that the settled times in between were not boring in the least. The cultural currents running in, and to a much lesser extent between, anthropology and sociology shifted constantly in terms of the attention of the professions as measured by words on the published page. If we think of the dynamics of each discipline in a competitive frame of mind, then it was often the case that sociology was the stronger contestant. It more frequently exhibited strong secular growth in its own disciplinary vocabulary, whereas the best trends in anthropology were often those that succeeded in remaining stable over the decades.

An area for further exploration concerns the topics that were flashes in the pan, that were so short lived as to not warrant an analysis of trend. An interesting case of this is topic 24, which is packed with date terms, the names of the years themselves, and the terms surrounding them. It is located directly over the turn of the century and likely indicates a collective fascination with the transition into the new era, which we have associated with the founding of the American university system. Further research is required to understand what the historical significance, if any, of such fads and fashions may be [@Berger2009How; @Bikhchandani1992Theory; @Hirsch1972Processing].

## A census of social science scholarship

In this study we have constructed a new way of looking at intellectual history that makes a place for the study of culture beside the traditional work horses of economic and demographic time series data [@Bail2014cultural]. In an eminently sociological fashion we have rendered the domain of the humanist, text and the ideas enshrined in it, more legible to the quantitative historian. It is worth reflecting on the limits of this approach. While it is tempting to refer to a measurement derived from the topic model as a time series of ideas, in fact it is much less than this. Topics are vocabularies, not ideas. They may be the material with which ideas are codified and communicated, but they do not and can not discover the messages sent. Only the humanist's close reading of texts can get at ideas, and the hermeneutic historian remains the leader among scholars who can claim to know now what historical actors once knew.

What we have provided is nonetheless relevant to the hermeneutic historian. While vocabularies may not measure ideas, they are in fact very good indicators of the boundaries of what Wuthnow [-@Wuthnow1989Communities] has called communities of discourse. Because scholarly disciplines are the institutional foundations for the emergence, growth, and decline of communities of discourse within the professions they encapsulate, the ability to identify them historically and at scale is a powerful tool both for establishing hermeneutic context and for analyzing the fates of scholars. A time series of topics set alongside economic and political indicators helps to acknowledge the complicated linkages in which scholars think.
