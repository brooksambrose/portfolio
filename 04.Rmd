# `r tvoc` {#voc}

#### Abstract {-}

(ref:abs-voc)

#### Keywords {-}

(ref:key-voc)

***

## Knowledge Development {#kd}

What were the ideas that predominated in the social sciences at their formation as professions in the postbellum United States? What was the course of their development over a generation of scholarship?  In this study I will answer these questions inductively through a reading of the original journals in each discipline. Though the goal is substantive, the methodological challenges of consuming a large quantity of text will feature importantly in the story that unfolds. Along the way I will demonstrate the usefulness of the computational *distant reading* that is being explored in the humanities and how it can be combined with traditional textual analysis for social science purposes. While controversial in humanistic circles that emphasize the primacy of the reader’s novel interpretive work when consuming text, distant reading fits comfortably within a social science epistemology that aims to achieve an objective description of intellectual history Indeed, computational methods offer a useful backstop to the idiosyncrasy of a particular person’s reading of history.

Computational textual analysis promises to automate a particular slice of what hermeneutic methods accomplish. Hermeneutics claims that through historical methods it is possible to reconstruct the interpretive context of texts such that they can be understood in the same way that contemporary historical actors understood them. Establishing such context is a laudable yet arduous feat of historical research to uncover the social and intellectual milieu of a particular text. This is the gold standard approach, but one that restricts the field to specialists with the training and resources necessary for the undertaking.

Computers cannot study history in this way. What they can do, however, is mine source material for limited kinds of contexts. The kind I am concerned with below are the *historical vocabularies* that writers used to construct texts in historical time. Vocabularies are glyphs without grammar; they do not mean anything, but nothing meaningful can be said without them in the present or in the past. They are the mediated form of language, and in communicating with each other historical actors leave traces that survive perfectly in time so long as texts themselves survive. 

While computers cannot read meaning in texts, and can barely recognize it, they are almost as good as humans at recognizing the glyphs of texts, and vocabularies are nothing but glyphs. What computers lack in smarts, they make up in speed and memory. The quantitative scale of their recognition makes for a qualitative shift because vocabularies can be enumerated across immense corpora of texts. Immense, at least, by human standards as there are limits to even computer memory and speed. Yet such enumeration of texts into objective historical categories; this is a profound resource for the intellectual historian. That one could begin a reading with such context would be a transformative research tool. Vocabulary enumeration, by which I mean simply the counting and classifying of texts according to the vocabularies they contain, invites a population studies approach to intellectual history. Where sense-making is driven by comparisons, a reader’s arbitrary combination of texts is guaranteed to lead to anachronism. But if we can know that texts are relevant to each other without knowing why, we have done some small amount of hermeneutic work by supplying texts as historically correct context to each other.

And even going so far as abandoning the project of reading texts in a historically correct way, vocabulary enumeration can still lend objectivity to a novel construction, a productive anachronism, of textual meaning. Because vocabularies, the problems solved by computers, are mathematically, algorithmically, or stochastically determined, they may provide an immutable description of corpora that, like a map, enables individual and collective exploration within a common framework. Such maps may become the parameters of interpretive methods, which we may use to surface and control some of our subjectivity.

This at least is the rationale for what follows. I begin with a discussion of intellectual history of two social sciences, anthropology and sociology, in the United States. I take a coarse view of national history as the history of wars because of their downstream effects on government activity and institutional investments. The first period is between the end of the American Revolution (1783) and the end of the American Civil War (1865) and is the national context for the origin of U.S. anthropology. The second period is after the Civil War until the end of World War I (1918) and is the context for the origin of U.S. sociology and of modern U.S. higher education generally. Wars of territorial expansion are waged regularly during both periods against native peoples and rival colonial empires, and social research was always recruited to solve attendant problems of population and to provide rationales for the relationships with and understandings of conquered or would-be conquered people.

I use intellectual histories of anthropology to characterize the antebellum period, and the same for the postbellum period including sociology. The most important journals in each field date from the postbellum period, and the appearance of each is implicated in the project of professionalization for each discipline. The 1920s marked the end of war with the last of the militating American Indian tribes, and a reckoning with the darkest sides of industrialization laid bare by WWI. Social research had by this time completed a shift from colonial to industrial problems and enjoyed a golden decade of development as a profession, punctuated by the next great historical crisis in the Great Depression. With the 1920s begins the adolescence of social research, which is beyond the present scope. This study is of its childhood, which ends with the Great War. I however draw the study out until 1922 because it is the end of the public domain in U.S. copyright, to aid in the reproducibility of the analysis and so that all readers may recover the texts in question without difficulty.

## Social Science Journals {#kd-dq2}

The journals within social science cover `r jclu$tab %>% length %>% nn` different subdisciplines.

```{r jclu-tab-sub,include=T}
jclu_sub<-jstorm[sapply(discipline,function(x) 'Social Sciences'%in%x)][,discipline:=lapply(discipline,setdiff,'Social Sciences')] %>% jstorm2jclu.f
setnames(jclu_sub$tab,'super','Subdiscipline')
sg(jclu_sub$tab,tit = "JSTOR Social Sciences Journal Counts")
```

```{r jclu-tab-sub-econ,eval=F,include=T}
jclu_sub_econ<-jstorm[sapply(discipline,function(x) 'Business & Economics'%in%x)][,discipline:=lapply(discipline,setdiff,'Business & Economics')] %>% jstorm2jclu.f
jsube<-jclu_sub_econ$tab
setnames(jsube,'super','Subdiscipline')
sg(jsube,tit = "JSTOR Business & Economics Journal Counts")
```

```{r jstorm2tab}
jtab<-jstorm2tab.f(jstorm,beg.bef = 1900,end.aft = 1900)
sg(jtab[,.(
  `Title History`=publication_title
  ,Discipline=discipline
  ,Start=start %>% as.character
  ,Stop=stop %>% as.character
)],tit='20th Century Social Science Journals in JSTOR',new.col.align = 'p{0.4\\\\linewidth}p{0.3\\\\linewidth}rr'
#,rplc = ec('\\{tabular\\},\\{longtable\\}')
)
```

```{r jstorm2fig,include=T,fig.cap='Periods in the Growth of the Number of Social Science Journals in the JSTOR Archive'}
f<-'d/q/jfig.RData'
if(file.exists(f)){
  load(f)
} else {
  jfig<-jstorm2fig.f(
    jstorm,jclu,series='Social Sciences',
    ann=rbindlist(list(
      data.table(text='Sample',xmin=1888,xmax=1922,dodge=.01)
      ,data.table(text='Civil War',xmin=1861,xmax=1865,dodge=.99)
      ,data.table(text='WWI',xmin=1914,xmax=1918,dodge=.99)
      ,data.table(text='WWII',xmin=1939,xmax=1945,dodge=.99)
    ))[,col:=c('darkgray',rep('red',3))]
  )
  save(jfig,file=f)
}
rm(f)
plt(jfig$p,tooltip=ec('text'))
```


```{r nces2phd,include=T,fig.cap='Decennial growth in number of PhD degrees conferred in the U.S.'}
f<-'d/q/nces.RData'
if(file.exists(f)){
  load(f)
} else {
  nces<-nces2phd.f()
  save(nces,file=f)
}
rm(f)
plt(nces$fig)
```


```{r nces/jstorm,include=T,fig.cap='Number of PhDs conferred in the United States per Social Science Journal'}
rat<-jfig$d[between(year,1800,2000)&super=='Social Sciences'] %>% setkey(year)
setkey(nces$int,year)
rat<-merge(rat,nces$int)
rat<-rat[,.(year,ratio=N.y/N.x,g1)]
r<-rat[,range(ratio)]
n<-diff(r)*.1

plt(myth(
  ggplot(data=rat,mapping = aes(x=year,y=ratio)) +
    ggplot2::annotate('rect',xmin=1888,xmax=1922,ymin=-Inf,ymax=Inf,alpha=.1) +
    geom_line(aes(color=g1),size=1.5) +
    geom_point(data=rat[.(nces$tab$year)] %>% na.omit,mapping = aes(x=year,y=ratio),shape=21,size=1.5,stroke=1,fill='white')
) + theme(legend.position="none",axis.title.x = element_blank()))
wrk<-rat[between(year,1888,1922),summary(ratio)]
```

This period represents one of stable growth, as the size of the field grows with the number of players on it. Between 1888 and 1922 there tended to be about `r nn(wrk['Median'],0)` new PhD's in the U.S. for every social science journal even as each population grew year over year. These growth patterns begin to diverge around `r jfig$d[super=='Social Sciences'][g1==3][1,year]` as a decades long acceleration of personnel begins, relatively slowly between 1920 and 1960 at an average acceleration rate of `r rat[between(year,1920,1960,F),nn(ratio %>% mean)]` PhDs per journal per year, and then quite precipitously in the 1960s at an average acceleration rate of `r rat[between(year,1960,1980,T),nn(ratio %>% mean)]`.


## Topics `r if(latex) '\\normalfont{≟}' else '≟'` Ideas

The strategy of the study occurs in four steps.

1. Sort text into categories of similar vocabulary.
2. Describe the vocabularies that define category membership.
3. Describe vocabulary prevalence across time and discipline.
4. Validate category contents by a traditional qualitative reading of texts.

I will spend considerable effort on solving the problem presented by step 1, as here everything depends on the computational methods employed. Steps 2 and 3 are straightforward given a successful mathematical model of texts. Step 4 is seldom attempted, and may be the hardest of all, because it is here that machine and human learning must be integrated. If I am successful, if through these steps I may operationalize the notion of cultural meaning or cultural logic as conformity to vocabularies, then I believe a new horizon of intellectual scholarship is possible. If on the other hand I find that machine-learned vocabularies do not correspond to human-learned understandings of the texts drawing on those vocabularies, then the discovery will be negative, that distant reading is not a scientific, historical, or hermeneutic method, but rather a toy at worst and a best new humanistic method of reading texts de novo.

The mathematical tool I will rely on in step 1 is called topic modeling, which refers to a variety of computational approaches to text data that blur the distinction between qualitative and quantitative analysis. The topic model paints a lexicographic picture of texts, analogous to the demographic picture gained by a civil census survey of cities and towns. To a topic model, texts are merely collections of terms (usually words) that are counted to create the so-called "bag of words" description of a text. In the same way that a census reduces communities to counts of the names of people who live in them, topic modeling reduces texts to the frequency of word choices in texts, to their diction or vocabulary. Just as a census of people fails to capture the nuanced interactivity of human settlements found in their culture, politics, and economic activity, the topic model washes away the meanings and intentions behind the words that are enumerated.

A population census would not be very helpful were it only a count of the names of respondents, and of course the really helpful data derive from the demographic and economic survey attached to the name. Text data do not usually come with such a collection of rich covariates, yet nevertheless topic models promise to discern helpful patterns from counts alone. The trick behind the estimation of a topic model is that it attempts to learn the demographic information (topics) without asking, by merely looking at how the names alone (terms) are distributed across geographies of interest (texts). If it can keep its promise, a topic model applied to census data might recover the cultural patterns latent in the distribution of names. It might, for instance, learn different groupings of names that in turn correspond to markers like age, race, national origin, or gender, so long as membership in those categories was related to geography. It might, for instance, successfully separate a category of Hmong names out from among the names of all people living in St. Paul because the non-Hmong names appeared in other regions where no Hmong names appeared.

To call the category of names "Hmong" requires an interpretation of the model, which by itself is just lists of names. This is the work of step 2, and requires a little bit of shoe leather by trying to make sense of what a list of names refers to.  Here reading texts is like a census taker knocking on a door, and a topic model's latent analysis saves on this effort. Sometimes bringing domain knowledge to bear on the list itself will suggest a category label, but often choosing a small sample of texts as exemplars of the category. Still this requires much less shoe leather than a traditional qualitative analysis in which each text is studied directly. Of course the census is much more informative because it asks about demographic categories directly thereby avoiding the need for a latent analysis. In domains where rich covariates are not yet available or are prohibitively expensive to acquire, latent analysis provides promising clues of patterns that already exist. What is even more interesting, and something that might surprise even census analysts, is when latent categories do not correspond to known survey items. In either event the power of topic modeling for inductive analysis is to reveal structure in how names hang together that was hidden.

Even without conducting the second labeling step, in step 3 it will already be possible from the output of the model to inspect the distribution of topics across available covariates, especially time. These are the patterns that will help validate the topic models against what is already known about intellectual history. For instance, the power of institutional and generational change may well be apparent in the historical distribution of topics. This step leads naturally into step 4 by suggesting anomalies that can only be explained by a closer look at the texts, the chore that the entire preceding analysis punts on. In step 4 we learn either that our understanding of history was wrong, or that our topic model was wrong, and there may be no method other than one's judgement to decide.

<!-- Operationally, what I have called vocabularies are the columns of a term-topic table, which is one of the two matrices that constitute a topic model. To explain the contents of the bag-of-word, topics are proposed. Topics can be thought of as catelogs out of which words are ordered and placed into the shopping cart that is the document. Different catelogs, different word availabilities, will produce different documents. The final bit of inference that makes topic models so practically useful is the idea that documents may be composed of multiple topics. The surprising qualities of texts are explained to be how authors draw on regular and commonplace topics to say something different. -->

In the next section, before we delve into the statistical and computational nuances of topic models, I will spend some time developing a few themes to help organize the blending of quantitative and qualitative methods invited by topic modeling in particular and computational text analysis generally.

## Prior Work {#kd-lit}


## Information

Understanding differences in the ontological status of the "topic" concept is a good way to begin to understand how this method of analysis is used by researchers.

Analysts have conceptualized the use of topic models in very different ways. Some researchers treat topics as useful for a particular purpose and not as true descriptions of real phenomena. Topics as information enhances the ability to search for relevant documents or statistical trends in otherwise unwieldy corpora as a time-saving alternative to manually reading large collections. [@Boyd-Graber2017Applications] Empirical problems, used as demonstrations of statistical techniques, have included

This is the "needle and haystack" approach favored by computer and information scientists who tend not to be interested in theoretical interpretations beyond the statistical definitions of topics.

## Meaning

Other researchers instead grant topics ontological status, and these can be divided into three types. Most ambitiously, topics may be treated as representing categories of thought. Latent semantic structure
latent semantic structure [@WallachStatisticalTopicModels2011]

## Communication

representational style [@Grimmer2016Measuring]
frame [@DiMaggio2013Exploiting]



## Full-Text

Computational text analysis requires that text corpora be transformed from a human to a machine readable format. Several efforts to digitize paper archives have made historical research designs possible, notably the Google Books project, HathiTrust, and ITHAKA JSTOR archive. Digital storage devices like the portable document format (PDF) have also enabled texts to be represented in both a digital version and as a reasonable facsimile of paper originals. Reasonable, we should say, for most sociological purposes, put not for other historical questions where materiality of culture is important. [@Schreibman2014NonConsumptive\:149]

Digital archives make research into the production of culture difficult, precisely because they misrepresent several aspects of the means of production. Because researchers should be mindful that digitization of texts abstracts some qualities of texts and renders many others invisible. The importance of physical space and material qualities of libraries is illegible when working with digital archives, while the verbal content of texts is highlighted. We must keep in mind that we are not viewing what historical actors saw. Digital texts are almost perfectly fungible, while, variability in historical texts. We are liable, for instance, to underestimate the search costs to locate texts, and the fungibility of texts themselves.

There are reasons, however, to believe that digital text archives provide not just a useful but an historically valid abstraction from the material texts. If we want to understand how an individual scholar understood a particular text, better to have her personal copy, margin notes and all. Yet how would that scholar have treated the text as a cultural item? She would abstract her own copy to a format credibly held in common, the more antiseptically clean version that we see in digital archives. These are the ghosts of the texts, so to speak, but they are what would be left when all idiosyncrasies were removed, the version that one would assume colleagues thought of when declaring that text publicly.

<!-- This comment on evidentiary foundations introduces the theme of this paper, which is the difference between personal and social meanings of texts, and more generally how tendencies toward idiosyncracy attenuate those toward conformity, and vice versa. -->

This is by way of saying that the texts I compile below are not the same that were read by the historical actors under consideration. They are the texts that historical actors would assume their contemporaries were reading, that is, the sanitized, fungible, original published form of the text. By getting at these texts, we are getting at the real historical infrastructure for scholarly communication.

The optical character recognition that computers require in order to store text digitally depends critically on the hard work of creating quality scans of journal archives. JSTOR has done a commendable job of this. Next we will describe what the JSTOR archive has to offer.


## Data {#kd-dd}

```{r zot2bib}
f<-'d/q/zot2bib.RData'
if(file.exists(f)){
  load(f)
} else{
  zot2bib<-zot2bib.f(
    users='2730456'
    ,collections = fread('d/q/zotcollections.txt')$URL
    ,key = 'qMxyytAhp07W9jNOGssIQasg'
  )
  save(zot2bib,file=f)
}
rm(f)
d<-lapply(zot2bib,function(x) x[,.(publicationTitle,date,volume,issue,pages,bp,ep,title,creators,dateModified,url)]) %>% rbindlist %>% `[`(!duplicated(.[,!9]))
setorder(d,publicationTitle,dateModified)
##### find corrupt
d[is.na(publicationTitle),]
d<-d[!is.na(publicationTitle),]
```

Every record for every journal was downloaded manually, including front and back matter, articles, and book reviews.

```{r z2b-dups}
# find duplicates
d[duplicated(d[,!9:10]),.(dateModified,pub=publicationTitle,tit=substr(title,1,50),date,volume,issue,pages)][,cat(date,' ',volume,' ',issue,' ',tit,' ',pages,'\n',sep=''),by=dateModified]
```

```{r z2p-pgap}
# find page gaps
p<-d[!(duplicated(d[,!9:10])|is.na(bp)),.(
  t=publicationTitle
  ,d=date
  ,v=volume
  ,i=issue
  ,p=mapply(function(b,e) b:e,b=bp,e=ep)
),by=.I]
p[,.(gap=setdiff(
  p %>% unlist %>% range %>% as.list %>% do.call(seq,.)
  ,p %>% unlist %>% unique %>% sort
)),by=.(t,d,v,i)][,.(gap=list(gap)),by=.(t,d,v,i)]
```

```{r z2b-vgap}
# find volume gaps
p[,table(t,i)]
cat('',sep='\n')
vg<-p[,table(d,factor(i,levels=1:6),t)]
vg[vg==0]<-NA
h<-hist(vg)
h<-which(vg<=h$breaks[2],arr.ind = T)
check<-cbind(h[,-1],vg[h])[order(h[,3],h[,1]),]
colnames(check)<-c('i','j','n')
cat('\nDouble check these volumes:\n\n')
check
```

```{r z2b-short}
# find short runs
setorder(d,publicationTitle,date,volume,issue,bp)
lt<-d[!is.na(bp+ep),.(title=title[.N],pages=pages[.N],dateModified=dateModified[.N],url=url[.N]),by=.(publicationTitle,date,volume,issue)]
ltn<-lt[,.N,by=title][N<3,title]
setkey(lt,title)
# will cause autoban, but this will test automatically
if(F) w<-lt[ltn,list({
  Sys.sleep(rnorm(1) %>% abs %>% `+`(1.5))
  cat('.')
  try(url %>% read_html %>% html_text %>% grepl('Next Item',.))
}),by=url]
lt[ltn][,cat(publicationTitle,' ',date,' ',volume,' ',issue,' ',title,' ',pages,'\n',url,'\n\n',sep=''),by=dateModified] %>% invisible
```

```{r jpdf2imp}
f<-'d/d/jpdf.RData'
if(file.exists(f)){
  load(f)
} else{
  # seed<-.Random.seed %>% sample(1)
  # cat('Seed: ',seed,'\n',sep='')
  # set.seed(seed)
  jpdf<-jpdf2imp.f(
    dir('d/d',full.names = T,recursive = T,pattern = '\\.pdf$') # %>% sample(100) # readLines('test.txt')
    ,ocr = T) # write feedback for long imports
  save(jpdf,file=f)
}
rm(f)
if(!'filt'%in%ls()) filt<-list()
filt$import<-cor2aud.f(jpdf$imp,'imported')
```

```{r imp2cln}
f<-'d/d/clnp.RData'
if(file.exists(f)){
  load(f)
} else {
  # seed<-.Random.seed %>% sample(1)
  # cat('Seed: ',seed,'\n',sep='')
  clnp<-imp2cln.f(
    imp = jpdf$imp
    ,hand = 'd/d/cln1537906742.txt'
    # ,seed=seed    
  )
  save(clnp,file=f)
}
rm(f)
if(!'filt'%in%ls()) filt<-list()
filt$clean<-cor2aud.f({setkey(jpdf$imp,doc,par);jpdf$imp[clnp$prd[(mth),.(doc,par)]]},'cleaned')
```

## Sampling {#kd-dp1}

```{r imp2ftx,eval=F}
f<-'d/p/ftx.RData'
if(file.exists(f)){
  load(f)
} else {
  # seed<-.Random.seed %>% sample(1)
  # cat('Seed: ',seed,'\n',sep='')
  # set.seed(seed)
  # s<-sample(jpdf$met[,doc],10)
  # ftx<-imp2ftx.f(jpdf$imp[s])
  ftx<-imp2ftx.f({setkey(jpdf$imp,doc,par);jpdf$imp[clnp$prd[(mth),.(doc,par)]]})
  save(ftx,file = f)
}
if(!'filt'%in%ls()) filt<-list()
filt$token<-cor2aud.f(ftx,'tokenized')
rm(f,jpdf,clnp)
```

```{r ftx2pre}
f<-'d/p/pre.RData'
if(file.exists(f)){
  load(f)
} else {
  pre<-ftx2pre.f(ftx,frq = 5,nchr=3)
  save(pre,file = f)
}
rm(f,ftx)
if(!'filt'%in%ls()) filt<-list()
filt$pre<-cor2aud.f(pre[!is.na(stm)],'preprocessed')
```

```{r pre2sam}
f<-'d/p/sam.RData'
if(file.exists(f)){
  load(f)
} else {
  seed<-.Random.seed %>% sample(1)
  cat('Seed: ',seed,'\n',sep='')
  set.seed(seed)
  sam<-pre[sample(doc[!is.na(stm)] %>% unique,1e2)]
  sam[,stm:=droplevels(stm,exclude=unique(sam[,.(stm,doc)])[,.N,by=stm][N==1,stm %>% as.character]) %>% droplevels %>% droplevels(exclude=NA)]
  attr(sam,'seed')<-seed
  save(sam,file=f)
}
rm(f)
if(!'filt'%in%ls()) filt<-list()
filt$sam<-cor2aud.f(sam[!is.na(stm)] %>% setkey(doc),'sampled')
```

```{r filt,include=T}
f<-'d/b/filt.RData'
if(file.exists(f)){
  load(f)
} else {
  filt<-rbindlist(filt)
  filt<-rbindlist(list(
  lapply(filt,function(x) if(is.integer(x)) round(x/max(na.omit(x))*100,2) else x) %>% do.call(data.table,.)
  ,lapply(filt,function(x) if(is.numeric(x)) max(na.omit(x)) else '100%') %>% do.call(data.table,.)))
  save(filt,file=f)
}
rm(f)
sg(filt,tit='Filtering due to Data Management')
```

## Units of Analysis

Conventionally researchers feed entire documents into the construction of term frequencies. This method treats any term in a document as being related to any other term by the same degree. The goal of any topic mixture model algorithm is to sift these terms into different topic categories basically by looking for clues across documents; a topic can be "seen" in a particular document to the extent that other documents include that topic and *other* topics different from the focal article, so that the intersection of terms reveals the topic. But a much simpler assumption to reduce the attendant noise within a document is to merely feed lower level syntactic structures--paragraphs and sentences--to the algorithm. We will see that doing so greatly improves the usefulness of discovered topics.

The irony of this approach is that while topics become more clear as documents become shorter, the assignment of any particular shorter document to a topic is murkier due to the smaller word count. 

Long documents will contribute more text to the corpus, but this is fair as they make up more of the population of text. Thus a simple random sample will allow better descriptive statistics. I sampled at the paragraph level because.

```{r pre2mlc}
f<-'d/p/mlc.RData'
if(file.exists(f)){
  load(f)
} else {
  mlc<-pre2mlc.f(sam)
  save(mlc,file = f)
}
rm(f)
```

```{r pre2des}
# des.com<-pre2des.f(pre)
# des.stm<-pre2des.f(pre[!is.na(stm)])
#kab(list(des.com,des.stm,data.table(tot=round(des.stm$cnt/des.com$cnt*100,2))),caption = 'Full Text Descriptives')
```

## Topics {#kd-dp2}

The modeling objective is twofold, to sort text into categories of similarity, and to describe the qualitative content that defines the category membership. In this way we may operationalize the notion of cultural meaning or cultural logic as the rules of category classification. reduce expressions as instances of a latent category of expression.

### How many topics?

```{r mlc2kmc,eval=F}
```

```{r pre2tel}
f<-'d/p/tel.RData'
if(file.exists(f)){
  load(f)
} else {
  tel<-pre2tel.f(pre[!is.na(stm)],stop = 3)
  save(tel,file = f)
}
rm(pre)
rm(f)
```

```{r tamK}
f<-'d/p/clu.RData'
if(file.exists(f)){
  load(f)
} else {
  tel<-tel[!s%in%tm::stopwords()] # remove stopwords
  tel<-tel[!r%in%tm::stopwords()]
  tel<-tel[s!=r] # remove remaining loops
  system.time(clu<-tel2clu.f(tel))
  save(clu,file=f)  
}
rm(f)
```

```{r ctr,eval=F}
ctr<-apply(clu$memberships,1,function(x) x %>% table %>% sort(decreasing = T))
View(ctr)
top<-ctr[[1]] %>% prop.table %>% `*`(100) %>% round(3) 
top %>% head(30)

cp<-tilit::ov2chpt.f(top %>% head(50),min.period = 2,drv = 1)
top %>% head(50) %>% as.vector %>%  plot(col=cp$g)

```

```{r crs,eval=F}
sm<-list()
for(i in 1:nrow(clu$memberships)) sm[[i]]<-factor(clu$memberships[i,],levels=order(table(clu$memberships[i,]),decreasing = T))
sm<-do.call(data.table,sm)
setnames(sm,paste0('h',ncol(sm):1))
sm[,cr:=clu$names]
sm[,m:=as.integer(h1)] %>% setkey(m)
sm %>% setkey(cr)

tel[,`:=`(ms=sm[s,h3],mr=sm[r,h3])]
tel[,bc:=ms==mr]
bct<-list(tel[s!=r,.(tew=sum(ew)),by=.(s,bc)] %>% setnames('s','cr')
,tel[s!=r,.(tew=sum(ew)),by=.(r,bc)] %>% setnames('r','cr')
) %>% rbindlist %>% .[,.(tew=sum(tew)),by=.(cr,bc)] %>% setkey(cr,bc)
bct[,m:=sm[cr,h3]]
bct<-dcast(bct,cr+m~bc,value.var = 'tew',fill=0)
bct[,tew:=`FALSE`+`TRUE`]
bct[,pit:=`FALSE`/tew] # proportion internal ties
bct[,`:=`(`FALSE`=NULL,`TRUE`=NULL)]
bct[,m:=as.integer(m)]
bct %>% setorder(m,-pit)
bct %>% setkey(m)
plt(
myth(
ggplot(bct[.(9)],aes(x=tew,y=pit)) + geom_text(aes(label=cr,angle=45),alpha=.5)
) + scale_x_log10())
```

```{r mlc2mlk,eval=F}
f<-'d/p/mlk.txt'
if(file.exists(f)) {mlk<-fread(f)} else {
  system.time(mlc2mlk.f(mlc,verb=T) %>% fwrite(f))
  pbapply::pbreplicate(999,mlc2mlk.f(mlc) %>% fwrite('d/p/mlk.txt',append = T),cl = 1)
}
rm(f)
mlk<-melt(mlk,measure.vars = names(mlk),variable.name = 'level',value.name = 'K') %>% setkey(level)
```

```{r mlk2sim}
f<-'d/b/sim.RData'
if(file.exists(f)) {load(f)} else {
  sim<-mlk2sim.f(mlk,rep = 1e3)
  save(sim,file=f)
}
rm(f)
```

```{r sim-fig,include=T,fig.cap='Distribution of K by convex hull'}
sim$fig1
```

```{r mlk2k,include=T}
f<-'d/b/k.RData'
if(file.exists(f)){
  load(f)
} else {
  k<-mlk[,.(k=pbapply::pbreplicate(1e5,psych::kurtosi(K %>% sample(replace = T)),cl = 1)),by=level][,.(e=mean(k),se=sd(k),l99=quantile(k,.005),u99=quantile(k,.995),`P(e ≦ 0)`=mean(k<=0)),by=level]
  save(k,file=f)
}
rm(f)
#debugonce(sg)
sg(k[,round(.SD,4),.SDcols=names(k)[-1],by=level],tit='Kurtosis Permutation Test')
```

```{r mlk-tab,include=T,fig.cap='Significant Counts of K'}
sim$fig2

#sg(sim$tab,'Significant Counts of K')
```

## Model selection

```{r mlc2stm}
# mod<-stmbow2lda.f(list(documents=mlc$par,vocab=mlc$voc),k=50,out.dir='d/p',verbose=F,visualize.results = T)
# mod$top.word.phi.beta[mod$top.word.phi.beta==0] <-.Machine$double.eps
# debugonce(lda2viz.f)
# viz<-lda2viz.f(mod,'d/b')
```



## `r tcit` {#cit}

#### Abstract {-}

(ref:abs-cit)

#### Keywords {-}

(ref:key-cit)

***

If knowledge is power then scholar must be a powerful class. But what kind of power is knowledge and in what way do scholars wield it? Is knowledge powerful a utility, like water or electricity, to drive a tool and accomplish a task? Is it an asymmetry of information, like a stock tip or the combination to a safe, that gives one a leg up on her competition? Is knowledge like the power of an authority, like a governor, a military commander, or clergyman, to compel the loyalty and obedience of another person? 

How we conceive of knowledge affects how we view the nature and importance of the people and institutions that produce it. Scholars certainly do not have a monopoly on the utilization of production of knowledge in society, but their occupational roles are conditioned by the stuff of knowledge at the same time that knowledge is itself conditioned by the technology and social arrangements that constitute scholarly occupations. 

### Scholarly Communication vs Knowledge Terrain

If the production of culture perspective were to argue against Marx's German Ideology, it might say, "Not all mental laborers have soft hands." Marx drew a course distinction between mental and material labor to demonstrate that the former is not possible without the latter, even when at the time mental labor had already been commodified with the advent of print media. The production of culture perspective simply effaces the distinction altogether; mental labor, or cultural products, are like any other industrialized commodity.

The production of culture perspective is at odds with public sector economics that argues that non market mechanisms create value where markets fail to do so. [@Hayes2000Assessing]

Remuneration 

Are culturally interior products <!-- artifacts --> referenced by socially superior <!-- artificers --> producers?

### Mapping Knowledge Terrain {#wok}

#### knowledge stuff

There are two reasons to map knowledge spaces. First, we may want to know how knowledge develops as a resource unto itself. Second, we may want to exploit such a map for a productive purpose. Here we will attempt the second as prologue to the first. We will tackle the technical problems of constructing a map. We will show how a map can be put to use. Finally, we will investigate how the particular map we make may tend to predictably get us lost.

All knowledge mapping requires first an ontological and then an analytical action. Ontological actions delineate the things that matter. They arbitrarily construct from perception the items that we then think about. While ontological decisions tend to define the scope of everything that may be learned from an investigation, they are often assumed rather than demonstrated. Actor Network Theory (ANT) provides a unique example of a method of research that, because it is ethnographic and thus marinating in an abundance of perception, allows the cast of ontic characters to grow. Literally anything can be deigned significant for inclusion in a web of knowledge. In an ANT study of science, if the feel of a reading chair modifies a reader's orientation to a text they are reading, the chair counts.

The lion's share of knowledge mapping studies are not so ontologically radical as ANT. Take the field of bibliometrics. The ontological decision here is to take documents as the primary ontic. Documents are nothing but collections of glyphs, so the first task of bibliometricians tends to be to map glyphs to terms and analyze them. Here we have already used the ontic triad underlying bibliometrics. In the sentence

> "Go, dog, go!"

there are twelve discrete glyphs and two terms. A grammatical cutting rule renders the glyph sequences as 

> "Go, " "dog, " "go!"

and a tokenization rule maps the cuts to two terms

> "go" "dog" "go"

 which may in turn be analyzed, for instance by counting the tokens. The documents form the bins within and across which the terms will be analyzed. The token, as a mere operational step, is used and then dispensed with unless questions of measurement surface. Clearly the *glyph-term-document* (GTD) ontic does not care about the armchair of a reader of a document, and indeed does not even care about the reader herself. <!-- An ANTy idea that is more directly adjecent to GTD is inquiry into the materiality of texts, a topic with a long history in the humanities and a much more recent history in digital analysis.  -->

So the reader is invisible because she is not inscribed in the document. What about the writer? Bibliometricians may back fill GTD by entity recognition or grounding. Once terms are recognized, we may further recognize that we know more about them. A simple example of this is pulling out "metadata", for instance, the author of a document. The author's name is not just any term, but a conceptually very important one. Grounding is how bibliometrics may be linked to theories and programs of greater importance.

Bibliometrics has indeed been based more on the reference of a text as a particular grounded entity rather than on the use of the full text of a document. If a text is a building, the reference is its address. More precise than a name, an address is a codification of different hierarchically ordered elements that describe the location of an entity. The consistent tokenization of a reference is not an easy task, as it depends on entity recognition of several different kinds of things, including year of publication, author, title, and source.

The citation became the basis of the concept of a web of knowledge as coined in the work of Eugene Garfield and institutionalized in the Institute for Scientific Information (ISI). 

Citations solved the problem that ideas do not have signatures or addresses that we can trace reliably. Jargon is an attempt to give an idea a unique address as an idiosyncratic term, and etymology seeks to hierarchically order words according to their origins, but an idea per se will always elude precise identification. Unlike a document, an idea is not mechanically reproducible; it always requires interpretation and understanding in a mind, and a mental event as subtle as an idea cannot be observed.

[@Lederberg2000How] Garfield conflates citations with several roles in the network around ideas. Compares value of citations to value of subject coders, coding meaning of paragraphs intractable. ISI became a commercial pursuit because Garfield failed to get scientific institutions, especially the NSF, to fund it. The goal was primarily practical, to give researches access to current or historical references relevant to articles, perhaps especially their own, they knew they were already interested in.

Unlike ideas, documents are physical artifacts and can be traced empirically. They are fungible, reproducible, and locatable with addresses.

The reproduction and location of ideas cannot be reliably observed, and documents only contain ideas in a metaphorical sense, as a Leyden jar was once thought to contain electricity.

Documents are the tangible and fungible currency with which scholars communicate about ideas, yet how knowledge is actually communicated via documents is not amenable to direct observation at scale. In bibliometrics they have served as a proxy for ideas.

There have been two main orientations to mapping the web of knowledge, description and conscription. Description has either scientific aims, to understand and explain the facts of knowledge development, or practical aims, to locate and retrieve knowledge required for a particular purpose. Conscription on the other hand aims to mobilize bibliometric patterns of knowledge as measures of value in competitive markets, namely hiring, promotion, and awards within scholarly professions.

There are several ways to digitally represent texts as knowledge.

From an empirical perspective, texts are nothing but collections or bins of glyphs.  The current paradigm is to render glyphs and recognize them as terms. Such terms may then be analyzed, for instance, by counting diction. Alternative paradigms are cropping up

Second is entity recognition or grounding, where recognized terms are mapped to an existing database of structured knowledge.  

[@Pilkington2009evolution]

### Disciplines as a Large World Co-reference Network

A large world network is not amenable to traditional visual representations due to its extreme density. Scholars often use edge filtering to reduce this density down to a manageable size for visualization. Unfortunately this convenience function renders a large world as a small world and grossly misrepresents the true structure of the network. In the KCC representation, the network is partitioned into subnetworks of differential density. Nodes are included in a subnetwork if they are involved in ties at a given floor of density, for instance, they need to be tied to at least five other nodes. At a level of five, then, nodes involved in only four ties would be excluded. As this standard is raised, more nodes are excluded. This results in a nested set of subnetworks, where nodes included in a community at a lower threshold are excluded at a higher threshold. Subnetworks of lower density thresholds are always as big or larger than those at higher thresholds. Moreover, higher density subnetworks are always subsets of lower density communities, as their density meets and exceeds the standard for inclusion at the lower level. As one can imagine, inclusive levels are larger. As the threshold is raised subgroupings are sloughed off until reaching points of maximal density. In a world where almost everything is connected, there are no structural holes to reveal differences between subnetworks. Instead, we can view the structure as gradations in density within a very densely connected world.

Nodes meeting the highest standards can be thought of as omnivorous; their ties draw them to the masses, but the masses are not sufficiently tied to the higher standard community. Where the gentry may be as comfortable at the movies as at the symphony, the laity lacks access to the more erudite circles.

What is the credential that would allow a node to climb the hierarchy? One's list of acquaintances must overlap by a certain amount (defined by the threshold) with the membership of the higher tier. Indeed their inclusion would change the credentials of everyone they are tied with, as anyone who was just under the standard would be tipped in based on their friend's promotion.

In the KCC model the references are the members of the hierarchy. Their association with each other is determined by how they are used by published authors. Authors who include two references on their bibliography tie those references together in the network. Indeed each citing article lays down a dense clique of references, and the impact of an article grows quadratically with the length of its reference list.


## Methods

## Data


```{r wok2dbl}
f<-'d/p/wok2dbl.RData'
if(file.exists(f)){
  load(f)
} else {
  wok2dbl<-wok2dbl.f(dir='d/d/wok0041','d/p')
}
rm(f)
```

```{r dbl2bel}
f<-'d/p/dbl2bel.RData'
if(file.exists(f)){
  load(f)
} else {
  load('d/d/wok/fuzzy-sets.RData')
  dbl2bel<-dbl2bel.f(wok2dbl,out = 'd/p',saved_recode = fuzzy.sets)
}
rm(f,fuzzy.sets)
```

```{r bel2mel}
f<-'d/p/bel2mel.RData'
if(file.exists(f)){
  load(f)
} else {
  bel2mel<-bel2mel.f(dbl2bel[!((pend)|(zpend)|(zdup)|(loop)|(zloop)),.(ut,cr=zcr)],out='d/p')
}
rm(f)
```

```{r mel2hcs}
f<-'d/p/mel2hcs.RData'
if(file.exists(f)){
  load(f)
} else {
  mel2hcs<-mel2hcs.f(bel2mel,wok2dbl)
  save(mel2hcs,file=f)
}
rm(f)
```

```{r mel2comps}
f<-'d/p/mel2comps.RData'
if(file.exists(f)){
  load(f)
} else {
  if(dir.exists('d/p/mel2comps')) system('rm -rf d/p/mel2comps')
  mel2comps<-mel2comps.f(bel2mel,out='d/p',min.ew = 1,min.size = 3)
  save(mel2comps,file=f)
}
rm(f)
```

```{r comps2cos}
f<-'d/p/comps2cos.RData'
if(file.exists(f)){
  load(f)
} else {
  comps2cos<-comps2cos.f('d/p/mel2comps')
  save(comps2cos,file=f)
}
rm(f)
```

```{r cos2kcc}
f<-'d/p/cos2kcc.RData'
if(file.exists(f)){
  load(f)
} else {
  cos2kcc<-cos2kcc.f('d/p/mel2comps',out = 'd/p',type='crel') 
  save(cos2kcc,file=f)
}
rm(f)
```

## Results

The structure of a large world as revealed by KCC can be explored in a bottom-up and top-down fashion. Bottom-up observes 3-clique communities first. In the social science co-reference network.

```{r kcc2tree, fig.scap='K-clique Community Structure. [*Interactive pop-out.*](exh/tree.html){target="_blank"}',eval=T,include=T, message=FALSE, warning=FALSE,screenshot.force = TRUE}
f<-'d/b/kcc2tree.RData'
if(file.exists(f)){
  load(f)
} else {
  #{load('d/p/cos2kcc.RData');load('d/p/bel2mel.RData');load('d/p/wok2dbl.RData')}
  system.time(kcc2tree<-kcc2tree.f(cos2kcc,bel2mel,wok2dbl,sides = 100,transform=T))
  save(kcc2tree,file=f)
htmlwidgets::saveWidget(kcc2tree$int,file = getwd() %>% paste('exh','tree.html',sep=.Platform$file.sep),background='gray',selfcontained = F)
}
rm(f)
kcc2tree$int
```



Figure \@ref(fig:kcc2tree) shows a KCC model of the social sciences in the first half of the twentieth century.

Disciplinarity and interdisciplinarity are revealed in a novel fashion in the KCC model. Disciplinarity is shown as a level of exclusion.

### Continents

The global map is made of many separate regions ranging in scale from large continents to small isles. These regions are either shallowly connected or entirely separated from each other. The vast majority of these regions are "flat isles" with little to no internal structure of their own. Most flat isles are supported by only a single article, some by a couple of articles penned by the same author, and only a few represent real activity among a small group of different authors.

```{r flat-isle,fig.cap='Flat Isles, where Reviewers tend their Flock',include=T}
include_graphics('img/flat-isle.png')
```


The most substantial flat isle <!-- k50c1-5 -->, the largest unenclosed and unenclosing circle in Figure \@ref(fig:flat-isle), comes from four authors publishing in the same 1930 [issue](https://www.jstor.org/stable/i40084238) of *`r j9so('Z NATIONALOKON')`*. It includes 50 references the most prominent of which are Angell's 1926 *The Theory of International Prices* and Tugwell's 1924 *The Trends of Economics*. The structure of the group is provided entirely by an article by Robert Reisch <!-- https://www.jstor.org/stable/41792307 -->; the other three shared no references in common and Reisch's article, titled "The 'Deposit'-Myth In Banking Theory" and containing 108 references, is likely to have been written as an introduction to the journal on the basis of what had already been accepted for publication. 

Another flat isle of four articles has the exact same pattern, also from *`r j9so('Z NATIONALOKON')`* but from an [issue](https://www.jstor.org/stable/i40084262) in 1937, the article on the first page of the issue, titled "Theory Of Capital, Introduction" <!-- k16c1-47 --> by von Hayek and containing 25 references, includes subsections of the bibliographies of three other articles that do not themselves overlap. Normally the longer a bibliography the more likely it is that an article functions as a review linking other disparate bibliographies. That von Hayek's article has such a short bibliography and yet still links three otherwise separate articles confirms its derivative character.

The following features then suggest when a flat isle represents an issue introduction. All articles are published in the same issue. The removal of the longest bibliography in the community yields a network of disconnected components each uniquely representing the remaining bibs. This longest bib is also either the first article in the issue or precedes the others in pagination. These characteristics suggest authorship internal to the editorial process itself. Later we will explore how the removal of such articles helps to reveal "bottom-up" structure by removing the editorial advantage of certain authors to bestow an ad hoc intellectual coherence on scholarship.

Table \@ref(tab:iss-int) enumerates issue introductions and shows how many are the structuring article in their community.

```{r iss-int}
# kcc level
# number issues repped
# if one, longest bib is first

kt<-wok2dbl[CJ(unlist(id) %>% unique,ec('PY,J9,AU,TI,VL,IS,SN,BP,EP,PG'),sorted = F)] %>% dcast(id~field,value.var='val',fun.aggregate=list,fill=NA)
#%>% kable(caption = 'Issue introductions') # too big!
#need to merge CR to KCC

# issue level
# number kccs repped
# if any 
```

```{r hill-isle,fig.cap='Hill Isles, where the Wild Things Are',include=T}
include_graphics('img/hill-isle.png')
```

Reviews, whether they are self described as such, borrow directly from the bibliographies of one or more seed articles, and in this way they contribute a disproportionate amount of the global structure of the co-reference network. This kind of review, rather than looking again at an existing intellectual trend, creates the cohesion it purports to describe. Flat isles, especially if they are large, are flat due to the retrospection of a usually solitary reviewer. Compare this to "hill isles" with more internal structure growing out of the related but uncoordinated reference activity of authors. 

```{r k3c1-22,fig.cap='Hill Isle in Graph Layout',include=T}
tree2igr.f(kcc2tree,mel2hcs,vpal='E',light=1,dark=100,transform=T,root=T,print=F,plot=T,ew=1,3,1,22)
#include_graphics('img/k3c1-22.png')
```

<!-- k3.k3c1-22 top cluster in screen grab--> 


### Peaks

The KCC model reveals 

### Valleys

### Do reference lists describe author knowledge?

The peer review process can now be thought of as a process of auditing credentials. An author makes an opening bid with the submission of a particular reference list. What this reference list implies about what the author knows is unclear. One may omit a knowledge signal because it is truthfully irrelevant or in a deceptive sin of omission oriented to what they expect to be the expectations of editors and reviewers. One may also include what they do not know out of error, braggadocio, carelessness, or fraud. Part of the work of reviewers will be to validate those claims to knowledge.


```{r kcc2isl,fig.scap='K-clique Community Island Plot. [*Interactive pop-out.*](exh/isl3d.html){target="_blank"}',include=T,eval=T,cache=T}
f<-'d/b/kcc2isl.RData'
if(file.exists(f)){
  load(f)
  kcc2isl<-kcc2isl.f(bel2mel,cos2kcc,wok2dbl,dbl2bel,co=kcc2isl,ordinal=F,minew=1)
} else {
  seed<-.Random.seed %>% sample(1)
  cat('Seed: ',seed,'\n',sep='')
  withr::with_seed(seed,kcc2isl<-kcc2isl.f(bel2mel,cos2kcc,wok2dbl,dbl2bel,ordinal=F,rad=1.8,area=1.6,nit=1e4,minew=1,res=200,hex = .01))
  attr(kcc2isl,'seed')<-seed
  save(kcc2isl,file=f)
}
rm(f)
```

```{r isl3d,message=FALSE, warning=FALSE, include=F,eval=F, fig.cap='Time to Explore.'}
f<-'d/b/isl3d.RData'
if(file.exists(f)){
  #load(f)
} else {
  isl3d<-isl2isl3d.f(kcc2isl,rad=1,res=200,ln=T,floor=T,close = T,tight = F,cont=F,zasp=.15,zoom=.75,pins=T,medcen = F,raise=1)
  save(isl3d,file=f)
  htmlwidgets::saveWidget(isl3d,file = getwd() %>% paste('exh','isl3d.html',sep=.Platform$file.sep),background='gray',selfcontained = F,title='isl3d')
}
rm(f)
#isl3d
```

