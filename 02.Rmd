# `r tkd` {#kd}

#### Abstract {-}

(ref:abs-kd)

#### Keywords {-}

sociology of knowledge, topic modeling, history of social science

## History of Ideas {#kd-problem}

What were the ideas that predominated in the social sciences at their formation as professions in the postbellum United States? What was the course of their development over a generation of scholarship? Though the intellectual history of the social sciences begins well before the American Civil War (1861-1865), the current epoch of its institutional history, the epoch of professions, becomes possible only in the postbellum period. Before the Civil War, social research was a skilled occupation, and individual researchers found patrons through government and civil institutions supporting pursuits of knowledge, such the American Philosophical Society (APS) founded in 1743 in Philadelphia. One of the great consequences of war was renewed federal investment in nation building in its aftermath. Some saw the rebellion as a failure of education, prompting the beginning of the U.S. Department of Education in 1867. Thus in the postbellum period both anthropology and sociology develop as professions due primarily to the growth of universities as a new context of their activity. Without universities, social research would have remained an occupation in need of clientele. With them, social research develops the resources enabling relative autonomy, self reproduction, and occupational closure, the hallmarks of a profession.

This study seeks to examine the period of transition after the Civil War from social research as occupation to social research as profession. While the roots of anthropology are as old as the republic, sociology did not develop in earnest until after the Civil War.

U.S. nation building had continued since the end of the American Revolution and had enrolled researchers in the projects of westward expansion against native peoples, the consolidation of slave economies against Africans, and the legitimation of the American experiment against European detractors. These were pressing problems to the intellectuals among government leaders at different levels, and they worked to make investments in new knowledge to resolve them. Such new knowledge was initially an extension of older "theories of man" in theology and enlightment natural philosophy, which had a foothold in the private education of the American so-called natural aristocracy (think Thomas Jefferson's library) as well as in urban colonial institutions like the APS serving as meeting places for intellectual elites and scholars.

An idea current after the revolution was that progress was located on a scale from barabrism to civilization, and that movement in the direction of progress was a function of time and innate capacity. European societies of the Old World had the best endowment of each, for they were very old (seen as continuous with pre-Christian antiquity) and of the highest capacity. Conversely colonized peoples were understood to suffer low endowments of both, developing slowly due to their lack of morality, reason, and aesthetics and having had less time to achieve what little progress they were capable of. Colonial theologens, philosophers, and learned elites had long provided the intellectual rationalizations of the domination of women and the environment, and such reasoning was readily exported to the yet unresolved problems of native tribes and African slaves.

American intellectuals knew the same logic could applied to Americans who if they could not match the rate of progress of European society would be judged a newly diverging and potentially inferior race of men. Though it did little to ameliorate white male supremacist thinking, the contradictions between the univeralistic and egalitarian values expressed in the U.S. Constitution did occassionally create grist for academic debate leading to some investments in research critical of extant racist typologies.  In Europe scholars of comparative languages charted the supposed commmon origins of European cultures and the time it had taken them to diverge, and this inspired Thomas Jefferson to patronize ethnological research on the languages of native tribes in America. He had by 1785 amassed enough data to appreciate their diversity, which he took as evidence that New World cultures may be older, and therefore more developed, than commonly assumed by colonizers. 

The egalitarian-for-its-time thinking of Jefferson did not contravene racism, but it did provide a different model of racial hierarchy, which could be consequential for government projects. Jefferson thought that if all men are equal, that they share the same human capacity for progress, then native people had merely not had the time to develop and discover progress for themselves. They could and should be taught, especially to abandon hunting in favor of agriculture. For a brief time in the 1840s the Virginia legislature used tax incentives and educational programs to promote intermarriage between male settlers and native women with the goal of accelerating the natural development of native culture. [@Patterson2001Social:9] Such experiments were short lived, as the predictable backlash was that such mixing would risk lowering white culture rather than elevating Indian, and genocidal and segregationist policies won out. Thus while all racial models concluded with the domination of natives by settlers, different models of racial hierarchy sewed policy disputes within colonial leadership.

The role of social research then was to be in service of either government or private political associations. Careers were made for anthropologists in patronage relationships to generate knowledge to aid nation builders. These patrons facilities, resources, and institutions. Institutional foundations for 

To discuss the epoch of professions 

We can consider two approaches to answering these questions.

The slow and steady historian would identify diverse documentary sources allowing her to collect the names of important people and organizations and learn what she could of their biographical facts and event timelines. She would then read the scholarship both produced and consumed by these important actors. The identification of ideas would be the most difficult task, not just because reading takes time and effort and there would be much of it, but because ideas exist in the minds of people, and thought leaves no direct empirical trace. The interpretation of writing in a historical context would suggest candidate ideas; their prevalence across time and place would indicate their historical importance. It would be tedious work requring intelligence and patience.

The hasty and impudent sociologist would identify a convenient source and ask a computer to do the rest. Lacking the historian's fortitude and patience, the sociologist uses a mental prosthetic to achieve and perhaps exceed the scale at which a human reader can cosume documentation and in a fraction of the time. Much less, however, is learned. Whereas people can learn about what they were not looking for, machines learn only what they are told to learn.

For shorthand, we can refer to these two approaches are Aesop's tortoise and hare, the classic humanist mode and the contemporary computational mode.

Berlin 

### Fox



### Hedgehog


### Plan of the Study

```{r fab-tab, include=T}
fab<-  data.table(` `=ec('\U1F422,\U1F407') ,'\U1F98A'=ec(',,'),'\U1F994'=ec(',,'))
fab<-sg(fab,tit="Fable Table",type = 'latex') 
if(latex) gsub('(\U1F422|\U1F407|\U1F994|\U1F98A)','\\\\normalfont{\\1}',fab) else fab
```

To save some face and avoid a totally impudent analysis, I will begin with an essay on computational text analysis to describe what a computer can actually do, and then discuss how this might be leveraged to pry at a more respectable history of ideas. Where we let the tail wag the dog, choosing cases for methodological convenience.

The historian of ideas can be studied at different levels that are each easy to theorize but not always easy to observe. 

In the history of ideas the social sciences are young players, and the U.S. social sciences are even younger. Though anthropology has generically referred to writings about . The oldest. Ph.D.s U.S. higher education, after all, is younger than the country. Actually American sociology had the first department in the world.

### Ontologies {#kd-concept}

Ontologies are categories of being, or more simply, they are the assumptions that answer the (usually implicit) question, "What is this?". Ontologies may be descriptive categories of classification or explanatory causal mechanisms. It is neither possible to describe nor explain without making onotological assumptions.

Understanding differences in the ontological status of the "topic" concept is a good way to begin to understand how this method of analysis is used by researchers.

Analysts have conceptualized the use of topic models in very different ways. Some researchers treat topics as useful for a particular purpose and not as true descriptions of real phenomena. Topics as information enhances the ability to search for relevant documents or statistical trends in otherwise unwieldy corpora as a time-saving alternative to manually reading large collections. [@Boyd-Graber2017Applications] Empirical problems, used as demonstrations of statistical techniques, have included

> One's own consciousness dances about upon the words like a will-o'-the-wisp.  
> --Niklas Luhmann [-@Luhmann2002Theories]


```{r dgr-ont,include=T,fig.cap='Ontological Approaches to Topics',screenshot.force = T}
# DiagrammeR::grViz('
#   graph {
#     graph [center=true]
#     node [shape=plaintext]
#     topic--useful
#     topic--true
#     true--thought
#     true--communication
#     true--artifact
#   }
#   ') 
DiagrammeR::grViz('
graph {
  node [style=plaintext]
  subgraph cluster_1 {
    label = "phone"
    1 [shape=plaintext,label="machine"]
    subgraph cluster_2 {
      style=filled
      fillcolor=gray
      label = "pheme"
      2 [shape=plaintext,label="machine\nhuman"]
      subgraph cluster_3 {
        style=filled
        fillcolor=white
        label = "rheme"
        3 [shape=plaintext,label="human"]
      }
    }
  }
}
')
```

This is the "needle and haystack" approach favored by computer and information scientists who tend not to be interested in theoretical intepretations beyond the statistical definitions of topics.

Other researchers instead grant topics ontological status, and these can be divided into three types. Most ambitiously, topics may be treated as representing categories of thought. Latent semantic structure
latent semantic structure [@WallachStatisticalTopicModels2011]
representational style [@Grimmer2016Measuring]
frame [@DiMaggio2013Exploiting]


Text as thought, or as communication. Thought holds that the ideas can be reliably interpreted, perhaps hermeneutically, to recover the mental events or intentions of authors and readers. Ambitious. Easier is to treat texts as communications, as messages, and worry little about their meanings or interpretations. A study of communication is a good foundation for the study of thought, but it is a separate task and the one we undertake here.

### Topics `r if(latex) '\\normalfont{≟}' else '≟'` Ideas

The strategy of the following analysis occurs in four steps.

1. Sort text into categories of similar vocabulary.
2. Describe the vocabularies that define category membership.
3. Describe vocabulary strengths across time and discipline.
4. Validate category contents by a traditional qualitative reading of texts.

Steps 1-3 constitute a normal approach to quantitative intellectual history, whereas step 4 is seldom attempted. If in this way we may operationalize the notion of cultural meaning or cultural logic as conformity to vocabularies, then a new horizon of intellectual scholarship is possible that would allow a so-called "distant reading" of texts. If on the other hand we find that machine learned vocabularies do not correspond to human learned understandings of the texts drawing on those vocabularies, then we may in fact discover that distant reading is not a scientific, historical, or hermenuetical method, but rather a new humanistic method of reading texts de novo.

A population study of ideas at a societal scale has never been conducted in the social sciences.

Topic modeling refers to a variety of approaches to the statistical modeling of texts that blurs the distinction between qualitative and quantitative analysis. Texts are merely collections of terms (usually words) that are counted, and such counts may describe a text. In the same way that a civil census reduces communities to counts of the people who live in them, topic modeling reduces texts to a count of words, to diction. And just as a census of people fails to capture the nuanced interactivity of human settlements found in their culture, politics, and economic activity, the meanings and intentions behind words are washed away.  At a very general level this term census paints a lexographic picture of texts, analogous to the demographic picture gained by a survey census of cities and towns. This approach to document description is sometimes called a "bag of words".

To explain the contents of the bag-of-word, topics are proposed. Topics can be thought of as catelogs out of which words are ordered and placed into the shopping cart that is the document. Different catelogs, different word availabilities, will produce different documents. The final bit of inference that makes topic models so practically useful is the idea that documents may be composed of multiple topics.

The surprising qualities of texts are explained to be how authors draw on regular and commonplace topics to say something different.

## Data {#kd-d}

Computational text analysis requires that text corpora be transformed from a human to a machine readable format. Several efforts to digitize paper archives have made historical research designs possible, notably the Google Books project, HathiTrust, and ITHAKA JSTOR archive. Digital storage devices like the portable document format (PDF) have also enabled texts to be represented in both a digital version and as a reasonable facsimile of paper originals. Reasonable, we should say, for most sociological purposes, put not for other historical questions where materiality of culture is important.

Digital archives make research into the production of culture difficult, precisely because they misrepresent several aspects of the means of production. Because researchers should be mindful that digitization of texts abstracts some qualities of texts and renders many others invisible. The importance of physical space and material qualities of libraries is illegible when working with digital archives, while the verbal content of texts is highlighted. We must keep in mind that we are not viewing what historical actors saw. Digital texts are almost perfectly fungible, while, variability in historical texts. We are liable, for instance, to underestimate the search costs to locate texts, and the fungibility of texts themselves.

There are reasons, however, to believe that digital text archives provide not just a useful but a historically valid abstraction from the material texts. If we want to understand how an individual scholar understood a particular text, better to have her personal copy, margin notes and all. Yet understanding how that scholar treated the text as a cultural item, she would abstract her own copy to a format credibly held in common, the more aniseptically clean version that we see in digital archives. These are the ghosts of the texts, so to speak, but they are what would be left when all idiosyncracies were removed, the version that one would assume colleagues thought of when declaring that text publically.

This comment on evidentiary foundations introduces the theme of this paper, which is the difference between personal and social meanings of texts, and more generally how tendencies toward idiosyncracy attenuate those toward conformity, and vice versa.

making database decisions about how texts should be represented for statistical manipulation. 

### Social science journals {#kd-dq}

```{r master2jstorm}
f<-'d/q/jstorm.RData'
if(file.exists(f)) {
  load(f)
} else {
  jstorm<-master2jstorm.f()
  save(jstorm,file=f)
}
rm(f)
```

```{r jstorm2jclu}
f<-'d/q/jclu.RData'
if(file.exists(f)){
  load(f)
} else {
  jclu<-jstorm2jclu.f(jstorm)
  save(jclu,file=f)
}
rm(f)
```

We rely on the JSTOR digital archive which gives access to optical scans of historical journals. The coverage of journals in the archive is very complete for those journals chosen for the database. As of this writing JSTOR contained `r jstorm[,nn(.N)]` journals organized into `r jclu$tab[,nn(.N)]` superdisciplines.

```{r jclu-tab-sup,include=T}
jsup<-jclu$tab %>% setorder(-N)
jsup[,Pct:=round(prop.table(N)*100,1)]
setnames(jsup,'super','Superdiscipline')
sg(jsup,tit = "JSTOR Journal Counts")
```

```{r jclu-tab-sub,include=T}
jclu_sub<-jstorm[sapply(discipline,function(x) 'Social Sciences'%in%x)][,discipline:=lapply(discipline,setdiff,'Social Sciences')] %>% jstorm2jclu.f
jsub<-jclu_sub$tab
setnames(jsub,'super','Subdiscipline')
sg(jsub,tit = "JSTOR Social Sciences Journal Counts")
```

Social science journals are overrepresented due to JSTOR's initial focus in that area. The journals within social science cover `r jsub %>% length %>% nn` different subdisciplines.

```{r jclu-tab-sub-econ,include=T}
jclu_sub_econ<-jstorm[sapply(discipline,function(x) 'Business & Economics'%in%x)][,discipline:=lapply(discipline,setdiff,'Business & Economics')] %>% jstorm2jclu.f
jsube<-jclu_sub_econ$tab
setnames(jsube,'super','Subdiscipline')
sg(jsube,tit = "JSTOR Business & Economics Journal Counts")
```

```{r jstorm2tab}
jtab<-jstorm2tab.f(jstorm,beg.bef = 1900,end.aft = 1900)
sg(jtab[,.(
  `Title History`=publication_title
  ,Discipline=discipline
  ,Start=start %>% as.character
  ,Stop=stop %>% as.character
)],tit='20th Century Social Science Journals in JSTOR',new.col.align = 'p{0.4\\\\linewidth}p{0.3\\\\linewidth}rr'
#,rplc = ec('\\{tabular\\},\\{longtable\\}')
)
```

```{r jstorm2fig,include=T,fig.cap='Periods in the Growth of the Number of Social Science Journals in the JSTOR Archive'}
f<-'d/q/jfig.RData'
if(file.exists(f)){
  load(f)
} else {
  jfig<-jstorm2fig.f(jstorm,jclu,series=jclu$tab$super[1])
  save(jfig,file=f)
}
rm(f)
plt(jfig$p)
```


```{r nces2phd,include=T,fig.cap='Decennial growth in number of PhD degrees conferred in the U.S.'}
f<-'d/q/nces.RData'
if(file.exists(f)){
  load(f)
} else {
  nces<-nces2phd.f()
  save(nces,file=f)
}
rm(f)
plt(nces$fig)
```


```{r nces/jstorm,include=T,fig.cap='Number of PhDs conferred in the United States per Social Science Journal'}
rat<-jfig$d[between(year,1800,2000)&super=='Social Sciences'] %>% setkey(year)
setkey(nces$int,year)
rat<-merge(rat,nces$int)
rat<-rat[,.(year,ratio=N.y/N.x,dif1g)]
r<-rat[,range(ratio)]
n<-diff(r)*.1

plt(myth(
  ggplot(data=rat,mapping = aes(x=year,y=ratio)) +
    annotate('rect',xmin=1888,xmax=1922,ymin=-Inf,ymax=Inf,alpha=.1) +
    geom_line(aes(color=dif1g),size=1.5) +
    geom_point(data=rat[.(nces$tab$year)] %>% na.omit,mapping = aes(x=year,y=ratio),shape=21,size=1.5,stroke=1,fill='white')
) + theme(legend.position="none",axis.title.x = element_blank()))
wrk<-rat[between(year,1888,1922),summary(ratio)]
```

This period represents one of stable growth, as the size of the field grows with the number of players on it. Between 1888 and 1922 there tended to be about `r nn(wrk['Median'],0)` new PhDs in the U.S. for every social science journal even as each population grew year over year. These growth patterns begin to diverge around `r jfig$d[super=='Social Sciences'][dif1g==3][1,year]` as a decades long acceleration of personnel begins, relatively slowly between 1920 and 1960 at an average acceleration rate of `r rat[between(year,1920,1960,F),nn(ratio %>% mean)]` PhDs per journal per year, and then quite precipitously in the 1960s at an average acceleration rate of `r rat[between(year,1960,1980,T),nn(ratio %>% mean)]`.

### JSTOR archive {#kd-dd}

```{r zot2bib}
f<-'d/q/zot2bib.RData'
if(file.exists(f)){
  load(f)
} else{
  zot2bib<-zot2bib.f(
    users='2730456'
    ,collections = fread('d/q/zotcollections.txt')$URL
    ,key = 'qMxyytAhp07W9jNOGssIQasg'
  )
  save(zot2bib,file=f)
}
rm(f)
d<-lapply(zot2bib,function(x) x[,.(publicationTitle,date,volume,issue,pages,bp,ep,title,creators,dateModified,url)]) %>% rbindlist %>% `[`(!duplicated(.[,!9]))
setorder(d,publicationTitle,dateModified)
##### find corrupt
d[is.na(publicationTitle),]
d<-d[!is.na(publicationTitle),]
```

Every record for every journal was downloaded manually, including front and back matter, articles, and book reviews.

```{r z2b-dups}
# find duplicates
d[duplicated(d[,!9:10]),.(dateModified,pub=publicationTitle,tit=substr(title,1,50),date,volume,issue,pages)][,cat(date,' ',volume,' ',issue,' ',tit,' ',pages,'\n',sep=''),by=dateModified]
```

```{r z2p-pgap}
# find page gaps
p<-d[!(duplicated(d[,!9:10])|is.na(bp)),.(
  t=publicationTitle
  ,d=date
  ,v=volume
  ,i=issue
  ,p=mapply(function(b,e) b:e,b=bp,e=ep)
),by=.I]
p[,.(gap=setdiff(
  p %>% unlist %>% range %>% as.list %>% do.call(seq,.)
  ,p %>% unlist %>% unique %>% sort
)),by=.(t,d,v,i)][,.(gap=list(gap)),by=.(t,d,v,i)]
```

```{r z2b-vgap}
# find volume gaps
p[,table(t,i)]
cat('',sep='\n')
vg<-p[,table(d,factor(i,levels=1:6),t)]
vg[vg==0]<-NA
h<-hist(vg)
h<-which(vg<=h$breaks[2],arr.ind = T)
check<-cbind(h[,-1],vg[h])[order(h[,3],h[,1]),]
colnames(check)<-c('i','j','n')
cat('\nDouble check these volumes:\n\n')
check
```

```{r z2b-short}
# find short runs
setorder(d,publicationTitle,date,volume,issue,bp)
lt<-d[!is.na(bp+ep),.(title=title[.N],pages=pages[.N],dateModified=dateModified[.N],url=url[.N]),by=.(publicationTitle,date,volume,issue)]
ltn<-lt[,.N,by=title][N<3,title]
setkey(lt,title)
# will cause autoban, but this will test automatically
if(F) w<-lt[ltn,list({
  Sys.sleep(rnorm(1) %>% abs %>% `+`(1.5))
  cat('.')
  try(url %>% read_html %>% html_text %>% grepl('Next Item',.))
}),by=url]
lt[ltn][,cat(publicationTitle,' ',date,' ',volume,' ',issue,' ',title,' ',pages,'\n',url,'\n\n',sep=''),by=dateModified] %>% invisible
```

```{r jpdf2imp}
f<-'d/d/jpdf.RData'
if(file.exists(f)){
  load(f)
} else{
  # seed<-.Random.seed %>% sample(1)
  # cat('Seed: ',seed,'\n',sep='')
  # set.seed(seed)
  jpdf<-jpdf2imp.f(
    dir('d/d',full.names = T,recursive = T,pattern = '\\.pdf$') # %>% sample(100) # readLines('test.txt')
    ,ocr = T) # write feedback for long imports
  save(jpdf,file=f)
}
rm(f)
if(!'filt'%in%ls()) filt<-list()
filt$import<-cor2aud.f(jpdf$imp,'imported')
```

```{r imp2cln}
f<-'d/d/clnp.RData'
if(file.exists(f)){
  load(f)
} else {
  # seed<-.Random.seed %>% sample(1)
  # cat('Seed: ',seed,'\n',sep='')
  clnp<-imp2cln.f(
    imp = jpdf$imp
    ,hand = 'd/d/cln1537906742.txt'
    # ,seed=seed    
  )
  save(clnp,file=f)
}
rm(f)
if(!'filt'%in%ls()) filt<-list()
filt$clean<-cor2aud.f({setkey(jpdf$imp,doc,par);jpdf$imp[clnp$prd[(mth),.(doc,par)]]},'cleaned')
```

### Sampling {#kd-dp1}

```{r imp2ftx,eval=F}
f<-'d/p/ftx.RData'
if(file.exists(f)){
  load(f)
} else {
  # seed<-.Random.seed %>% sample(1)
  # cat('Seed: ',seed,'\n',sep='')
  # set.seed(seed)
  # s<-sample(jpdf$met[,doc],10)
  # ftx<-imp2ftx.f(jpdf$imp[s])
  ftx<-imp2ftx.f({setkey(jpdf$imp,doc,par);jpdf$imp[clnp$prd[(mth),.(doc,par)]]})
  save(ftx,file = f)
}
if(!'filt'%in%ls()) filt<-list()
filt$token<-cor2aud.f(ftx,'tokenized')
rm(f,jpdf,clnp)
```

```{r ftx2pre}
f<-'d/p/pre.RData'
if(file.exists(f)){
  load(f)
} else {
  pre<-ftx2pre.f(ftx,frq = 5,nchr=3)
  save(pre,file = f)
}
rm(f,ftx)
if(!'filt'%in%ls()) filt<-list()
filt$pre<-cor2aud.f(pre[!is.na(stm)],'preprocessed')
```

```{r pre2sam}
f<-'d/p/sam.RData'
if(file.exists(f)){
  load(f)
} else {
  seed<-.Random.seed %>% sample(1)
  cat('Seed: ',seed,'\n',sep='')
  set.seed(seed)
  sam<-pre[sample(doc[!is.na(stm)] %>% unique,1e2)]
  sam[,stm:=droplevels(stm,exclude=unique(sam[,.(stm,doc)])[,.N,by=stm][N==1,stm %>% as.character]) %>% droplevels %>% droplevels(exclude=NA)]
  attr(sam,'seed')<-seed
  save(sam,file=f)
}
rm(f)
if(!'filt'%in%ls()) filt<-list()
filt$sam<-cor2aud.f(sam[!is.na(stm)] %>% setkey(doc),'sampled')
```


### Units of Analysis

Conventionally researchers feed entire documents into the construction of term frequencies. This method treats any term in a document as being related to any other term by the same degree. The goal of any topic mixture model algorithm is to sift these terms into different topic categories basically by looking for clues across documents; a topic can be "seen" in a particular document to the extent that other documents include that topic and *other* topics different from the focal article, so that the intersection of terms reveals the topic. But a much simpler assumption to reduce the attendant noise within a document is to merely feed lower level syntactic structures--paragraphs and sentences--to the algorithm. We will see that doing so greatly improves the usefulness of discovered topics.

The irony of this approach is that while topics become more clear as documents become shorter, the assignment of any particular shorter document to a topic is murkier due to the smaller word count. 

Long documents will contribute more text to the corpus, but this is fair as they make up more of the population of text. Thus a simple random sample will allow better descriptive statistics. I sampled at the paragraph level because.

```{r pre2mlc}
f<-'d/p/mlc.RData'
if(file.exists(f)){
  load(f)
} else {
  mlc<-pre2mlc.f(sam)
  save(mlc,file = f)
}
rm(f)
```

```{r pre2des}
# des.com<-pre2des.f(pre)
# des.stm<-pre2des.f(pre[!is.na(stm)])
#kab(list(des.com,des.stm,data.table(tot=round(des.stm$cnt/des.com$cnt*100,2))),caption = 'Full Text Descriptives')
```

## Modeling {#kd-dp2}

The modeling objective is twofold, to sort text into categories of similarity, and to describe the qualitative content that defines the category membership. In this way we may operationalize the notion of cultural meaning or cultural logic as the rules of category classification. reduce expressions as instances of a latent category of expression.

### How many topics?

```{r mlc2kmc,eval=F}
```

```{r pre2tel}
f<-'d/p/tel.RData'
if(file.exists(f)){
  load(f)
} else {
  tel<-pre2tel.f(pre[!is.na(stm)],stop = 3)
  save(tel,file = f)
}
rm(f)
```

```{r tamK}
f<-'d/p/clu.RData'
if(file.exists(f)){
  load(f)
} else {
  tel<-tel[!s%in%tm::stopwords()] # remove stopwords
  tel<-tel[!r%in%tm::stopwords()]
  tel<-tel[s!=r] # remove remaining loops
  system.time(clu<-tel2clu.f(tel))
  save(clu,file=f)  
}
rm(f)
```

```{r ctr}
ctr<-apply(clu$memberships,1,function(x) x %>% table %>% sort(decreasing = T))
View(ctr)
top<-ctr[[1]] %>% prop.table %>% `*`(100) %>% round(3) 
top %>% head(30)

cp<-tilit::ov2chpt.f(top %>% head(50),min.period = 2,drv = 1)
top %>% head(50) %>% as.vector %>%  plot(col=cp$g)

```

```{r crs}
sm<-list()
for(i in 1:nrow(clu$memberships)) sm[[i]]<-factor(clu$memberships[i,],levels=order(table(clu$memberships[i,]),decreasing = T))
sm<-do.call(data.table,sm)
setnames(sm,paste0('h',ncol(sm):1))
sm[,cr:=clu$names]
sm[,m:=as.integer(h1)] %>% setkey(m)
sm %>% setkey(cr)

tel[,`:=`(ms=sm[s,h3],mr=sm[r,h3])]
tel[,bc:=ms==mr]
bct<-list(tel[s!=r,.(tew=sum(ew)),by=.(s,bc)] %>% setnames('s','cr')
,tel[s!=r,.(tew=sum(ew)),by=.(r,bc)] %>% setnames('r','cr')
) %>% rbindlist %>% .[,.(tew=sum(tew)),by=.(cr,bc)] %>% setkey(cr,bc)
bct[,m:=sm[cr,h3]]
bct<-dcast(bct,cr+m~bc,value.var = 'tew',fill=0)
bct[,tew:=`FALSE`+`TRUE`]
bct[,pit:=`FALSE`/tew] # proportion internal ties
bct[,`:=`(`FALSE`=NULL,`TRUE`=NULL)]
bct[,m:=as.integer(m)]
bct %>% setorder(m,-pit)
bct %>% setkey(m)
plt(
myth(
ggplot(bct[.(9)],aes(x=tew,y=pit)) + geom_text(aes(label=cr,angle=45),alpha=.5)
) + scale_x_log10())
```



```{r mlc2mlk}
f<-'d/p/mlk.txt'
if(file.exists(f)) {mlk<-fread(f)} else {
  system.time(mlc2mlk.f(mlc,verb=T) %>% fwrite(f))
  pbapply::pbreplicate(999,mlc2mlk.f(mlc) %>% fwrite('d/p/mlk.txt',append = T),cl = 1)
}
rm(f)
mlk<-melt(mlk,measure.vars = names(mlk),variable.name = 'level',value.name = 'K') %>% setkey(level)
```

```{r mlk2sim}
f<-'d/b/sim.RData'
if(file.exists(f)) {load(f)} else {
  sim<-mlk2sim.f(mlk,rep = 1e3)
  save(sim,file=f)
}
rm(f)
```

```{r sim-fig,include=T,fig.cap='Distribution of K by convex hull'}
sim$fig1
```

```{r mlk2k,include=T}
f<-'d/b/k.RData'
if(file.exists(f)){
  load(f)
} else {
  k<-mlk[,.(k=pbapply::pbreplicate(1e5,psych::kurtosi(K %>% sample(replace = T)),cl = 1)),by=level][,.(e=mean(k),se=sd(k),l99=quantile(k,.005),u99=quantile(k,.995),`P(e ≦ 0)`=mean(k<=0)),by=level]
  save(k,file=f)
}
rm(f)
#debugonce(sg)
sg(k[,round(.SD,4),.SDcols=names(k)[-1],by=level],tit='Kurtosis Permutation Test')
```

```{r mlk-tab,include=T,fig.cap='Significant Counts of K'}
sim$fig2

#sg(sim$tab,'Significant Counts of K')
```

### Model selection

```{r mlc2stm}
# mod<-stmbow2lda.f(list(documents=mlc$par,vocab=mlc$voc),k=50,out.dir='d/p',verbose=F,visualize.results = T)
# mod$top.word.phi.beta[mod$top.word.phi.beta==0] <-.Machine$double.eps
# debugonce(lda2viz.f)
# viz<-lda2viz.f(mod,'d/b')
```

## Making sense
