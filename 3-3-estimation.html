<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Knowledge of the U.S. Social Sciences</title>
  <meta name="description" content="Knowledge of the U.S. Social Sciences">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Knowledge of the U.S. Social Sciences" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Knowledge of the U.S. Social Sciences" />
  
  
  

<meta name="author" content="Brooks Ambrose">


<meta name="date" content="2019-09-10">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="3-2-data.html">
<link rel="next" href="3-4-diagnostics.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.8.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.41.3/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.41.3/plotly-latest.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<link href="libs/wordcloud2-0.0.1/wordcloud.css" rel="stylesheet" />
<script src="libs/wordcloud2-0.0.1/wordcloud2-all.js"></script>
<script src="libs/wordcloud2-0.0.1/hover.js"></script>
<script src="libs/wordcloud2-binding-0.2.0/wordcloud2.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.4/datatables.js"></script>
<link href="libs/dt-core-1.10.16/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.16/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.16/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<script src="libs/d3-4.5.0/d3.min.js"></script>
<script src="libs/sankey-1/sankey.js"></script>
<script src="libs/sankeyNetwork-binding-0.4/sankeyNetwork.js"></script>



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Brooks Ambrose</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Getting Started</a></li>
<li class="chapter" data-level="" data-path="chapter-abstracts.html"><a href="chapter-abstracts.html"><i class="fa fa-check"></i>Chapter Abstracts</a><ul>
<li class="chapter" data-level="" data-path="chapter-abstracts.html"><a href="chapter-abstracts.html#what-to-read-how-archivists-professors-and-educators-cultivate-knowledge-terrains"><i class="fa fa-check"></i>What to Read? How Archivists, Professors, and Educators Cultivate Knowledge Terrains</a></li>
<li class="chapter" data-level="" data-path="chapter-abstracts.html"><a href="chapter-abstracts.html#social-science-disciplines-today"><i class="fa fa-check"></i>Social Science Disciplines Today</a></li>
<li class="chapter" data-level="" data-path="chapter-abstracts.html"><a href="chapter-abstracts.html#genre-and-the-literature"><i class="fa fa-check"></i>Genre and the Literature</a></li>
<li class="chapter" data-level="" data-path="chapter-abstracts.html"><a href="chapter-abstracts.html#economy-society-and-vocabulary-the-origins-of-u.s.-anthropology-and-sociology-1888-1922"><i class="fa fa-check"></i>Economy, Society, and Vocabulary: The Origins of U.S. Anthropology and Sociology, 1888-1922</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-int.html"><a href="1-int.html"><i class="fa fa-check"></i><b>1</b> What to Read? How Archivists, Professors, and Educators Cultivate Knowledge Terrains</a><ul>
<li class="chapter" data-level="1.0.1" data-path="1-int.html"><a href="1-int.html#knowledge-roles"><i class="fa fa-check"></i><b>1.0.1</b> Knowledge roles</a></li>
<li class="chapter" data-level="1.1" data-path="1-1-knowledge-as-password.html"><a href="1-1-knowledge-as-password.html"><i class="fa fa-check"></i><b>1.1</b> Knowledge as password</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-1-knowledge-as-password.html"><a href="1-1-knowledge-as-password.html#knowledge-as-influence"><i class="fa fa-check"></i><b>1.1.1</b> Knowledge as influence</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-2-knowledge-terrain.html"><a href="1-2-knowledge-terrain.html"><i class="fa fa-check"></i><b>1.2</b> Knowledge terrain</a></li>
<li class="chapter" data-level="1.3" data-path="1-3-generic-isomorphism.html"><a href="1-3-generic-isomorphism.html"><i class="fa fa-check"></i><b>1.3</b> Generic isomorphism</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-dis.html"><a href="2-dis.html"><i class="fa fa-check"></i><b>2</b> Social Science Disciplines Today</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-kd-dq1.html"><a href="2-1-kd-dq1.html"><i class="fa fa-check"></i><b>2.1</b> JSTOR Journals</a></li>
<li class="chapter" data-level="2.2" data-path="2-2-network-mode-projection.html"><a href="2-2-network-mode-projection.html"><i class="fa fa-check"></i><b>2.2</b> Network Mode Projection</a></li>
<li class="chapter" data-level="2.3" data-path="2-3-ncd.html"><a href="2-3-ncd.html"><i class="fa fa-check"></i><b>2.3</b> Network Community Detection</a></li>
<li class="chapter" data-level="2.4" data-path="2-4-network-visualization.html"><a href="2-4-network-visualization.html"><i class="fa fa-check"></i><b>2.4</b> Network Visualization</a></li>
<li class="chapter" data-level="2.5" data-path="2-5-discussion.html"><a href="2-5-discussion.html"><i class="fa fa-check"></i><b>2.5</b> Discussion</a></li>
<li class="chapter" data-level="2.6" data-path="2-6-cartography-of-classification.html"><a href="2-6-cartography-of-classification.html"><i class="fa fa-check"></i><b>2.6</b> Cartography of Classification</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-gen.html"><a href="3-gen.html"><i class="fa fa-check"></i><b>3</b> Genre and the Literature</a><ul>
<li class="chapter" data-level="3.1" data-path="3-1-method.html"><a href="3-1-method.html"><i class="fa fa-check"></i><b>3.1</b> Method</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-1-method.html"><a href="3-1-method.html#distant-sampling"><i class="fa fa-check"></i><b>3.1.1</b> Distant sampling</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-1-method.html"><a href="3-1-method.html#no-cigar"><i class="fa fa-check"></i><b>3.1.2</b> No cigar</a></li>
<li class="chapter" data-level="3.1.3" data-path="3-1-method.html"><a href="3-1-method.html#topic-models"><i class="fa fa-check"></i><b>3.1.3</b> Topic Models</a></li>
<li class="chapter" data-level="3.1.4" data-path="3-1-method.html"><a href="3-1-method.html#qualitative-cross-validation"><i class="fa fa-check"></i><b>3.1.4</b> Qualitative Cross Validation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-2-data.html"><a href="3-2-data.html"><i class="fa fa-check"></i><b>3.2</b> Data</a></li>
<li class="chapter" data-level="3.3" data-path="3-3-estimation.html"><a href="3-3-estimation.html"><i class="fa fa-check"></i><b>3.3</b> Estimation</a></li>
<li class="chapter" data-level="3.4" data-path="3-4-diagnostics.html"><a href="3-4-diagnostics.html"><i class="fa fa-check"></i><b>3.4</b> Diagnostics</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-4-diagnostics.html"><a href="3-4-diagnostics.html#lower-tail-probabilities"><i class="fa fa-check"></i><b>3.4.1</b> Lower tail probabilities</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-4-diagnostics.html"><a href="3-4-diagnostics.html#topic-graphs"><i class="fa fa-check"></i><b>3.4.2</b> Topic Graphs</a></li>
<li class="chapter" data-level="3.4.3" data-path="3-4-diagnostics.html"><a href="3-4-diagnostics.html#summary"><i class="fa fa-check"></i><b>3.4.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-5-topic-interpretation.html"><a href="3-5-topic-interpretation.html"><i class="fa fa-check"></i><b>3.5</b> Topic interpretation</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-5-topic-interpretation.html"><a href="3-5-topic-interpretation.html#journals"><i class="fa fa-check"></i><b>3.5.1</b> Journals</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-5-topic-interpretation.html"><a href="3-5-topic-interpretation.html#bellwether-texts"><i class="fa fa-check"></i><b>3.5.2</b> Bellwether texts</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-5-topic-interpretation.html"><a href="3-5-topic-interpretation.html#terms"><i class="fa fa-check"></i><b>3.5.3</b> Terms</a></li>
<li class="chapter" data-level="3.5.4" data-path="3-5-topic-interpretation.html"><a href="3-5-topic-interpretation.html#dossiers"><i class="fa fa-check"></i><b>3.5.4</b> Dossiers</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3-6-topic-clusters.html"><a href="3-6-topic-clusters.html"><i class="fa fa-check"></i><b>3.6</b> Topic clusters</a></li>
<li class="chapter" data-level="3.7" data-path="3-7-reading-strata.html"><a href="3-7-reading-strata.html"><i class="fa fa-check"></i><b>3.7</b> Reading strata</a></li>
<li class="chapter" data-level="3.8" data-path="3-8-results.html"><a href="3-8-results.html"><i class="fa fa-check"></i><b>3.8</b> Results</a></li>
<li class="chapter" data-level="3.9" data-path="3-9-discussion-1.html"><a href="3-9-discussion-1.html"><i class="fa fa-check"></i><b>3.9</b> Discussion</a></li>
<li class="chapter" data-level="3.10" data-path="3-10-island-of-california.html"><a href="3-10-island-of-california.html"><i class="fa fa-check"></i><b>3.10</b> Island of California</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-voc.html"><a href="4-voc.html"><i class="fa fa-check"></i><b>4</b> Economy, Society, and Vocabulary: The Origins of U.S. Anthropology and Sociology, 1888-1922</a><ul>
<li class="chapter" data-level="4.1" data-path="4-1-social-science-history-in-context.html"><a href="4-1-social-science-history-in-context.html"><i class="fa fa-check"></i><b>4.1</b> <!--B--> Social science history in context</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4-1-social-science-history-in-context.html"><a href="4-1-social-science-history-in-context.html#the-social-sciences"><i class="fa fa-check"></i><b>4.1.1</b> The social sciences</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-2-a-census-of-words.html"><a href="4-2-a-census-of-words.html"><i class="fa fa-check"></i><b>4.2</b> <!--M1--> A census of words</a></li>
<li class="chapter" data-level="4.3" data-path="4-3-digital-full-text-archives.html"><a href="4-3-digital-full-text-archives.html"><i class="fa fa-check"></i><b>4.3</b> Digital full-text archives</a><ul>
<li class="chapter" data-level="4.3.1" data-path="4-3-digital-full-text-archives.html"><a href="4-3-digital-full-text-archives.html#quality-of-evidence"><i class="fa fa-check"></i><b>4.3.1</b> Quality of evidence</a></li>
<li class="chapter" data-level="4.3.2" data-path="4-3-digital-full-text-archives.html"><a href="4-3-digital-full-text-archives.html#data-1"><i class="fa fa-check"></i><b>4.3.2</b> Data</a></li>
<li class="chapter" data-level="4.3.3" data-path="4-3-digital-full-text-archives.html"><a href="4-3-digital-full-text-archives.html#an-historical-interlude"><i class="fa fa-check"></i><b>4.3.3</b> An historical interlude</a></li>
<li class="chapter" data-level="4.3.4" data-path="4-3-digital-full-text-archives.html"><a href="4-3-digital-full-text-archives.html#how-many-topics"><i class="fa fa-check"></i><b>4.3.4</b> How many topics?</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-4-topic-interpretation-1.html"><a href="4-4-topic-interpretation-1.html"><i class="fa fa-check"></i><b>4.4</b> <!--M2--> Topic interpretation</a><ul>
<li class="chapter" data-level="4.4.1" data-path="4-4-topic-interpretation-1.html"><a href="4-4-topic-interpretation-1.html#anthropology"><i class="fa fa-check"></i><b>4.4.1</b> Anthropology</a></li>
<li class="chapter" data-level="4.4.2" data-path="4-4-topic-interpretation-1.html"><a href="4-4-topic-interpretation-1.html#sociology"><i class="fa fa-check"></i><b>4.4.2</b> Sociology</a></li>
<li class="chapter" data-level="4.4.3" data-path="4-4-topic-interpretation-1.html"><a href="4-4-topic-interpretation-1.html#interdisciplinarity"><i class="fa fa-check"></i><b>4.4.3</b> Interdisciplinarity</a></li>
<li class="chapter" data-level="4.4.4" data-path="4-4-topic-interpretation-1.html"><a href="4-4-topic-interpretation-1.html#better-worse-or-stayed-the-same"><i class="fa fa-check"></i><b>4.4.4</b> Better, worse, or stayed the same</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4-5-discussion-2.html"><a href="4-5-discussion-2.html"><i class="fa fa-check"></i><b>4.5</b> <!--E--> Discussion</a></li>
<li class="chapter" data-level="4.6" data-path="4-6-a-census-of-social-science-scholarship.html"><a href="4-6-a-census-of-social-science-scholarship.html"><i class="fa fa-check"></i><b>4.6</b> A census of social science scholarship</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/brooksambrose" target="blank">GitHub</a></li>
<!--<li><a href="https://www.ischool.berkeley.edu/people/brooks-ambrose" target="blank">UC Berkeley</a></li>-->

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Knowledge of the U.S. Social Sciences</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="estimation" class="section level2">
<h2><span class="header-section-number">3.3</span> Estimation</h2>
<p>I will I use the <code>stm</code> package in R to estimate a series of topic models <span class="citation">(Roberts et al. <a href="bibliography.html#ref-Roberts2013structural" role="doc-biblioref">2013</a>, <a href="bibliography.html#ref-Roberts2018stm" role="doc-biblioref">2018</a>)</span>. The structural topic model (STM) is a variation on the correlated topic model (CTM) that allows for direct estimation of how covariates affect topic formation. The CTM was an early modification of the initial latent Dirichlet allocation (LDA) estimator, which tended to create topics that were statistically independent of each other and which therefore made it difficult to model documents as composed of multiple topics, a feature which has become central to the usefulness of topic models for applied research <span class="citation">(Blei and Lafferty <a href="bibliography.html#ref-Blei2007correlated" role="doc-biblioref">2007</a>)</span>. It will be helpful to understand the complexity of the CTM before complicating it further, thus for the sake of simplicity I use the <code>stm</code> package to fit CTMs without leveraging the additional feature of covariate modeling.</p>
<p>To briefly explain the difference, the STM builds on the CTM by modeling the effect of document level covariates on topics in two different ways. First, covariates may affect topic prevalence. For example, including a dummy variable for the JSTOR discipline label Social Science interacted across all topic by document probabilities would provide a parameter measuring the degree to which social science texts contribute terms more or less frequently to that topic than do non social science texts. For example, a binary category between social sciences and humanities interacted with a topic about music might show that social science texts are ten percent less prevalent in the music topic than are humanities texts. Second, covariates may affect topic content. Here the terms of a document inherit the covariate assigned to their document of origin. A social science dummy interacted across all topic by term probabilities provides a parameter measuring the degree to which a term of a particular covariate origin is more or less likely to contribute to a topic. In practice, content models help construct two different term rankings for the same topic, two because estimation on the high dimensional term vector space is intractable for all but the simplest binary covariate. In the same social science versus humanities binary, the content model would show how the vocabulary of social science texts differs from the vocabulary of humanities texts when talking about the same topic, music. In a subsequent chapter I will find occasion to use these more powerful features of the STM.</p>
<p>Because there are so many parameters CTM models are difficult to estimate, but the core approach is the familiar maximum likelihood framework. Estimators attempt to discover the parameters for the unobserved portions of the model that are most likely given the observed portions, the document by term counts. The estimator used in the <code>stm</code> package is a version of expectation maximization (EM) in which some parameters of the model are set arbitrarily, for instance randomly, in order to reduce the likelihood function to something tractable that can be maximized. The outcomes to each step of this expectation (guessing) and maximization (solving) procedure are then fed into another iteration. In practice each step of guessing leads to a smaller change in the parameters, and the model is said to have converged when the changes fall below a predetermined threshold.</p>
<p>The parameter space of topic models is far too complex to be able to write solvable likelihood equations and even for EM estimators to guess at them with consistent and accurate results, so topic models frequently include a raft of simplifying hyperparameters to reduce the dimensionality of the problem. It is not within the present scope to discuss these hyperparameters unless they are exogenous and can be set in ways that are practically meaningful for applied research problems. I have already discussed two of these, the alpha and sigma priors, which let me control the level of mixture of topics within documents and the correlation of topics respectively. I trust that others that are endogenous to model estimation lead to sensible results.</p>
<p>Hyperparameters aside, it is also necessary to initialize the substantive parameters of the model for the first EM step. The choice of model initialization is substantively meaningful and under the user’s control in the stm package. For example, the CTM model may be initialized with the values of an LDA model where topics are uncorrelated; in this situation EM would step the topic by document probabilities toward a more correlated outcome in which certain topics appear together frequently, if this model is more likely given the data.</p>
<p>The initialization I will use is called spectral initialization, which is related to the concept of anchor words discussed above. A spectral model considers only the square term by term matrix where each column and row refers to the number of times a particular word co-occurs within any document with every other word in the vocabulary. A dimensionality reduction technique such as principle component analysis or matrix factorization can be used to represent each term in a number of dimensions equal to the desired number of topics. This can in turn be used to initialize the topic by term matrix of the model. Finally, the usually much simpler topic by document matrix can converge quickly using EM on the basis of the good guess supplied by the spectral model.</p>
<p>Because the vocabulary vector tends to be very long it is not trivial even for spectral methods to reduce the term by term matrix to the number of topics without additional assumptions. <span class="citation">Arora et al. (<a href="bibliography.html#ref-Arora2018Learning" role="doc-biblioref">2018</a>)</span> have shown that assuming the existence of anchor words makes the decomposition fast and efficient while retaining the feature of a single determinate solution <span class="citation">(Roberts <a href="bibliography.html#ref-Roberts2016Navigating" role="doc-biblioref">2016</a>)</span>. An anchor word is one whose probability is one for one topic and zero for all others. In the space of the solution the anchor words become the farthest corners of the multidimensional cloud of terms, and a convex hull drawn through them will contain all other terms. If the anchors are treated as singularly representing their entire topic, the position of every other term can be represented as a linear combination of the positions of all the anchors. The linear weights of the anchors then become the topic probabilities of the words, such that the closer a term is to an anchor the higher its probability from the anchor’s topic and the lower the probability for all other anchors’ topics. An anchor for each topic must be anointed so that its vector can be set to the assumed maximum sparsity, and the criterion for doing so is to find words with the above mentioned maximum frequency and exclusivity, words that always appear only given a particular set of other words. Even if the anchor word assumption is not strictly valid, using an anchor based spectral initialization in combination with the EM estimator may relax the assumption of sparsity (monosemy) and allow some distribution of erstwhile anchor words (polysemy) among topics.</p>
<p>Above I commented that sparse model techniques like L1 regularization could help clarify topics by setting more coefficients to zero. Such techniques create biased models in that they are less likely given the data, but the hope is that in the case of topic models it is the irrelevant terms of a topic or topics of a document that will be biased downward, in essence making regularization a kind of filter on the idiosyncratic portions of the corpus. Unfortunately, this desirable filter may not be the actual effect of regularization. Sparse model techniques tend to bias downward the coefficients of terms that are highly correlated with other terms that themselves have a stronger association with the outcome. By assigning the portion of variance explained that overlaps among correlated predictors to the stronger term, it resolves an intractable ambiguity in an arbitrary way. In this situation L1 regularization may, in the topic by term matrix, occlude important and relevant terms rather than prune irrelevant ones, such as idiosyncratic or suppressed topic terms. This may actually make it harder to interpret topics without helping resolve topic corruption.</p>
<p>In the document by topic matrix L1 regularization may be more helpful by leading to topic concentration, which creates an effect similar to setting the alpha concentration parameter of the Dirichlet distribution in LDA below one. Like a short blanket that cannot keep the head and feet warm at once, regularization may also offset the goal of modeling topic correlation introduced by the CTM. There is no statistical guide out of this morass. The impractical solution is to fit models under multiple assumptions and compare the results by QCV. For a model that is already as complicated to interpret as the topic model, this would be a steep climb for most researchers. The normal remedy is liberal use of George Box’s assertion that “all [models] are wrong” <span class="citation">(DiMaggio et al. <a href="bibliography.html#ref-DiMaggio2013Exploiting" role="doc-biblioref">2013</a>:582)</span>, which may not satisfy those hoping that topic models can shed light on the more easily occluded corners of intellectual history.</p>
<p>Notwithstanding the deep inventory of research decisions I have mentioned, I will begin with the conventional hyperparameter assumption of the number of topics <span class="math inline">\(K\)</span>. I fit nine models in sequence from <span class="math inline">\(K\)</span> = 2 to <span class="math inline">\(K\)</span> = 10 in order to use the development of topics through the <span class="math inline">\(K\)</span> space as context for the interpretation of the focal ten topic model. I set the sigma prior to zero to allow for the free estimation of document by topic correlations, which can be set as high as one to mitigate the CTM. I use spectral initialization, which recall relies on the anchor words assumption to facilitate a determinate solution the topic by term matrix that is then updated to find a more likely within document topic mixture. In spectral initialization there is no alpha concentration parameter as in LDA, and because I do not use L1 regularization I create no preference during estimation for sparse, concentrated document by topic distributions. These choices favor a less biased and more saturated model.</p>
<p>To set up QCV prior to model inspection, I use the document by topic matrix of the focal ten topic model to establish a sampling frame for the creation of test comparisons. These comparisons are designed to establish the presumptive substantive validity of the head of the document by topic probabilities without consideration of pathologies arising from tail-based classification errors. In these “sniff tests”, which are explained in greater detail below, I ask myself to recover model classifications of documents by inspecting selected documents without prior knowledge of topic by term content. While this is an admittedly seat-of-the-pants goodness of fit test, if I cannot make sense of topic separation then there is a more serious problem with the core deliverable of the topic model. Passing these tests is a necessary check before getting into more subtle model interpretation concerns.</p>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="3-2-data.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="3-4-diagnostics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": "https://github.com/brooksambrose/portfolio/blob/draft/03.Rmd",
"text": "Edit"
},
"download": ["ambrose_dissertation.pdf"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
